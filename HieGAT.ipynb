{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hshuai97/Colab20210803/blob/main/HieGAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udY97loZQQA2"
      },
      "source": [
        "cf. [dgl gat](https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html#)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbK7EFLcirun"
      },
      "source": [
        "# install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aIClaKj9iUSC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "c5d870c4-82cc-4671-e042-4bfd2f5fe7f5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 200})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl-cu113\n",
            "  Downloading https://data.dgl.ai/wheels/dgl_cu113-0.8.2.post1-cp37-cp37m-manylinux1_x86_64.whl (220.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 220.6 MB 16 kB/s \n",
            "\u001b[?25hCollecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[K     |████████████████████████████████| 281 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (4.64.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (2.6.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (2022.6.15)\n",
            "Installing collected packages: psutil, dgl-cu113\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed dgl-cu113-0.8.2.post1 psutil-5.9.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting word2vec\n",
            "  Downloading word2vec-0.11.1.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 804 kB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.21.6)\n",
            "Building wheels for collected packages: word2vec\n",
            "  Building wheel for word2vec (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2vec: filename=word2vec-0.11.1-py2.py3-none-any.whl size=156431 sha256=b2468c494732ec2707c3e35f9e24c1b42c1fbc1729502c48c3687015eee31188\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/c0/d4/29d797817e268124a32b6cf8beb8b8fe87b86f099d5a049e61\n",
            "Successfully built word2vec\n",
            "Installing collected packages: word2vec\n",
            "Successfully installed word2vec-0.11.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 6.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 200})'''))\n",
        "\n",
        "import torch\n",
        "try:\n",
        "  import dgl\n",
        "except ModuleNotFoundError:\n",
        "  CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "  !pip install dgl-{CUDA} -f https://data.dgl.ai/wheels/repo.html\n",
        "\n",
        "try:\n",
        "  import word2vec\n",
        "except ModuleNotFoundError:\n",
        "  !pip install word2vec\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "try:\n",
        "  import torch_scatter\n",
        "except ModuleNotFoundError:\n",
        "  TORCH = torch.__version__.split('+')[0]\n",
        "  CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "  !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms09x9hcivhD"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVxQo-Bwiyje",
        "outputId": "2c75da5e-c6e8-4c92-c143-c5358118d7b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "device: cuda:0\n",
            "name: Tesla P100-PCIE-16GB\n",
            "**************************************************\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "import torch as th\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import  DataLoader\n",
        "from torch_scatter import scatter_add, scatter_mean\n",
        "\n",
        "import dgl\n",
        "from dgl.nn import GATConv\n",
        "\n",
        "import word2vec\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "\n",
        "\n",
        "def MyDatasets(dataset_name):\n",
        "\n",
        "    NAME = dataset_name\n",
        "    if NAME not in ['20ng', 'r8', 'r52', 'oh', 'mr']:\n",
        "      raise ValueError('The dataset is not support')\n",
        "\n",
        "    PATH = '/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/data/'\n",
        "\n",
        "    train_texts = []\n",
        "    train_labels = []\n",
        "    with open(os.path.join(PATH, NAME+'-train-stemmed.txt'), 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        for line in data:\n",
        "          line = line.strip()\n",
        "          t = line.split('\\t')\n",
        "          train_texts.append(t[1])\n",
        "          train_labels.append(t[0])\n",
        "\n",
        "    dev_texts = []\n",
        "    dev_labels = []\n",
        "    with open(os.path.join(PATH, NAME+'-dev-stemmed.txt'), 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        for line in data:\n",
        "          line = line.strip()\n",
        "          t = line.split('\\t')\n",
        "          dev_texts.append(t[1])\n",
        "          dev_labels.append(t[0])\n",
        "\n",
        "    test_texts = []\n",
        "    test_labels = []\n",
        "    with open(os.path.join(PATH, NAME+'-test-stemmed.txt'), 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        for line in data:\n",
        "          line = line.strip()\n",
        "          t = line.split('\\t')\n",
        "          test_texts.append(t[1])\n",
        "          test_labels.append(t[0])\n",
        "\n",
        "    target_names = list(set(train_labels))\n",
        "    label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
        "\n",
        "    print(f'Dataset: {NAME}, Total train: {len(train_texts)+len(dev_texts)}, Train size: {len(train_texts)}, Dev size: {len(dev_texts)}, Test size: {len(test_texts)}, Num_class: {len(label2idx)}')\n",
        "    print(f'labels: {label2idx}')\n",
        "    print('*'*50)\n",
        "\n",
        "    return train_texts,  train_labels, dev_texts, dev_labels, test_texts, test_labels, label2idx\n",
        "\n",
        "\n",
        "class GATLayerSenLev(torch.nn.Module):   # word level --> sentence level\n",
        "  def __init__(self, in_dim, hidden_dim, num_classes, device):\n",
        "    super(GATLayerSenLev, self).__init__()\n",
        "\n",
        "    self.in_dim = in_dim\n",
        "    self.num_hidden = hidden_dim\n",
        "    self.num_classes = num_classes\n",
        "    self.device = device\n",
        "\n",
        "    self.heads =  [1]\n",
        "    self.feat_drop = 0.5\n",
        "    self.attn_drop = 0.5\n",
        "    self.negative_slope = 0.2\n",
        "    self.activation = F.elu\n",
        "    self.gram = 1\n",
        "\n",
        "    self.gat_wor = GATConv(self.in_dim, self.num_hidden, self.heads[0], self.feat_drop, self.attn_drop, self.negative_slope, False, None)\n",
        "    self.gat_sen = GATConv(self.num_hidden, self.num_classes, self.heads[0], self.feat_drop, self.attn_drop, self.negative_slope, False, None)\n",
        "\n",
        "    self.lin = torch.nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "  def add_edges_sen(self, n): # add edges for sentence-level\n",
        "    edges = []\n",
        "    for i in range(n):\n",
        "        u = i\n",
        "        for j in range(max(0, i-self.gram), min(i+self.gram+1, n)):\n",
        "            v = j\n",
        "            # - first connect the new sub_graph\n",
        "            edges.append([u, v])\n",
        "    return edges\n",
        "\n",
        "  def sentence2graph(self, inputs):  # for sen-level, \n",
        "    n = len(inputs)  # Number of sentence in each sample\n",
        "    edges = self.add_edges_sen(n) \n",
        "    u, v = zip(*edges)\n",
        "    g = dgl.graph((u, v), num_nodes=n, idtype=torch.int32).to(self.device)\n",
        "\n",
        "    g.ndata['h'] = inputs\n",
        "\n",
        "    return g\n",
        "\n",
        "  def spl_mat(self, index: list, matrix: torch):\n",
        "    split_size = [ index.count(i) for i in set(index)]\n",
        "    s_mat = torch.split(matrix, split_size)\n",
        "    return s_mat  # splited matrix\n",
        "\n",
        "  def forward(self, g,  h, all_wor_id:list):\n",
        "    x = self.gat_wor(g, h)\n",
        "    g.ndata['h'] = x.mean(1)\n",
        "\n",
        "    out_worlev = dgl.max_nodes(g, feat='h')  # (all_num_sen_one_batch, hidden_dim)\n",
        "    temp = self.spl_mat(all_wor_id, out_worlev)\n",
        "    sen_g = [self.sentence2graph(t) for t in temp]\n",
        "\n",
        "    sen_batch_g = dgl.batch(sen_g)\n",
        "    h = sen_batch_g.ndata['h']\n",
        "    x = self.gat_sen(sen_batch_g, h)\n",
        "    sen_batch_g.ndata['h'] = x.mean(1)\n",
        "\n",
        "    out_senlev = dgl.mean_nodes(sen_batch_g, feat='h')  # (N, C)\n",
        "\n",
        "    all_wor_id = torch.tensor(all_wor_id, device=self.device)  # list to tensor\n",
        "    out_worlev = scatter_add(out_worlev, all_wor_id, dim=0)  # (all_num_sen_one_batch, hidden_dim) ---> (N, hidden_dim)\n",
        "\n",
        "    return out_senlev, self.lin(out_worlev)\n",
        "\n",
        "\n",
        "class GATLayerforDocLev(torch.nn.Module):  # doc-lev\n",
        "  def __init__(self, in_dim, hidden_dim, num_classes):\n",
        "    super(GATLayerforDocLev, self).__init__()\n",
        "\n",
        "    self.in_dim = in_dim\n",
        "    self.num_hidden = hidden_dim\n",
        "\n",
        "    self.num_layers = 3\n",
        "    self.heads = [3]*(self.num_layers-1) + [1]\n",
        "    self.gat_layers = th.nn.ModuleList()\n",
        "    self.feat_drop = 0.5\n",
        "    self.attn_drop = 0.5\n",
        "    self.negative_slope = 0.2\n",
        "    self.activation = F.elu\n",
        "\n",
        "    if self.num_layers > 1:\n",
        "        # input projection (no residual)\n",
        "        self.gat_layers.append(GATConv(self.in_dim, self.num_hidden, self.heads[0], self.feat_drop, self.attn_drop, self.negative_slope, False, self.activation))\n",
        "\n",
        "        # hidden layers\n",
        "        for l in range(1, self.num_layers-1):\n",
        "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "            self.gat_layers.append(GATConv(self.num_hidden *self. heads[l-1], self.num_hidden, self.heads[l], self.feat_drop, self.attn_drop, self.negative_slope, False, self.activation))\n",
        "        # output projection\n",
        "        self.gat_layers.append(GATConv(self.num_hidden * self.heads[-2], num_classes, self.heads[-1], self.feat_drop, self.attn_drop, self.negative_slope, False, None))\n",
        "    \n",
        "    else:\n",
        "        self.gat_layers.append(GATConv(self.in_dim, num_classes, self.heads[0], self.feat_drop, self.attn_drop, self.negative_slope, False, None))\n",
        "  \n",
        "  def forward(self, g, inputs):\n",
        "    h = inputs\n",
        "    for l in range(self.num_layers):\n",
        "        h = self.gat_layers[l](g, h)\n",
        "        h = h.flatten(1) if l != self.num_layers - 1 else h.mean(1)\n",
        "    return h  # (N, C)\n",
        "\n",
        "\n",
        "class HieGAT(torch.nn.Module):\n",
        "  def __init__(self, vocab, in_dim, hidden_dim, num_classes, sen_max_len, device):\n",
        "    super(HieGAT, self).__init__()\n",
        "    self.vocab_size = len(vocab)\n",
        "    self.vocab = vocab\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    self.node_hidden_doclev = torch.nn.Embedding(self.vocab_size, in_dim)  # (num_vocab+1, num_hidden), include 'unk\n",
        "    self.node_hidden_doclev.weight.data.copy_(torch.tensor(self.load_w2v(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/glove.6B/glove.6B.{in_dim}d.w2vformat.txt')))\n",
        "    self.node_hidden_doclev.weight.requires_grad = True\n",
        "\n",
        "    self.node_hidden_senlev = torch.nn.Embedding(self.vocab_size, in_dim)  # (num_vocab+1, num_hidden), include 'unk\n",
        "    self.node_hidden_senlev.weight.data.copy_(torch.tensor(self.load_w2v(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/glove.6B/glove.6B.{in_dim}d.w2vformat.txt')))\n",
        "    self.node_hidden_senlev.weight.requires_grad = True\n",
        "\n",
        "\n",
        "    self.gram = 1\n",
        "\n",
        "    self.device = device\n",
        "    self.sen_max_len = sen_max_len\n",
        "\n",
        "    self.gat_senlev = GATLayerSenLev(in_dim, hidden_dim, num_classes, device)\n",
        "    self.gat_doclev = GATLayerforDocLev(in_dim, hidden_dim, num_classes)\n",
        "   \n",
        "\n",
        "\n",
        "  def load_w2v(self, path):\n",
        "    w2v = word2vec.load(path)\n",
        "    embedding_matrix = []\n",
        "    unk_d = len(w2v['the'])  # unknow word dimension\n",
        "    for word in self.vocab:\n",
        "      try:\n",
        "        embedding_matrix.append(w2v[word])\n",
        "      except KeyError:\n",
        "        embedding_matrix.append(np.zeros(unk_d))\n",
        "    \n",
        "    return np.asarray(embedding_matrix)\n",
        "\n",
        "  def add_edges(self, sample,  local_vocab_id):  # add edges for word-level\n",
        "    edges = []\n",
        "    for i, src in enumerate(sample):\n",
        "      u = local_vocab_id[src]\n",
        "      for j in range(max(0, i-self.gram), min(i+self.gram +1, len(sample))):\n",
        "        dst = sample[j]\n",
        "        v = local_vocab_id[dst]\n",
        "\n",
        "        edges.append([u, v])\n",
        "    return edges\n",
        "\n",
        "  def wordlev2graph(self, sample):  # for  word-level, sample: [78, 63, 63, 33, 78,  ...],\n",
        "    if len(sample) == 0:\n",
        "      raise Exception('sample length is equal 0')\n",
        "    if len(sample)>self.sen_max_len:\n",
        "      sample = sample[:self.sen_max_len]\n",
        "\n",
        "    local_vocab = set(sample)  # {78, 63, 33, ...}\n",
        "\n",
        "    n = len(local_vocab)\n",
        "    local_vocab_id = dict(zip(local_vocab, range(n)))  # {78:0, 63:1, 33:2, ...}\n",
        "    u, v = zip(*self.add_edges(sample, local_vocab_id))\n",
        "    \n",
        "    g = dgl.graph((u, v), num_nodes=n, idtype=torch.int32).to(self.device)\n",
        "\n",
        "    local_vocab_tensor = torch.tensor(list(local_vocab)).to(self.device)\n",
        "\n",
        "    g.ndata['h'] =  self.node_hidden_senlev(local_vocab_tensor)\n",
        "\n",
        "    return g\n",
        "\n",
        "  def doc2graph(self, sample):  # for doc-level, sample: [78, 63, 63, 33, 78,  ...],\n",
        "    if len(sample) == 0:\n",
        "      raise Exception('sample length is equal 0')\n",
        "    if len(sample)>self.sen_max_len:\n",
        "      sample = sample[:self.sen_max_len]\n",
        "\n",
        "    local_vocab = set(sample)  # {78, 63, 33, ...}\n",
        "\n",
        "    n = len(local_vocab)\n",
        "    local_vocab_id = dict(zip(local_vocab, range(n)))  # {78:0, 63:1, 33:2, ...}\n",
        "    u, v = zip(*self.add_edges(sample, local_vocab_id))\n",
        "    \n",
        "    g = dgl.graph((u, v), num_nodes=n, idtype=torch.int32).to(self.device)\n",
        "\n",
        "    local_vocab_tensor = torch.tensor(list(local_vocab)).to(self.device)\n",
        "    g.ndata['h'] =  self.node_hidden_doclev(local_vocab_tensor)\n",
        "\n",
        "    return g\n",
        "\n",
        "  def lamb(self, x):  # lambda parameter for sen-lev and docl-ev out put\n",
        "    lam = []  # d, s , w\n",
        "    t = 1 / ( np.log(x) + 1)\n",
        "    d =  t * 2.0/3.0\n",
        "    s = 1 - t\n",
        "    w = t * 1.0/3.0\n",
        "    lam.append(d)\n",
        "    lam.append(s)\n",
        "    lam.append(w)\n",
        "    return lam\n",
        "\n",
        "\n",
        "  def forward(self, inputs):  # inputs: (one_batch_size, num_sentence, tokens)\n",
        "\n",
        "    all_wor_g = []  # all word level graphs in one batch\n",
        "    all_wor_id = []  # [0, 0, 0, 1,1,1,1,1, 2, 2, 2, ...]\n",
        "    doc_g = [] # inputs for doc level: (batch_size, tokens)\n",
        "\n",
        "    for i, (_x, _) in enumerate(inputs):\n",
        "      # word level graphs\n",
        "      wor_g = [self.wordlev2graph(wor) for wor in _x]\n",
        "      _wor_id = [i for j in range(len(wor_g))]\n",
        "      all_wor_id.extend(_wor_id)\n",
        "      all_wor_g.extend(wor_g)\n",
        "\n",
        "      # doc level graphs\n",
        "      temp = [w for t in _x for w in t]\n",
        "      doc_g.append(self.doc2graph(temp))\n",
        "\n",
        "    # word level\n",
        "    batch_wor_g = dgl.batch(all_wor_g)\n",
        "    h = batch_wor_g.ndata['h']\n",
        "\n",
        "    out_senlev, out_worlev = self.gat_senlev(batch_wor_g, h, all_wor_id)  # (N, C)\n",
        "\n",
        "\n",
        "    # TODO 将sen-lev图和doc-level 图同时批量处理    \n",
        "    \n",
        "    # doc level output\n",
        "    doc_batch_g = dgl.batch(doc_g)\n",
        "    h = (doc_batch_g.ndata['h'])\n",
        "    doc_batch_g.ndata['h'] = self.gat_doclev(doc_batch_g, h)\n",
        "    \n",
        "    out_doclev =  dgl.mean_nodes(doc_batch_g, feat='h')  # [N, C]\n",
        "\n",
        "\n",
        "    average_sen = len(all_wor_id)/len(inputs)\n",
        "    assert average_sen >=1, 'Average num sentence in one batch is less than 1, error!'\n",
        "    lam = self.lamb(average_sen)\n",
        "\n",
        "\n",
        "    total = lam[0]*F.log_softmax(out_doclev, dim=-1) + lam[1]*F.log_softmax(out_senlev, dim=-1) + lam[2]*F.log_softmax(out_worlev, dim=-1)  # (N, C)\n",
        "    #total =F.log_softmax(out_doclev, dim=-1)  # (N, C)\n",
        "    return total\n",
        "\n",
        "\n",
        "def train(model, epoch, input, dev_input, DEVICE, DATASET, label_weight, lr):\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), weight_decay=1e-2, lr=lr)  # Adam\n",
        "\n",
        "  PATIENCE = 8  # Patience on dev set to finish training\n",
        "  no_improve = 0  # No improvement on dev set\n",
        "\n",
        "  best_acc = 0.0\n",
        "  dur = []\n",
        "\n",
        "  for e in range(epoch):\n",
        "    improved = ''\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "\n",
        "    for ba in input:  # Total 80s\n",
        "      t1 = time.time()\n",
        "      y = torch.tensor([label for _, label in ba]).to(DEVICE)  # 0.09s      \n",
        "    \n",
        "      # 0.09s\n",
        "      outputs = model(ba)\n",
        "      loss = F.nll_loss(outputs, y, weight=label_weight)\n",
        "      \n",
        "      loss.backward()  # Derive gradients\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()  # Clear gradients\n",
        "    \n",
        "    val_acc = dev(model, dev_input, DEVICE)  # 5s\n",
        "\n",
        "    if val_acc>best_acc:\n",
        "      best_acc = val_acc\n",
        "      no_improve = 0\n",
        "      improved = '*'\n",
        "      torch.save(model, f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/HieGAT_{DATASET}.pkl')\n",
        "    else:\n",
        "      no_improve+=1\n",
        "    \n",
        "    dur.append(time.time()-t0)\n",
        "    print(f'Epoch: {e}, Train loss:{loss.item():.4f}, Val acc: {val_acc:.4f}, Times: {np.mean(dur):.4f}s, {improved}')\n",
        "\n",
        "    if no_improve>=PATIENCE:\n",
        "      print(f'No improvement on dev set, early stopping')\n",
        "      break\n",
        "\n",
        "def dev(model, input, DEVICE):\n",
        "  model.eval()\n",
        "  total_pred = 0.0\n",
        "  correct = 0.0\n",
        "  for ba in input:\n",
        "    y = torch.tensor([label for _, label in ba]).to(DEVICE)  # 0.05s\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(ba)\n",
        "      pred = torch.argmax(outputs, dim=1)\n",
        "      \n",
        "      correct_pred = torch.sum(pred==y)\n",
        "      correct += correct_pred\n",
        "      total_pred += len(y)\n",
        "  \n",
        "  return torch.div(correct, total_pred)  # Acc on dev set\n",
        "\n",
        "def test(model, input, DEVICE):\n",
        "  model.eval()\n",
        "  total_pred = 0.0\n",
        "  correct = 0.0\n",
        "  for ba in input:\n",
        "    y = torch.tensor([label for _, label in ba]).to(DEVICE)  # 0.05s\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(ba)\n",
        "      pred = torch.argmax(outputs, dim=1)\n",
        "      \n",
        "      correct_pred = torch.sum(pred==y)\n",
        "      correct += correct_pred\n",
        "      total_pred += len(y)\n",
        "  \n",
        "  return torch.div(correct, total_pred)  # Acc on dev set\n",
        "\n",
        "def  buildvocab(sample, min_count=5):\n",
        "  # sample: ['wo xihuan ziran yuyan chuli', 'wo ai shengdu xuexi',  'wo xihuan jiqi xuexi']\n",
        "\n",
        "  MIN_COUNT = min_count\n",
        "\n",
        "  freq = {}\n",
        "  for i in sample:\n",
        "    for t in word_tokenize(i):\n",
        "      if t not in freq:\n",
        "        freq[t] = 0\n",
        "\n",
        "  for i in sample:\n",
        "    for t in word_tokenize(i):\n",
        "      freq[t] +=1\n",
        "\n",
        "  del_key = []\n",
        "  for i in freq:\n",
        "    if freq[i]<MIN_COUNT:\n",
        "      del_key.append(i)\n",
        "  \n",
        "  for i in del_key:\n",
        "    freq.pop(i)\n",
        "\n",
        "  vocab_id = {}\n",
        "  for i, key in enumerate(freq):\n",
        "    vocab_id[key] = i\n",
        "  print(f'vocab_id size: {len(vocab_id)}')\n",
        "  print('*'*50)\n",
        "  \n",
        "  return vocab_id\n",
        "\n",
        "#vocab = {'this':0, 'is':1, 'first':2, 'sentence':3, 'however':4, 'could':5, 'be':6, '<unk>':7, ... ,  '<pad>':100}\n",
        "def w2id(input, vocab):\n",
        "  ids = []\n",
        "  for w in input:\n",
        "    if w in [',', '.', '!', '?']:\n",
        "      continue\n",
        "    if w in vocab:\n",
        "      ids.append(vocab[w])\n",
        "    else: \n",
        "      ids.append(vocab['<unk>'])\n",
        "  return ids\n",
        "\n",
        "def batch_hiegnn(texts, labels, batch_size, label2idx, vocab_id):\n",
        "  x = texts\n",
        "  y = [label2idx[t] for t in labels]\n",
        "  data = [(x[i], y[i]) for i in range(len(y))]\n",
        "  random.shuffle(data)\n",
        "\n",
        "  input = [(sent_tokenize(x), y)  for x, y in data]\n",
        "\n",
        "  input1 = []\n",
        "  for sample, y in input:\n",
        "    t = []\n",
        "    for s in sample:\n",
        "      temp = w2id(word_tokenize(s), vocab_id)\n",
        "      if len(temp) >0:\n",
        "        t.append(temp)\n",
        "    input1.append((t, y))\n",
        "\n",
        "  input2 = [input1[i: i+batch_size] for i in range(0, len(input1), batch_size)]\n",
        "\n",
        "  return input2\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "  print(f'device: {DEVICE}')\n",
        "  print(f'name: {torch.cuda.get_device_name(0)}')\n",
        "  print(f'*'*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWlZdpxji3Rd"
      },
      "source": [
        "# run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "ml-Rx4WzquTz",
        "outputId": "888a896f-8d33-4135-acb0-00e1c2228798"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 400})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: mr, Total train: 7108, Train size: 6397, Dev size: 711, Test size: 3554, Num_class: 2\n",
            "labels: {'0': 0, '1': 1}\n",
            "**************************************************\n",
            "vocab_id size: 4981\n",
            "**************************************************\n",
            "Epoch: 0, Train loss:0.6731, Val acc: 0.6751, Times: 12.0499s, *\n",
            "Epoch: 1, Train loss:0.6512, Val acc: 0.7075, Times: 12.0824s, *\n",
            "Epoch: 2, Train loss:0.6198, Val acc: 0.7243, Times: 12.3656s, *\n",
            "Epoch: 3, Train loss:0.6320, Val acc: 0.7314, Times: 12.2930s, *\n",
            "Epoch: 4, Train loss:0.5869, Val acc: 0.7426, Times: 12.2361s, *\n",
            "Epoch: 5, Train loss:0.5888, Val acc: 0.7595, Times: 12.1998s, *\n",
            "Epoch: 6, Train loss:0.5862, Val acc: 0.7496, Times: 12.1631s, \n",
            "Epoch: 7, Train loss:0.5450, Val acc: 0.7553, Times: 12.1327s, \n",
            "Epoch: 8, Train loss:0.5336, Val acc: 0.7525, Times: 12.1053s, \n",
            "Epoch: 9, Train loss:0.5661, Val acc: 0.7595, Times: 12.0958s, \n",
            "Epoch: 10, Train loss:0.4941, Val acc: 0.7693, Times: 12.1752s, *\n",
            "Epoch: 11, Train loss:0.4806, Val acc: 0.7679, Times: 12.1564s, \n",
            "Epoch: 12, Train loss:0.5094, Val acc: 0.7693, Times: 12.1378s, \n",
            "Epoch: 13, Train loss:0.4382, Val acc: 0.7637, Times: 12.1225s, \n",
            "Epoch: 14, Train loss:0.4539, Val acc: 0.7707, Times: 12.1158s, *\n",
            "Epoch: 15, Train loss:0.4377, Val acc: 0.7750, Times: 12.1061s, *\n",
            "Epoch: 16, Train loss:0.4241, Val acc: 0.7679, Times: 12.1039s, \n",
            "Epoch: 17, Train loss:0.4003, Val acc: 0.7679, Times: 12.0935s, \n",
            "Epoch: 18, Train loss:0.4325, Val acc: 0.7679, Times: 12.1303s, \n",
            "Epoch: 19, Train loss:0.3947, Val acc: 0.7679, Times: 12.1189s, \n",
            "Epoch: 20, Train loss:0.3508, Val acc: 0.7707, Times: 12.1042s, \n",
            "Epoch: 21, Train loss:0.3744, Val acc: 0.7707, Times: 12.0991s, \n",
            "Epoch: 22, Train loss:0.3548, Val acc: 0.7693, Times: 12.0915s, \n",
            "Epoch: 23, Train loss:0.3977, Val acc: 0.7707, Times: 12.0828s, \n",
            "No improvement on dev set, early stopping\n",
            "Test accuracy: 0.7704\n",
            "Time: 5.7m, min_count=3\n",
            "**************************************************\n",
            "Epoch: 0, Train loss:0.6626, Val acc: 0.6048, Times: 12.0059s, *\n",
            "Epoch: 1, Train loss:0.6439, Val acc: 0.6850, Times: 12.0402s, *\n",
            "Epoch: 2, Train loss:0.6123, Val acc: 0.7229, Times: 12.0544s, *\n",
            "Epoch: 3, Train loss:0.6119, Val acc: 0.7314, Times: 12.0446s, *\n",
            "Epoch: 4, Train loss:0.5823, Val acc: 0.7342, Times: 12.0410s, *\n",
            "Epoch: 5, Train loss:0.5921, Val acc: 0.7567, Times: 12.1846s, *\n",
            "Epoch: 6, Train loss:0.5824, Val acc: 0.7679, Times: 12.1722s, *\n",
            "Epoch: 7, Train loss:0.5291, Val acc: 0.7665, Times: 12.1565s, \n",
            "Epoch: 8, Train loss:0.5375, Val acc: 0.7778, Times: 12.1348s, *\n",
            "Epoch: 9, Train loss:0.4973, Val acc: 0.7778, Times: 12.1146s, \n",
            "Epoch: 10, Train loss:0.5485, Val acc: 0.7764, Times: 12.0923s, \n",
            "Epoch: 11, Train loss:0.5088, Val acc: 0.7722, Times: 12.0764s, \n",
            "Epoch: 12, Train loss:0.4931, Val acc: 0.7750, Times: 12.0621s, \n",
            "Epoch: 13, Train loss:0.4515, Val acc: 0.7736, Times: 12.1148s, \n",
            "Epoch: 14, Train loss:0.4225, Val acc: 0.7778, Times: 12.1101s, \n",
            "Epoch: 15, Train loss:0.4352, Val acc: 0.7778, Times: 12.0949s, \n",
            "Epoch: 16, Train loss:0.4318, Val acc: 0.7707, Times: 12.0829s, \n",
            "No improvement on dev set, early stopping\n",
            "Test accuracy: 0.7459\n",
            "Time: 4.4m, min_count=3\n",
            "**************************************************\n",
            "Epoch: 0, Train loss:0.6777, Val acc: 0.6596, Times: 12.5540s, *\n",
            "Epoch: 1, Train loss:0.6372, Val acc: 0.7004, Times: 12.2920s, *\n",
            "Epoch: 2, Train loss:0.6166, Val acc: 0.7271, Times: 12.1731s, *\n",
            "Epoch: 3, Train loss:0.6248, Val acc: 0.7567, Times: 12.1344s, *\n",
            "Epoch: 4, Train loss:0.6127, Val acc: 0.7539, Times: 12.0966s, \n",
            "Epoch: 5, Train loss:0.5761, Val acc: 0.7567, Times: 12.0583s, \n",
            "Epoch: 6, Train loss:0.5693, Val acc: 0.7651, Times: 12.0666s, *\n",
            "Epoch: 7, Train loss:0.5669, Val acc: 0.7665, Times: 12.1232s, *\n",
            "Epoch: 8, Train loss:0.5609, Val acc: 0.7764, Times: 12.1512s, *\n",
            "Epoch: 9, Train loss:0.4996, Val acc: 0.7792, Times: 12.1353s, *\n",
            "Epoch: 10, Train loss:0.4974, Val acc: 0.7736, Times: 12.1178s, \n",
            "Epoch: 11, Train loss:0.4885, Val acc: 0.7722, Times: 12.1076s, \n",
            "Epoch: 12, Train loss:0.4817, Val acc: 0.7764, Times: 12.0949s, \n",
            "Epoch: 13, Train loss:0.4908, Val acc: 0.7806, Times: 12.0962s, *\n",
            "Epoch: 14, Train loss:0.4340, Val acc: 0.7792, Times: 12.0858s, \n",
            "Epoch: 15, Train loss:0.4368, Val acc: 0.7736, Times: 12.0989s, \n",
            "Epoch: 16, Train loss:0.4286, Val acc: 0.7764, Times: 12.1176s, \n",
            "Epoch: 17, Train loss:0.4952, Val acc: 0.7679, Times: 12.1025s, \n",
            "Epoch: 18, Train loss:0.4067, Val acc: 0.7679, Times: 12.0919s, \n",
            "Epoch: 19, Train loss:0.4144, Val acc: 0.7665, Times: 12.0889s, \n",
            "Epoch: 20, Train loss:0.4315, Val acc: 0.7707, Times: 12.0806s, \n",
            "Epoch: 21, Train loss:0.3315, Val acc: 0.7707, Times: 12.0696s, \n",
            "No improvement on dev set, early stopping\n",
            "Test accuracy: 0.7645\n",
            "Time: 5.3m, min_count=3\n",
            "**************************************************\n",
            "====================================================================================================\n",
            "Total time: 15.5071mins\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 400})'''))\n",
        "\n",
        "\n",
        "def LabelWeight(train_label, label2idx, dataset_name):  # for r8, r52, oh\n",
        "  NAME = dataset_name\n",
        "\n",
        "  if NAME in []:  # \"r8\", \"r52\" and \"oh\" are imbalanced\n",
        "    y = np.asarray([label2idx[t] for t in train_label])\n",
        "\n",
        "    class_weights=torch.tensor(sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y),y=y), dtype=torch.float)\n",
        "    return class_weights.to(DEVICE)\n",
        "  else: \n",
        "    return None\n",
        "\n",
        "\n",
        "def ParaTuning(DATASET, SEN_MAX_LEN, DEVICE, EPOCH, lr, MIN_COUNT):  # Parameters tuning\n",
        "\n",
        "  para = []  # parameters\n",
        "  acc = []  # accuracy\n",
        "  dur = []  # time\n",
        "\n",
        "  train_texts,  train_labels, dev_texts, dev_labels, test_texts, test_labels, label2idx = MyDatasets(DATASET)\n",
        "  NUM_CLASS = len(label2idx)\n",
        "  \n",
        "  vocab_id = buildvocab(train_texts, min_count=MIN_COUNT)\n",
        "  vocab_id['<unk>']=len(vocab_id)  # for OOV\n",
        "\n",
        "  BATCH_SIZE=128\n",
        "  train_batch = batch_hiegnn(train_texts, train_labels, BATCH_SIZE, label2idx, vocab_id)\n",
        "  dev_batch = batch_hiegnn(dev_texts, dev_labels, BATCH_SIZE, label2idx, vocab_id)\n",
        "  test_batch = batch_hiegnn(test_texts, test_labels, BATCH_SIZE, label2idx, vocab_id)\n",
        "\n",
        "  label_weight = LabelWeight(train_labels, label2idx, DATASET)\n",
        "\n",
        "  for i in range(3):\n",
        "    tt = time.time()\n",
        "    model = HieGAT(vocab_id, in_dim=300, hidden_dim=300,  num_classes=NUM_CLASS, sen_max_len=SEN_MAX_LEN, device=DEVICE)\n",
        "    model.to(DEVICE)\n",
        "  \n",
        "    train(model, EPOCH, train_batch, dev_batch, DEVICE, DATASET,  label_weight=label_weight, lr=lr)\n",
        "\n",
        "    best_model = torch.load(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/HieGAT_{DATASET}.pkl')\n",
        "    res = test(best_model, test_batch, DEVICE)\n",
        "    print(f'Test accuracy: {res.cpu().numpy():.4f}')\n",
        "\n",
        "    print(f'Time: {(time.time()-tt)/60:.1f}m, min_count={MIN_COUNT}')\n",
        "    print(f'*'*50)\n",
        "\n",
        "    acc.append(res.cpu().numpy())\n",
        "    dur.append((time.time()-tt)/60)\n",
        "    para.append(f'lr={lr}, min_count={MIN_COUNT}')\n",
        "\n",
        "  # comput average\n",
        "  acc.append(np.mean(acc))\n",
        "  dur.append(np.mean(dur))\n",
        "  para.append('average:')\n",
        "\n",
        "  df = pd.concat([pd.DataFrame({'para': para}), pd.DataFrame({'acc': acc}), pd.DataFrame({'dur': dur})], axis=1)\n",
        "\n",
        "  df.to_csv(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/HieGAT-{DATASET}.csv')\n",
        "\n",
        "\n",
        "# Parameters\n",
        "\n",
        "EPOCH = 100\n",
        "SEN_MAX_LEN = 500  # sentence max length\n",
        "\n",
        "min_c = {'20ng':5, 'r8': 3, 'r52':3,  'oh':1, 'mr':1,}  # mini count in vocab\n",
        "lr = {'20ng':1e-3, 'r8': 1e-3, 'r52':1e-3,  'oh':1e-3, 'mr':1e-4, }  # learning rate\n",
        "\n",
        "dataset = ['20ng', 'r8',  'r52', 'oh', 'mr', ]\n",
        "\n",
        "\n",
        "t0 = time.time()\n",
        "for DATASET in dataset:\n",
        "\n",
        "  if DATASET not in ['mr']: continue  # check for specific dataset\n",
        "\n",
        "  MIN_COUNT = min_c[DATASET]\n",
        "  LR = lr[DATASET]\n",
        "\n",
        "  ParaTuning(DATASET, SEN_MAX_LEN, DEVICE, EPOCH,  LR,  MIN_COUNT)\n",
        "  print('='*100)\n",
        "\n",
        "print(f'Total time: {(time.time()-t0)/60:.4f}mins')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRrhWlZCi4Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d0bfe84-99a5-4adc-82e5-58f3a8022ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T0: 0.4392988681793213\n",
            "T2: 0.20152521133422852\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "a = [i for i in range(1000000)]\n",
        "\n",
        "t0 = time.time()\n",
        "count = 0\n",
        "for i in range(len(a)):\n",
        "  count +=1\n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "t2 = time.time()\n",
        "for i, _ in enumerate(a):\n",
        "  j = i\n",
        "t3 = time.time()\n",
        "\n",
        "print(f'T0: {t1-t0}')\n",
        "print(f'T2: {t3-t2}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HieGAT",
      "toc_visible": true,
      "provenance": [],
      "mount_file_id": "11ICgvrCuyS8kQHYvsX4QzAMK4mdNs-i2",
      "authorship_tag": "ABX9TyOEb0WFWgalr23NaaZgzpRG",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}