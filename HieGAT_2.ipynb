{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hshuai97/Colab20210803/blob/main/HieGAT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FasnA_mVigIf"
      },
      "source": [
        "1. 改写于官方DGL库的GAT实现代码，[GAT代码链接](https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html)；[训练代码](https://github.com/dmlc/dgl/blob/master/examples/pytorch/gat/train.py)\n",
        "\n",
        "2. 使用纯净的GAT, 在word-level上使用GAT，在sen-level上使用GAT, 在doc-level上使用GAT，并使用三个独立的参数将三个输出整合\n",
        "\n",
        "3. 其中word-level将输出结果不分顺序直接相加表示所在样本的向量表示；sen-level则将word-level的输出再进行一次GAT，将输出结果作为所在样本的向量表示；doc-level直接在原样本上进行GAT，并将输出结果作为样本的向量表示\n",
        "\n",
        "4. 使用自定义的GAT结构来处理sen-level上的特征"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbK7EFLcirun"
      },
      "source": [
        "# install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aIClaKj9iUSC",
        "outputId": "7ad8d1d1-79ff-4204-8fb5-9f3dba34dc53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl-cu113\n",
            "  Downloading https://data.dgl.ai/wheels/dgl_cu113-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (220.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 220.6 MB 44 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (1.21.6)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (2.6.3)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[K     |████████████████████████████████| 281 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (4.64.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (2.10)\n",
            "Installing collected packages: psutil, dgl-cu113\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed dgl-cu113-0.8.2 psutil-5.9.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting word2vec\n",
            "  Downloading word2vec-0.11.1.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 603 kB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.21.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.1.0)\n",
            "Building wheels for collected packages: word2vec\n",
            "  Building wheel for word2vec (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2vec: filename=word2vec-0.11.1-py2.py3-none-any.whl size=156431 sha256=0c3c2c8d779811924199ada8dce08bb9aa15dd2eb8977469a58dfe663313ee29\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/c0/d4/29d797817e268124a32b6cf8beb8b8fe87b86f099d5a049e61\n",
            "Successfully built word2vec\n",
            "Installing collected packages: word2vec\n",
            "Successfully installed word2vec-0.11.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 29.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "try:\n",
        "  import dgl\n",
        "except ModuleNotFoundError:\n",
        "  CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "  !pip install dgl-{CUDA} -f https://data.dgl.ai/wheels/repo.html\n",
        "\n",
        "try:\n",
        "  import word2vec\n",
        "except ModuleNotFoundError:\n",
        "  !pip install word2vec\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "try:\n",
        "  import transformers\n",
        "except ModuleNotFoundError:\n",
        "  !pip install transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms09x9hcivhD"
      },
      "source": [
        "# parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah1oUMt8RHJG",
        "outputId": "39fbabd3-5d72-400e-f5bb-0b3cb86d13f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "import word2vec\n",
        "\n",
        "import dgl\n",
        "from dgl.nn import GATConv\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "def Data(dataset_name):\n",
        "    NAME = dataset_name\n",
        "    if NAME not in ['20ng', 'r8', 'r52', 'oh', 'mr']:\n",
        "      raise ValueError('The dataset is not support')\n",
        "\n",
        "    PATH = '/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/data/'+NAME\n",
        "\n",
        "    train_texts = []\n",
        "    train_labels = []\n",
        "    with open(os.path.join(PATH, NAME+'-train-stemmed.txt'), 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        for line in data:\n",
        "          t = line.split('\\t')\n",
        "          train_texts.append(t[1])\n",
        "          train_labels.append(t[0])\n",
        "\n",
        "    dev_texts = []\n",
        "    dev_labels = []\n",
        "    with open(os.path.join(PATH, NAME+'-dev-stemmed.txt'), 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        for line in data:\n",
        "          t = line.split('\\t')\n",
        "          dev_texts.append(t[1])\n",
        "          dev_labels.append(t[0])\n",
        "\n",
        "    test_texts = []\n",
        "    test_labels = []\n",
        "    with open(os.path.join(PATH, NAME+'-test-stemmed.txt'), 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        for line in data:\n",
        "          t = line.split('\\t')\n",
        "          test_texts.append(t[1])\n",
        "          test_labels.append(t[0])\n",
        "\n",
        "    target_names = list(set(train_labels))\n",
        "    label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
        "\n",
        "    print(f'Dataset: {NAME}, Total train: {len(train_texts)+len(dev_texts)}, Train size: {len(train_texts)}, Dev size: {len(dev_texts)}, Test size: {len(test_texts)}, Num_class: {len(label2idx)}')\n",
        "    print(f'labels: {label2idx}')\n",
        "    print('*'*50)\n",
        "\n",
        "    return train_texts,  train_labels, dev_texts, dev_labels, test_texts, test_labels, label2idx\n",
        "\n",
        "class GATLayer(nn.Module):  # word-level or doc-level\n",
        "    def __init__(self,\n",
        "                 num_layers=1,\n",
        "                 in_dim=300,\n",
        "                 num_hidden=300,\n",
        "                 out_dim=300,\n",
        "                 heads=[1],\n",
        "                 activation=F.elu,\n",
        "                 feat_drop=.6,\n",
        "                 attn_drop=.6,\n",
        "                 negative_slope=.2,\n",
        "                 residual=False):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        if num_layers > 1:\n",
        "        # input projection (no residual)\n",
        "            self.gat_layers.append(GATConv(\n",
        "                in_dim, num_hidden, heads[0],\n",
        "                feat_drop, attn_drop, negative_slope, False, self.activation))\n",
        "            # hidden layers\n",
        "            for l in range(1, num_layers-1):\n",
        "                # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "                self.gat_layers.append(GATConv(\n",
        "                    num_hidden * heads[l-1], num_hidden, heads[l],\n",
        "                    feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
        "            # output projection\n",
        "            self.gat_layers.append(GATConv(\n",
        "                num_hidden * heads[-2], out_dim, heads[-1],\n",
        "                feat_drop, attn_drop, negative_slope, residual, None))\n",
        "        else:\n",
        "            self.gat_layers.append(GATConv(\n",
        "                in_dim, out_dim, heads[0],\n",
        "                feat_drop, attn_drop, negative_slope, residual, None))\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        for l in range(self.num_layers):\n",
        "            h = self.gat_layers[l](g, h)\n",
        "            h = h.flatten(1) if l != self.num_layers - 1 else h.mean(1)  # last ouput layer is 'mean' operation\n",
        "        return h  # (num_nodes, out_dim)\n",
        "\n",
        "class GATLayer_SenLev(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(GATLayer_SenLev, self).__init__()\n",
        "        # equation (1)\n",
        "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        # equation (2)\n",
        "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
        "        self.reset_parameters()\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)       \n",
        "\n",
        "    def edge_attention(self, edges):\n",
        "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
        "        a = self.attn_fc(z2)\n",
        "        return {'e': F.leaky_relu(a)}\n",
        "    \n",
        "    def message_func(self, edges):\n",
        "      return {'z': edges.src['z'], 'e': edges.data['e']}\n",
        "    \n",
        "    def reduce_func(self, nodes):\n",
        "      alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "      h = torch.sum(alpha*nodes.mailbox['z'], dim=1)\n",
        "      return {'h': h}\n",
        "    \n",
        "    def forward(self, g, h):\n",
        "      z = self.fc(h)\n",
        "      g.ndata['z'] = z\n",
        "      g.apply_edges(self.edge_attention)\n",
        "      g.update_all(self.message_func, self.reduce_func)\n",
        "      return g.ndata.pop('h')  # return updated graph node feature, (num_nodes, out_dim)\n",
        "\n",
        "class HieGAT(torch.nn.Module):\n",
        "  def __init__(self, vocab, in_dim, out_dim, num_classes, sen_max_len, num_layers, num_heads, k, device):\n",
        "    super(HieGAT, self).__init__()\n",
        "    self.vocab_size = len(vocab)\n",
        "    self.vocab = vocab\n",
        "\n",
        "    self.node_hidden = torch.nn.Embedding(self.vocab_size, 300)  # (num_vocab+1, num_hidden), include 'unk\n",
        "    self.node_hidden.weight.data.copy_(torch.tensor(self.load_w2v('/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/glove.6B/glove.6B.300d.w2vformat.txt')))\n",
        "    self.node_hidden.weight.requires_grad = True\n",
        "\n",
        "    self.gram = 2\n",
        "    self.SEN_GRAM = 2\n",
        "    self.DEVICE = device\n",
        "    self.sen_max_len = sen_max_len\n",
        "\n",
        "    self.k = k\n",
        "\n",
        "\n",
        "    self.gat = GATLayer(in_dim=in_dim, out_dim=out_dim, num_layers=num_layers, heads=[num_heads]*(num_layers-1)+[1], residual=False)\n",
        "    self.gat_senlev = GATLayer_SenLev(in_dim=in_dim, out_dim=out_dim)\n",
        "    self.linear = torch.nn.Linear(out_dim, num_classes)\n",
        "\n",
        "  def load_w2v(self, path):\n",
        "    w2v = word2vec.load(path)\n",
        "    embedding_matrix = []\n",
        "    for word in self.vocab:\n",
        "      try:\n",
        "        embedding_matrix.append(w2v[word])\n",
        "      except KeyError:\n",
        "        embedding_matrix.append(np.zeros(len(w2v['the'])))\n",
        "    \n",
        "    embedding_matrix = np.array(embedding_matrix)\n",
        "    \n",
        "    return embedding_matrix\n",
        "\n",
        "  def add_edges(self, sample,  local_vocab_id):  # add edges for word-level\n",
        "    edges = []\n",
        "    for i, src in enumerate(sample):\n",
        "      u = local_vocab_id[src]\n",
        "      for j in range(max(0, i-self.gram), min(i+self.gram +1, len(sample))):\n",
        "        dst = sample[j]\n",
        "        v = local_vocab_id[dst]\n",
        "\n",
        "        edges.append([u, v])\n",
        "    return edges\n",
        "\n",
        "  def sample2graph(self, sample):  # sample: [78, 63, 63, 33, 78,  ...], Type: list\n",
        "    if len(sample) == 0:\n",
        "      raise Exception('sample length is equal 0')\n",
        "    if len(sample)>self.sen_max_len:\n",
        "      sample = sample[:self.sen_max_len]\n",
        "\n",
        "    local_vocab = set(sample)  # {78, 63, 33, ...}\n",
        "\n",
        "    n = len(local_vocab)\n",
        "    local_vocab_id = dict(zip(local_vocab, range(n)))  # {78:0, 63:1, 33:2, ...}\n",
        "    u, v = zip(*self.add_edges(sample, local_vocab_id))\n",
        "    u = torch.tensor(u).to(self.DEVICE)\n",
        "    v = torch.tensor(v).to(self.DEVICE)\n",
        "\n",
        "    g = dgl.graph((u, v), num_nodes=n).to(self.DEVICE)\n",
        "\n",
        "    local_vocab_tensor = torch.tensor(list(local_vocab)).to(self.DEVICE)\n",
        "\n",
        "    g.ndata['z'] =  self.node_hidden(local_vocab_tensor)\n",
        "\n",
        "    return g\n",
        "\n",
        "  def add_edges_sen(self, n): # add edges for sentence-level\n",
        "    edges = []\n",
        "    for i in range(n):\n",
        "        u = i\n",
        "        for j in range(max(0, i-self.SEN_GRAM), min(i+self.SEN_GRAM+1, n)):\n",
        "            v = j\n",
        "            # - first connect the new sub_graph\n",
        "            edges.append([u, v])\n",
        "    return edges\n",
        "\n",
        "  def sentence2graph(self, inputs):\n",
        "    n = len(inputs)  # Number of sentence in each sample\n",
        "    edges = self.add_edges_sen(n) \n",
        "    u, v = zip(*edges)\n",
        "    \n",
        "    u = torch.tensor(u).to(self.DEVICE)\n",
        "    v = torch.tensor(v).to(self.DEVICE)\n",
        "\n",
        "    g = dgl.graph((u, v), num_nodes=n).to(self.DEVICE)\n",
        "    g.ndata['z'] = inputs.to(self.DEVICE)\n",
        "\n",
        "    return g\n",
        "\n",
        "  def forward(self, inputs):  # inputs: (batch_size, num_sentence, tokens)\n",
        "    senlev_g = []\n",
        "    worlev_out = []\n",
        "    for token,_ in inputs:  # one sample\n",
        "      list_g = [self.sample2graph(sen)  for sen in token]\n",
        "      tg = dgl.batch(list_g)  # temp graph to store one sample\n",
        "      w_h = self.gat(tg, tg.ndata['z'] )\n",
        "      tg.ndata['z'] = w_h.to(self.DEVICE)  # Update h\n",
        "\n",
        "      sen_g = dgl.mean_nodes(tg, feat='z')  # word-level output: (num_sentence, hidden_dim)\n",
        "      worlev_out.append(torch.mean(sen_g, dim=0).unsqueeze(dim=0))\n",
        "      \n",
        "      s_g = self.sentence2graph(sen_g)  # sen-level graph\n",
        "      senlev_g.append(s_g)\n",
        "\n",
        "    doc_inputs = [] # inputs: (batch_size, tokens)\n",
        "    for x,_ in inputs:\n",
        "      temp = [w for t in x for w in t]\n",
        "      doc_inputs.append(temp)\n",
        "    doc_g = [self.sample2graph(d)  for d in doc_inputs]\n",
        "    batch_dg = dgl.batch(doc_g)\n",
        "    d_h = self.gat_senlev(batch_dg, batch_dg.ndata['z'])\n",
        "    batch_dg.ndata['z'] = d_h\n",
        "\n",
        "    sen_g = dgl.batch(senlev_g)\n",
        "    sen_h = self.gat(sen_g, sen_g.ndata['z'])\n",
        "    sen_g.ndata['z'] = sen_h.to(self.DEVICE)  # (batch_size, hidden_dim)\n",
        "\n",
        "\n",
        "    out_wor = torch.cat(worlev_out, dim=0)  # word-level output: (batch_size, hidden_dim)\n",
        "    out_sen = dgl.mean_nodes(sen_g, feat='z')  # sen-level output: \n",
        "    out_doc = dgl.mean_nodes(batch_dg, feat='z')  # doc-level output: (batch_size, hidden_dim)\n",
        "\n",
        "    total = self.k[0]*out_wor + self.k[1]*out_sen + self.k[2]*out_doc\n",
        "\n",
        "    return self.linear(total)\n",
        "\n",
        "\n",
        "def train(model, epoch, input, dev_input, DEVICE, DATASET):\n",
        "  num_train_steps = len(input) * epoch\n",
        "  num_warmup_steps = int(0.15 * num_train_steps)\n",
        "\n",
        "  loss_func = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4, lr=1e-4)\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps = num_train_steps)\n",
        "\n",
        "  PATIENCE = 8  # Patience on dev set to finish training\n",
        "  no_improve = 0  # No improvement on dev set\n",
        "\n",
        "  best_acc = 0.0\n",
        "  dur = []\n",
        "\n",
        "  for e in range(epoch):\n",
        "    t0 = time.time()\n",
        "    improved = ''\n",
        "    model.train()\n",
        "\n",
        "    for i, ba in enumerate(input):\n",
        "      y = []\n",
        "      for _,label in ba:\n",
        "        y.append(label)\n",
        "      y = torch.tensor(y).to(DEVICE)\n",
        "\n",
        "      outputs = model(ba)\n",
        "      loss = loss_func(outputs, y)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "\n",
        "    val_acc = dev(model, dev_input, DEVICE)\n",
        "    if val_acc>best_acc:\n",
        "      best_acc = val_acc\n",
        "      no_improve = 0\n",
        "      improved = '*'\n",
        "      torch.save(model, f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/hiegat_{DATASET}.pkl')\n",
        "    else:\n",
        "      no_improve+=1\n",
        "    dur.append(time.time()-t0)\n",
        "    print(f'Epoch: {e}, Train loss:{loss.item():.4f}, Val acc: {val_acc:.4f}, Times: {np.mean(dur):.4f}s, {improved}')\n",
        "\n",
        "    if no_improve>=PATIENCE:\n",
        "      print(f'No improvement on dev set, early stopping')\n",
        "      break\n",
        "\n",
        "def dev(model, input, DEVICE):\n",
        "  model.eval()\n",
        "  total_pred = 0.0\n",
        "  correct = 0.0\n",
        "  for i, ba in enumerate(input):\n",
        "    y = []\n",
        "    for _,label in ba:\n",
        "      y.append(label)\n",
        "    y = torch.tensor(y).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(ba)\n",
        "      pred = torch.argmax(outputs, dim=1)\n",
        "      \n",
        "      correct_pred = torch.sum(pred==y)\n",
        "      correct += correct_pred\n",
        "      total_pred += len(y)\n",
        "  \n",
        "  return torch.div(correct, total_pred)  # Acc on dev set\n",
        "\n",
        "def test(model, input, DEVICE):\n",
        "  model.eval()\n",
        "  total_pred = 0.0\n",
        "  correct = 0.0\n",
        "  for i, ba in enumerate(input):\n",
        "    y = []\n",
        "    for _,label in ba:\n",
        "      y.append(label)\n",
        "    y = torch.tensor(y).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(ba)\n",
        "      pred = torch.argmax(outputs, dim=1)\n",
        "      \n",
        "      correct_pred = torch.sum(pred==y)\n",
        "      correct += correct_pred\n",
        "      total_pred += len(y)\n",
        "  \n",
        "  return torch.div(correct, total_pred)  # Acc on dev set\n",
        "\n",
        "def  buildvocab(sample, min_count=5):\n",
        "  '''\n",
        "  sample: ['wo xihuan ziran yuyan chuli', 'wo ai shengdu xuexi',  'wo xihuan jiqi xuexi']\n",
        "\n",
        "  '''\n",
        "\n",
        "  MIN_COUNT = min_count\n",
        "\n",
        "  freq = {}\n",
        "  for i in sample:\n",
        "    for t in word_tokenize(i):\n",
        "      if t not in freq:\n",
        "        freq[t] = 0\n",
        "\n",
        "  for i in sample:\n",
        "    for t in word_tokenize(i):\n",
        "      freq[t] +=1\n",
        "\n",
        "  del_key = []\n",
        "  for i in freq:\n",
        "    if freq[i]<MIN_COUNT:\n",
        "      del_key.append(i)\n",
        "  \n",
        "  for i in del_key:\n",
        "    freq.pop(i)\n",
        "\n",
        "  vocab_id = {}\n",
        "  for i, key in enumerate(freq):\n",
        "    vocab_id[key] = i\n",
        "  print(f'vocab_id size: {len(vocab_id)}')\n",
        "  print('*'*50)\n",
        "  \n",
        "  return vocab_id\n",
        "\n",
        "#vocab = {'this':0, 'is':1, 'first':2, 'sentence':3, 'however':4, 'could':5, 'be':6, '<unk>':7, '<pad>':100}\n",
        "def w2id(input, vocab):\n",
        "  ids = []\n",
        "  for w in input:\n",
        "    if w in [',', '.', '!', '?']:\n",
        "      continue\n",
        "    if w in vocab:\n",
        "      ids.append(vocab[w])\n",
        "    else: \n",
        "      ids.append(vocab['<unk>'])\n",
        "  return ids\n",
        "\n",
        "def batch_hiegnn(texts, labels, batch_size, label2idx, vocab_id):\n",
        "  x = texts\n",
        "  y = [label2idx[t] for t in labels]\n",
        "  data = [(x[i], y[i]) for i in range(len(y))]\n",
        "  random.shuffle(data)\n",
        "\n",
        "  input = [(sent_tokenize(x), y)  for x, y in data]\n",
        "\n",
        "  input1 = []\n",
        "  for sample, y in input:\n",
        "    t = []\n",
        "    for s in sample:\n",
        "      temp = w2id(word_tokenize(s), vocab_id)\n",
        "      if len(temp) >0:\n",
        "        t.append(temp)\n",
        "    input1.append((t, y))\n",
        "\n",
        "  input2 = [input1[i: i+batch_size] for i in range(0, len(input1), batch_size)]\n",
        "\n",
        "  return input2\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dataset', required=True, type=str, default='mr',help='dataset name')\n",
        "parser.add_argument('--sen_max_len', required=True, type=int, default=100)\n",
        "parser.add_argument('--epoch', required=True, type=int, default=50)\n",
        "args = parser.parse_args()\n",
        "\n",
        "DATASET = args.dataset\n",
        "SEN_MAX_LEN = args.sen_max_len\n",
        "EPOCH = args.epoch\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "  print(f'device: {DEVICE}')\n",
        "  print(f'name: {torch.cuda.get_device_name(0)}')\n",
        "  print(f'memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}')\n",
        "  print(f'*'*50)\n",
        "\n",
        "\n",
        "# Parameters\n",
        "NUM_HEADS = [2]\n",
        "NUM_LAYER = [2,3]\n",
        "MIN_COUNT = [1,5] \n",
        "K = [ [0.1, 0.1, 0.8], [0.8, 0.1, 0.6], [0.3, 0.3, 0.3], [0.8, 0.8, 0.8]]  # lamda1, lamda2, lamda3\n",
        "\n",
        "def ParaTuning(DATASET, SEN_MAX_LEN, DEVICE, EPOCH, p1, p2, p3, p4):  # Parameters tuning\n",
        "  NUM_HEADS = p1\n",
        "  NUM_LAYER = p2\n",
        "  MIN_COUNT = p3\n",
        "  K = p4\n",
        "\n",
        "  comp_time = 0\n",
        "  total_comp = len(p1) * len(p2) * len(p3) *  len(p4)\n",
        "\n",
        "  para = []\n",
        "  acc = []\n",
        "  dur = []\n",
        "\n",
        "  train_texts,  train_labels, dev_texts, dev_labels, test_texts, test_labels, label2idx = Data(DATASET)\n",
        "  NUM_CLASS = len(label2idx)\n",
        "  \n",
        "\n",
        "  for min_count in MIN_COUNT:\n",
        "    vocab_id = buildvocab(train_texts, min_count=min_count)\n",
        "    vocab_id['<unk>']=len(vocab_id)  # for OOV\n",
        "\n",
        "    for num_head in NUM_HEADS:\n",
        "        BATCH_SIZE=64\n",
        "        train_batch = batch_hiegnn(train_texts, train_labels, BATCH_SIZE, label2idx, vocab_id)\n",
        "        dev_batch = batch_hiegnn(dev_texts, dev_labels, BATCH_SIZE, label2idx, vocab_id)\n",
        "        test_batch = batch_hiegnn(test_texts, test_labels, BATCH_SIZE, label2idx, vocab_id)\n",
        "\n",
        "        for num_layer in NUM_LAYER:\n",
        "          for k in K:\n",
        "              tt = time.time()\n",
        "              model = HieGAT(vocab_id,  in_dim=300, out_dim=300,  num_classes=NUM_CLASS, sen_max_len=SEN_MAX_LEN, num_layers=num_layer, num_heads=num_head, k= k,  device=DEVICE)\n",
        "              model.to(DEVICE)\n",
        "\n",
        "              train(model, EPOCH, train_batch, dev_batch, DEVICE, DATASET)\n",
        "\n",
        "              best_model = torch.load(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/hiegat_{DATASET}.pkl')\n",
        "              res = test(best_model, test_batch, DEVICE)\n",
        "              print(f'Test accuracy: {res.cpu().numpy():.4f}')\n",
        "\n",
        "              comp_time+=1\n",
        "              print(f'Run {comp_time}/{total_comp}, Time: {(time.time()-tt)/60:.1f}m, min_count={min_count}, num_head={num_head}, num_layer={num_layer}, kw={k[0]}, ks={k[1]}, kd={k[2]}')\n",
        "              print(f'*'*50)\n",
        "\n",
        "              acc.append(res.cpu().numpy())\n",
        "              dur.append((time.time()-tt)/60)\n",
        "              para.append(f'min_count={min_count}, num_head={num_head}, num_layer={num_layer}, kw={k[0]}, ks={k[1]}, kd={k[2]}')\n",
        "\n",
        "    \n",
        "  df = pd.concat([pd.DataFrame({'para': para}), pd.DataFrame({'acc': acc}), pd.DataFrame({'dur': dur})], axis=1)\n",
        "\n",
        "  df.to_csv(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/3-type-levels-{DATASET}.csv')\n",
        "\n",
        "ParaTuning(DATASET, SEN_MAX_LEN, DEVICE, EPOCH, NUM_HEADS, NUM_LAYER,  MIN_COUNT, K)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWlZdpxji3Rd"
      },
      "source": [
        "# run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRrhWlZCi4Kt",
        "outputId": "b3405155-1e88-49e4-b98c-73c69398c51f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda:0\n",
            "name: Tesla P100-PCIE-16GB\n",
            "memory: 17.1\n",
            "**************************************************\n",
            "Dataset: oh, Total train: 3357, Train size: 3021, Dev size: 336, Test size: 4043, Num_class: 23\n",
            "labels: {'C07': 0, 'C15': 1, 'C08': 2, 'C23': 3, 'C09': 4, 'C17': 5, 'C10': 6, 'C13': 7, 'C19': 8, 'C20': 9, 'C02': 10, 'C12': 11, 'C22': 12, 'C14': 13, 'C21': 14, 'C11': 15, 'C05': 16, 'C16': 17, 'C03': 18, 'C01': 19, 'C18': 20, 'C04': 21, 'C06': 22}\n",
            "**************************************************\n",
            "vocab_id size: 20957\n",
            "**************************************************\n",
            "Epoch: 0, Train loss:3.1427, Val acc: 0.0655, Times: 60.8290s, *\n",
            "Epoch: 1, Train loss:3.1293, Val acc: 0.0714, Times: 60.9559s, *\n",
            "Epoch: 2, Train loss:3.1118, Val acc: 0.1399, Times: 61.5805s, *\n",
            "Epoch: 3, Train loss:3.0698, Val acc: 0.2887, Times: 61.9796s, *\n",
            "Epoch: 4, Train loss:2.9754, Val acc: 0.1756, Times: 61.7403s, \n",
            "Epoch: 5, Train loss:2.9085, Val acc: 0.1756, Times: 61.3149s, \n",
            "Epoch: 6, Train loss:2.8951, Val acc: 0.1756, Times: 60.9633s, \n",
            "Epoch: 7, Train loss:2.8319, Val acc: 0.1786, Times: 60.6733s, \n",
            "Epoch: 8, Train loss:2.7176, Val acc: 0.1994, Times: 60.5491s, \n",
            "Epoch: 9, Train loss:2.7005, Val acc: 0.2381, Times: 60.3507s, \n",
            "Epoch: 10, Train loss:2.7058, Val acc: 0.2798, Times: 60.1543s, \n",
            "Epoch: 11, Train loss:2.6256, Val acc: 0.2946, Times: 60.1236s, *\n",
            "Epoch: 12, Train loss:2.5257, Val acc: 0.3036, Times: 60.1001s, *\n",
            "Epoch: 13, Train loss:2.4242, Val acc: 0.3095, Times: 59.9612s, *\n",
            "Epoch: 14, Train loss:2.3632, Val acc: 0.3155, Times: 59.9679s, *\n",
            "Epoch: 15, Train loss:2.3681, Val acc: 0.3333, Times: 59.9119s, *\n",
            "Epoch: 16, Train loss:2.2860, Val acc: 0.3482, Times: 59.8661s, *\n",
            "Epoch: 17, Train loss:2.2764, Val acc: 0.3661, Times: 59.7961s, *\n",
            "Epoch: 18, Train loss:2.2213, Val acc: 0.3720, Times: 59.7378s, *\n",
            "Epoch: 19, Train loss:2.1994, Val acc: 0.3780, Times: 59.6831s, *\n",
            "Epoch: 20, Train loss:2.1475, Val acc: 0.3869, Times: 59.6069s, *\n",
            "Epoch: 21, Train loss:2.1308, Val acc: 0.3869, Times: 59.5384s, \n",
            "Epoch: 22, Train loss:2.0945, Val acc: 0.3958, Times: 59.5583s, *\n",
            "Epoch: 23, Train loss:2.0615, Val acc: 0.4018, Times: 59.5250s, *\n",
            "Epoch: 24, Train loss:2.0297, Val acc: 0.4167, Times: 59.5714s, *\n",
            "Epoch: 25, Train loss:2.0224, Val acc: 0.4226, Times: 59.5413s, *\n",
            "Epoch: 26, Train loss:1.9847, Val acc: 0.4196, Times: 59.5005s, \n",
            "Epoch: 27, Train loss:1.9628, Val acc: 0.4345, Times: 59.4456s, *\n",
            "Epoch: 28, Train loss:1.9510, Val acc: 0.4464, Times: 59.4253s, *\n",
            "Epoch: 29, Train loss:1.9467, Val acc: 0.4435, Times: 59.2872s, \n",
            "Epoch: 30, Train loss:1.8796, Val acc: 0.4435, Times: 59.1542s, \n",
            "Epoch: 31, Train loss:1.8726, Val acc: 0.4375, Times: 59.0438s, \n",
            "Epoch: 32, Train loss:1.8268, Val acc: 0.4375, Times: 58.9238s, \n",
            "Epoch: 33, Train loss:1.8602, Val acc: 0.4435, Times: 58.8072s, \n",
            "Epoch: 34, Train loss:1.8503, Val acc: 0.4554, Times: 58.6887s, *\n",
            "Epoch: 35, Train loss:1.7940, Val acc: 0.4524, Times: 58.5862s, \n",
            "Epoch: 36, Train loss:1.8048, Val acc: 0.4524, Times: 58.4845s, \n",
            "Epoch: 37, Train loss:1.7466, Val acc: 0.4554, Times: 58.4013s, \n",
            "Epoch: 38, Train loss:1.7811, Val acc: 0.4583, Times: 58.3518s, *\n",
            "Epoch: 39, Train loss:1.7056, Val acc: 0.4613, Times: 58.2932s, *\n",
            "Epoch: 40, Train loss:1.7004, Val acc: 0.4643, Times: 58.2511s, *\n",
            "Epoch: 41, Train loss:1.7221, Val acc: 0.4643, Times: 58.1782s, \n",
            "Epoch: 42, Train loss:1.7422, Val acc: 0.4643, Times: 58.1219s, \n",
            "Epoch: 43, Train loss:1.6916, Val acc: 0.4643, Times: 58.0575s, \n",
            "Epoch: 44, Train loss:1.7405, Val acc: 0.4702, Times: 58.0017s, *\n",
            "Epoch: 45, Train loss:1.7072, Val acc: 0.4732, Times: 57.9989s, *\n",
            "Epoch: 46, Train loss:1.7303, Val acc: 0.4732, Times: 57.9451s, \n",
            "Epoch: 47, Train loss:1.6895, Val acc: 0.4762, Times: 57.9049s, *\n",
            "Epoch: 48, Train loss:1.6883, Val acc: 0.4762, Times: 57.8476s, \n",
            "Epoch: 49, Train loss:1.6455, Val acc: 0.4762, Times: 57.8066s, \n",
            "Test accuracy: 0.4781\n",
            "Run 1/16, Time: 49.5m, min_count=1, num_head=2, num_layer=2, kw=0.1, ks=0.1, kd=0.8\n",
            "**************************************************\n",
            "Epoch: 0, Train loss:3.1186, Val acc: 0.1756, Times: 57.5469s, *\n",
            "Epoch: 1, Train loss:3.0901, Val acc: 0.1756, Times: 57.7245s, \n",
            "Epoch: 2, Train loss:3.0581, Val acc: 0.1756, Times: 57.9337s, \n",
            "Epoch: 3, Train loss:2.9807, Val acc: 0.1756, Times: 57.9592s, \n",
            "Epoch: 4, Train loss:2.8434, Val acc: 0.1756, Times: 58.3794s, \n",
            "Epoch: 5, Train loss:2.8247, Val acc: 0.1756, Times: 58.7199s, \n",
            "Epoch: 6, Train loss:2.7551, Val acc: 0.1756, Times: 59.1496s, \n",
            "Epoch: 7, Train loss:2.6741, Val acc: 0.1875, Times: 59.4799s, *\n",
            "Epoch: 8, Train loss:2.6225, Val acc: 0.2232, Times: 59.7210s, *\n",
            "Epoch: 9, Train loss:2.5706, Val acc: 0.2946, Times: 60.0073s, *\n",
            "Epoch: 10, Train loss:2.5261, Val acc: 0.3125, Times: 60.1246s, *\n",
            "Epoch: 11, Train loss:2.3688, Val acc: 0.3423, Times: 60.1854s, *\n",
            "Epoch: 12, Train loss:2.6596, Val acc: 0.3601, Times: 60.2369s, *\n",
            "Epoch: 13, Train loss:2.3352, Val acc: 0.3661, Times: 60.3155s, *\n",
            "Epoch: 14, Train loss:2.2093, Val acc: 0.3780, Times: 60.3863s, *\n",
            "Epoch: 15, Train loss:2.1737, Val acc: 0.3780, Times: 60.4302s, \n",
            "Epoch: 16, Train loss:2.1817, Val acc: 0.4018, Times: 60.4965s, *\n",
            "Epoch: 17, Train loss:2.0242, Val acc: 0.3988, Times: 60.3984s, \n",
            "Epoch: 18, Train loss:2.2247, Val acc: 0.4077, Times: 60.3488s, *\n",
            "Epoch: 19, Train loss:2.0305, Val acc: 0.4167, Times: 60.4323s, *\n",
            "Epoch: 20, Train loss:2.0433, Val acc: 0.4226, Times: 60.4874s, *\n",
            "Epoch: 21, Train loss:1.9132, Val acc: 0.4196, Times: 60.5341s, \n",
            "Epoch: 22, Train loss:1.9126, Val acc: 0.4137, Times: 60.5389s, \n",
            "Epoch: 23, Train loss:1.7705, Val acc: 0.4196, Times: 60.5415s, \n",
            "Epoch: 24, Train loss:1.7171, Val acc: 0.4345, Times: 60.5226s, *\n",
            "Epoch: 25, Train loss:1.6758, Val acc: 0.4345, Times: 60.5308s, \n",
            "Epoch: 26, Train loss:1.7148, Val acc: 0.4375, Times: 60.5432s, *\n",
            "Epoch: 27, Train loss:1.7202, Val acc: 0.4375, Times: 60.5785s, \n",
            "Epoch: 28, Train loss:1.6593, Val acc: 0.4375, Times: 60.6029s, \n",
            "Epoch: 29, Train loss:1.7184, Val acc: 0.4405, Times: 60.6139s, *\n",
            "Epoch: 30, Train loss:1.6110, Val acc: 0.4464, Times: 60.6402s, *\n",
            "Epoch: 31, Train loss:1.6255, Val acc: 0.4554, Times: 60.6473s, *\n",
            "Epoch: 32, Train loss:1.5264, Val acc: 0.4583, Times: 60.6237s, *\n",
            "Epoch: 33, Train loss:1.8852, Val acc: 0.4524, Times: 60.6038s, \n",
            "Epoch: 34, Train loss:1.5631, Val acc: 0.4554, Times: 60.5888s, \n",
            "Epoch: 35, Train loss:1.4602, Val acc: 0.4643, Times: 60.5780s, *\n",
            "Epoch: 36, Train loss:1.5214, Val acc: 0.4702, Times: 60.5614s, *\n",
            "Epoch: 37, Train loss:1.5361, Val acc: 0.4702, Times: 60.5482s, \n",
            "Epoch: 38, Train loss:1.5292, Val acc: 0.4673, Times: 60.5660s, \n",
            "Epoch: 39, Train loss:1.5713, Val acc: 0.4702, Times: 60.5521s, \n",
            "Epoch: 40, Train loss:1.4834, Val acc: 0.4702, Times: 60.5273s, \n",
            "Epoch: 41, Train loss:1.5157, Val acc: 0.4732, Times: 60.5081s, *\n",
            "Epoch: 42, Train loss:1.4563, Val acc: 0.4732, Times: 60.4969s, \n",
            "Epoch: 43, Train loss:1.5138, Val acc: 0.4762, Times: 60.4900s, *\n",
            "Epoch: 44, Train loss:1.4374, Val acc: 0.4762, Times: 60.5038s, \n",
            "Epoch: 45, Train loss:1.4891, Val acc: 0.4732, Times: 60.5019s, \n",
            "Epoch: 46, Train loss:1.4999, Val acc: 0.4732, Times: 60.5218s, \n",
            "Epoch: 47, Train loss:1.4393, Val acc: 0.4732, Times: 60.5209s, \n",
            "Epoch: 48, Train loss:1.5067, Val acc: 0.4762, Times: 60.5196s, \n",
            "Epoch: 49, Train loss:1.4328, Val acc: 0.4762, Times: 60.5271s, \n",
            "Test accuracy: 0.4784\n",
            "Run 2/16, Time: 51.8m, min_count=1, num_head=2, num_layer=2, kw=0.8, ks=0.1, kd=0.6\n",
            "**************************************************\n",
            "Epoch: 0, Train loss:3.1582, Val acc: 0.0536, Times: 61.1300s, *\n",
            "Epoch: 1, Train loss:3.1069, Val acc: 0.0685, Times: 61.7222s, *\n",
            "Epoch: 2, Train loss:3.1185, Val acc: 0.1845, Times: 61.3974s, *\n",
            "Epoch: 3, Train loss:3.0750, Val acc: 0.1756, Times: 61.0711s, \n",
            "Epoch: 4, Train loss:2.9813, Val acc: 0.1756, Times: 60.6720s, \n",
            "Epoch: 5, Train loss:2.8423, Val acc: 0.1756, Times: 60.4656s, \n",
            "Epoch: 6, Train loss:2.8599, Val acc: 0.1756, Times: 60.2602s, \n",
            "Epoch: 7, Train loss:2.8063, Val acc: 0.1756, Times: 60.3234s, \n",
            "Epoch: 8, Train loss:2.7707, Val acc: 0.1756, Times: 60.4952s, \n",
            "Epoch: 9, Train loss:2.7876, Val acc: 0.1756, Times: 60.5138s, \n",
            "Epoch: 10, Train loss:2.7554, Val acc: 0.1756, Times: 60.6226s, \n",
            "No improvement on dev set, early stopping\n",
            "Test accuracy: 0.1603\n",
            "Run 3/16, Time: 12.5m, min_count=1, num_head=2, num_layer=2, kw=0.3, ks=0.3, kd=0.3\n",
            "**************************************************\n",
            "Epoch: 0, Train loss:3.1880, Val acc: 0.0060, Times: 61.4405s, *\n",
            "Epoch: 1, Train loss:3.1110, Val acc: 0.1756, Times: 61.4755s, *\n",
            "Epoch: 2, Train loss:3.0060, Val acc: 0.1756, Times: 61.8004s, \n",
            "Epoch: 3, Train loss:2.9528, Val acc: 0.1756, Times: 61.7629s, \n",
            "Epoch: 4, Train loss:2.9265, Val acc: 0.1756, Times: 61.6160s, \n",
            "Epoch: 5, Train loss:2.8709, Val acc: 0.1756, Times: 61.4575s, \n",
            "Epoch: 6, Train loss:2.8332, Val acc: 0.1756, Times: 61.4390s, \n",
            "Epoch: 7, Train loss:2.7702, Val acc: 0.1994, Times: 61.4919s, *\n",
            "Epoch: 8, Train loss:2.7251, Val acc: 0.2202, Times: 61.3132s, *\n",
            "Epoch: 9, Train loss:2.5900, Val acc: 0.2649, Times: 61.3007s, *\n",
            "Epoch: 10, Train loss:2.6390, Val acc: 0.2946, Times: 61.4554s, *\n",
            "Epoch: 11, Train loss:2.5030, Val acc: 0.3065, Times: 61.3763s, *\n",
            "Epoch: 12, Train loss:2.4508, Val acc: 0.3065, Times: 61.3358s, \n",
            "Epoch: 13, Train loss:2.5396, Val acc: 0.3363, Times: 61.3150s, *\n",
            "Epoch: 14, Train loss:2.3165, Val acc: 0.3482, Times: 61.2774s, *\n",
            "Epoch: 15, Train loss:2.2714, Val acc: 0.3750, Times: 61.3058s, *\n",
            "Epoch: 16, Train loss:2.3101, Val acc: 0.3690, Times: 61.3568s, \n",
            "Epoch: 17, Train loss:2.1968, Val acc: 0.3810, Times: 61.3769s, *\n",
            "Epoch: 18, Train loss:2.0596, Val acc: 0.3958, Times: 61.4972s, *\n",
            "Epoch: 19, Train loss:2.1742, Val acc: 0.3958, Times: 61.4794s, \n",
            "Epoch: 20, Train loss:2.1512, Val acc: 0.4077, Times: 61.4527s, *\n",
            "Epoch: 21, Train loss:2.0796, Val acc: 0.4137, Times: 61.4320s, *\n",
            "Epoch: 22, Train loss:2.0160, Val acc: 0.4256, Times: 61.4019s, *\n",
            "Epoch: 23, Train loss:1.9883, Val acc: 0.4286, Times: 61.3974s, *\n",
            "Epoch: 24, Train loss:1.9034, Val acc: 0.4405, Times: 61.3968s, *\n",
            "Epoch: 25, Train loss:1.8411, Val acc: 0.4375, Times: 61.4174s, \n",
            "Epoch: 26, Train loss:1.6632, Val acc: 0.4554, Times: 61.4095s, *\n",
            "Epoch: 27, Train loss:1.7993, Val acc: 0.4554, Times: 61.3751s, \n",
            "Epoch: 28, Train loss:1.7705, Val acc: 0.4524, Times: 61.2877s, \n",
            "Epoch: 29, Train loss:1.6962, Val acc: 0.4494, Times: 61.2020s, \n",
            "Epoch: 30, Train loss:1.6751, Val acc: 0.4524, Times: 61.1057s, \n",
            "Epoch: 31, Train loss:1.6763, Val acc: 0.4583, Times: 60.9247s, *\n",
            "Epoch: 32, Train loss:1.5987, Val acc: 0.4583, Times: 60.7937s, \n",
            "Epoch: 33, Train loss:1.6038, Val acc: 0.4554, Times: 60.7668s, \n",
            "Epoch: 34, Train loss:1.8774, Val acc: 0.4673, Times: 60.7257s, *\n",
            "Epoch: 35, Train loss:1.6392, Val acc: 0.4673, Times: 60.6411s, \n",
            "Epoch: 36, Train loss:1.5654, Val acc: 0.4732, Times: 60.5886s, *\n",
            "Epoch: 37, Train loss:1.5116, Val acc: 0.4792, Times: 60.5500s, *\n",
            "Epoch: 38, Train loss:1.6552, Val acc: 0.4792, Times: 60.5438s, \n",
            "Epoch: 39, Train loss:1.5212, Val acc: 0.4792, Times: 60.5692s, \n",
            "Epoch: 40, Train loss:1.5947, Val acc: 0.4881, Times: 60.5260s, *\n",
            "Epoch: 41, Train loss:1.5475, Val acc: 0.4821, Times: 60.4625s, \n",
            "Epoch: 42, Train loss:1.4761, Val acc: 0.4911, Times: 60.4047s, *\n",
            "Epoch: 43, Train loss:1.5634, Val acc: 0.4881, Times: 60.3530s, \n",
            "Epoch: 44, Train loss:1.4727, Val acc: 0.4851, Times: 60.3287s, \n",
            "Epoch: 45, Train loss:1.4509, Val acc: 0.4881, Times: 60.3337s, \n",
            "Epoch: 46, Train loss:1.4211, Val acc: 0.4911, Times: 60.3463s, \n",
            "Epoch: 47, Train loss:1.4127, Val acc: 0.4940, Times: 60.3242s, *\n",
            "Epoch: 48, Train loss:1.5026, Val acc: 0.4911, Times: 60.2954s, \n",
            "Epoch: 49, Train loss:1.6407, Val acc: 0.4911, Times: 60.2497s, \n",
            "Test accuracy: 0.4801\n",
            "Run 4/16, Time: 51.5m, min_count=1, num_head=2, num_layer=2, kw=0.8, ks=0.8, kd=0.8\n",
            "**************************************************\n",
            "Epoch: 0, Train loss:3.1498, Val acc: 0.0208, Times: 69.2398s, *\n",
            "Epoch: 1, Train loss:3.1299, Val acc: 0.0149, Times: 70.5045s, \n",
            "Epoch: 2, Train loss:3.1220, Val acc: 0.0238, Times: 70.5414s, *\n",
            "Epoch: 3, Train loss:3.1083, Val acc: 0.1637, Times: 70.8129s, *\n",
            "Epoch: 4, Train loss:3.1046, Val acc: 0.1845, Times: 71.3496s, *\n",
            "Epoch: 5, Train loss:2.9995, Val acc: 0.1815, Times: 71.5811s, \n",
            "Epoch: 6, Train loss:3.0282, Val acc: 0.1786, Times: 71.5397s, \n",
            "Epoch: 7, Train loss:2.8139, Val acc: 0.1815, Times: 71.3509s, \n",
            "Epoch: 8, Train loss:2.8177, Val acc: 0.1875, Times: 71.1840s, *\n",
            "Epoch: 9, Train loss:2.7139, Val acc: 0.1905, Times: 71.0975s, *\n",
            "Epoch: 10, Train loss:2.6959, Val acc: 0.2173, Times: 70.9925s, *\n"
          ]
        }
      ],
      "source": [
        "!python main.py --dataset='oh' --sen_max_len=150 --epoch=50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3Jfdy0QHC8bI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5701b2f-849e-4379-a193-f191a5b977a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ],
      "source": [
        "!python -V"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HieGAT_2",
      "toc_visible": true,
      "provenance": [],
      "mount_file_id": "11ICgvrCuyS8kQHYvsX4QzAMK4mdNs-i2",
      "authorship_tag": "ABX9TyMqSqGzx7LUitUzXR3WnR+f",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}