{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CreateYourOwnDataset_graph_pytorch_geometric",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZ/hCuDqAWUpcs4cZO86jv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hshuai97/Colab20210803/blob/main/CreateYourOwnDataset_graph_pytorch_geometric.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "将train, dev, test的文本实现构建成Data对象的图结构，然后批量处理\n",
        "\n",
        "Refer: [Youtube video](https://www.youtube.com/watch?v=QLIkOtKS4os)"
      ],
      "metadata": {
        "id": "bzlKRfDPwDdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "CR7GaOjXwBaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIIPoQopQVQg"
      },
      "outputs": [],
      "source": [
        "import os.path as osp\n",
        "import pickle as pkl\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch_geometric.data import Dataset, Data\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "class MyDataset(Dataset):  # build for 5 datasets\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "      '''\n",
        "      root: datasets path, is split into raw_dir (raw dataset) and processed_dir (graph data)\n",
        "      '''\n",
        "      super(MyDataset, self).__init__(root, transform, pre_transform)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "      '''\n",
        "      if return file exists, the download is not triggered\n",
        "      '''\n",
        "      for NAME in ['20ng', 'r8', 'r52', 'oh', 'mr']:\n",
        "        return [NAME+'-dev-stemmed.txt', NAME+'-test-stemmed.txt']\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        ''' \n",
        "        if these file exists in raw_dir, processing is skipped\n",
        "        '''\n",
        "        return ['not_implemented.pt']\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        NAME, MODE = 'mr', 'train'\n",
        "        self.text, self.label = self.ReadDataset(NAME, MODE)\n",
        "        \n",
        "        min_c = {'mr':1, '20ng': 10, 'other':5}  # min_count\n",
        "        min_count = min_c[NAME]\n",
        "\n",
        "        if MODE =='train':  # if vocab not exist, build vocab on train set\n",
        "          if not os.path.exists(f'M:/HieGAT/data2graph/{NAME}/{NAME}-vocab.pkl'):\n",
        "            vocab = self.buildvocab(self.text, min_count=min_count)  # dict\n",
        "            with open(f'M:/HieGAT/data2graph/{NAME}/{NAME}-vocab.pkl', 'wb') as f:\n",
        "              pkl.dump(vocab, f, protocol=pkl.HIGHEST_PROTOCOL)  # save vocab as pickle file\n",
        "\n",
        "        with open(f'M:/HieGAT/data2graph/{NAME}/{NAME}-vocab.pkl', 'rb') as f:\n",
        "          vocab = pkl.load(f)\n",
        "        print(len(vocab))\n",
        "        # todo\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    def ReadDataset(self, dataset_name, mode):\n",
        "        NAME = dataset_name\n",
        "        MODE = mode\n",
        "        if NAME not in ['20ng', 'r8', 'r52', 'oh', 'mr']:\n",
        "          raise ValueError('The dataset is not support')\n",
        "\n",
        "        PATH = 'M:/HieGAT/datasets/'\n",
        "\n",
        "        texts = []\n",
        "        labels = []\n",
        "        with open(os.path.join(PATH, NAME+'-' + MODE + '-stemmed.txt'), 'r') as f:\n",
        "            data = f.readlines()\n",
        "\n",
        "            for line in data:\n",
        "              line = line.strip()\n",
        "              t = line.split('\\t')\n",
        "              texts.append(t[1])\n",
        "              labels.append(t[0])\n",
        "\n",
        "\n",
        "        print(f'Dataset: {NAME}, Total {MODE} X: {len(texts)}, Total {MODE} Y: {len(labels)}')\n",
        "        print('*'*50)\n",
        "\n",
        "        return texts,  labels\n",
        "\n",
        "    def  buildvocab(self, sample, min_count=5):\n",
        "      '''\n",
        "      sample: ['wo xihuan ziran yuyan chuli', 'wo ai shengdu xuexi',  'wo xihuan jiqi xuexi']\n",
        "\n",
        "      '''\n",
        "      MIN_COUNT = min_count\n",
        "\n",
        "      freq = {}\n",
        "      for i in sample:\n",
        "        for t in word_tokenize(i):\n",
        "          if t not in freq:\n",
        "            freq[t] = 0\n",
        "\n",
        "      for i in sample:\n",
        "        for t in word_tokenize(i):\n",
        "          freq[t] +=1\n",
        "\n",
        "      del_key = []\n",
        "      for i in freq:\n",
        "        if freq[i]<MIN_COUNT:\n",
        "          del_key.append(i)\n",
        "      \n",
        "      for i in del_key:\n",
        "        freq.pop(i)\n",
        "\n",
        "      vocab_id = {}\n",
        "      for i, key in enumerate(freq):\n",
        "        vocab_id[key] = i\n",
        "      print(f'vocab_id size: {len(vocab_id)}')\n",
        "      print('*'*50)\n",
        "      \n",
        "      return vocab_id\n",
        "\n",
        "    def w2id(self, input, vocab):\n",
        "      ids = []\n",
        "      for w in input:\n",
        "        if w in ['.', '!', '?']:\n",
        "          continue\n",
        "        if w in vocab:\n",
        "          ids.append(vocab[w])\n",
        "        else: \n",
        "          ids.append(vocab['<unk>'])\n",
        "      return ids\n",
        "\n",
        "    def w2token(text, label):\n",
        "      print(1)\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.processed_file_names)\n",
        "\n",
        "    def get(self, idx):\n",
        "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "        return data\n",
        "\n",
        "data_mr = MyDataset(root='M:/HieGAT/data2graph/mr')"
      ]
    }
  ]
}