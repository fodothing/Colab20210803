{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S.Hua-New-Text-Level-GNN",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1-ZnDQQ7hyLI6X88yn0vUmxkzRHyhE5el",
      "authorship_tag": "ABX9TyO0aqxhFRzeGxPi/L5bJcdf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/overlyhandsome/Colab20210803/blob/main/S_Hua_New_Text_Level_GNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "参考Cynwell Lau对[Text-Level-GNN](https://arxiv.org/pdf/1910.02356.pdf)论文思想的代码复现：[code source](https://github.com/Cynwell/Text-Level-GNN/blob/main/train.py)"
      ],
      "metadata": {
        "id": "lWqQWlxCQOgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "论文的主要思想部分：\n",
        "1. Message Passing Mechanism: 信息传递规则\n",
        "2. TextLevelGNN：图卷积神经网络(GCN)"
      ],
      "metadata": {
        "id": "9HzROP-M6Zyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VLqGCP1P8WC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72d63d00-53f3-4e53-ebec-84fbf59249a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing parsing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile parsing.py\n",
        "# 标准库\n",
        "import pandas as pd\n",
        "from numpy import asarray\n",
        "import numpy as np\n",
        "from time import time\n",
        "import argparse\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# 操作文件数据\n",
        "import pickle\n",
        "\n",
        "\n",
        "#兼容原始GloVe和添加OPTED中'非常用词'后的Full_GloVe\n",
        "class Full_GloveTokenizer:\n",
        "    def __init__(self, filename, unk='<unk>', pad='<pad>'): \n",
        "        self.filename = filename  # Full_GloVe文件的路径\n",
        "        self.unk = unk  # 对于未知词汇的处理\n",
        "        self.pad = pad  # 按最大长度对齐每个文本\n",
        "        self.stoi = dict()\n",
        "        self.itos = dict()\n",
        "        self.embedding_matrix = list()  # [[embedded_word1], [embedded_word2],...]\n",
        "\n",
        "        Full_GloVe = pickle.load(open(self.filename, 'rb'))  # 读取Full_GloVe字典\n",
        "\n",
        "        for i, k in enumerate(Full_GloVe.keys()):\n",
        "          self.stoi[k] = i\n",
        "          self.itos[i] = k\n",
        "          self.embedding_matrix.append(Full_GloVe[k])\n",
        "\n",
        "\n",
        "        if self.unk is not None: # 未知词一律用一个随机embedded向量表示\n",
        "            i += 1\n",
        "            self.stoi[self.unk] = i\n",
        "            self.itos[i] = self.unk\n",
        "            self.embedding_matrix.append(asarray(np.random.rand(len(self.embedding_matrix[0])), dtype='float32'))\n",
        "        if self.pad is not None: # Add pad token into the tokenizer # padded词一律用全0的embedded向量表示\n",
        "            i += 1\n",
        "            self.stoi[self.pad] = i\n",
        "            self.itos[i] = self.pad\n",
        "            self.embedding_matrix.append(asarray(np.zeros(len(self.embedding_matrix[0])), dtype='float32'))\n",
        "        self.embedding_matrix = np.array(self.embedding_matrix).astype(np.float32) # Convert if from double to float for efficiency\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        if type(sentence) == str:\n",
        "            sentence = sentence.split(' ')\n",
        "        elif len(sentence): # Convertible to list\n",
        "            sentence = list(sentence)\n",
        "        else:\n",
        "            raise TypeError('sentence should be either a str or a list of str!')\n",
        "        encoded_sentence = []\n",
        "        for word in sentence:\n",
        "            encoded_sentence.append(self.stoi.get(word, self.stoi[self.unk]))\n",
        "        return encoded_sentence\n",
        "\n",
        "    def decode(self, encoded_sentence):\n",
        "        try:\n",
        "            encoded_sentence = list(encoded_sentence)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            raise TypeError('encoded_sentence should be either a str or a data type that is convertible to list type!')\n",
        "        sentence = []\n",
        "        for encoded_word in encoded_sentence:\n",
        "            sentence.append(self.itos[encoded_word])\n",
        "        return sentence\n",
        "\n",
        "    def embedding(self, encoded_sentence):\n",
        "        # Full_GloVe中有464885个嵌入词向量\n",
        "        return self.embedding_matrix[np.array(encoded_sentence)]  # 获得一个句子的每个词的embedded向量(max_sentence_length, d)\n",
        "\n",
        "\n",
        "class TextLevelGNNDataset(Dataset): # For instantiating train, validation and test dataset\n",
        "    def __init__(self, node_sets, neighbor_sets, public_edge_mask, labels):\n",
        "        super(TextLevelGNNDataset).__init__()\n",
        "        self.node_sets = node_sets  # 同batch_size张图的节点集合(batch_size, l)\n",
        "        self.neighbor_sets = neighbor_sets  # batch_size张图的节点邻居集合\n",
        "        self.public_edge_mask = public_edge_mask  # batch_size张图的布尔型张量边标记\n",
        "        self.labels = labels  # batch_size张图的标签\n",
        "\n",
        "    def __getitem__(self, i):  # 将batch_size张图中，每张图对应的四个属性封装为一个长张量单元，这样可以灵活调整batch_size的大小\n",
        "        return torch.LongTensor(self.node_sets[i]), \\\n",
        "               torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1), \\\n",
        "               self.public_edge_mask[torch.LongTensor(self.node_sets[i]).unsqueeze(-1).repeat(1, torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1).shape[-1]), torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1)], \\\n",
        "               torch.FloatTensor(self.labels[i])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "class TextLevelGNNDatasetClass: # This class is used to achieve parameters sharing among datasets (train/validation/test)\n",
        "    def __init__(self, train_filename, test_filename, opted_filename, full_glove_pth, tokenizer, MAX_LENGTH=50, p=2, min_freq=2, train_validation_split=0.8):\n",
        "        self.train_filename = train_filename\n",
        "        self.test_filename = test_filename\n",
        "        # OPTED词典路径\n",
        "        self.opted_filename = opted_filename\n",
        "        self.full_glove_pth = full_glove_pth \n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.MAX_LENGTH = MAX_LENGTH\n",
        "        self.p = p\n",
        "        self.min_freq = min_freq\n",
        "        self.train_validation_split = train_validation_split\n",
        "\n",
        "        self.train_data = pd.read_csv(self.train_filename, sep='\\t', header=None)\n",
        "        self.test_data = pd.read_csv(self.test_filename, sep='\\t', header=None)\n",
        "\n",
        "        self.stoi = {'<unk>': 0, '<pad>': 1} # Re-index\n",
        "        self.itos = {0: '<unk>', 1: '<pad>'} # Re-index\n",
        "        self.vocab_count = len(self.stoi)\n",
        "        self.embedding_matrix = None\n",
        "\n",
        "        # 通过OPTED词典，返回训练集上每个词对应的1个语义，没有语义的用0向量表示,对于没有语义的词，后续可以考虑抓取维基百科的词条解释，类似于不断学习增加知识库的过程\n",
        "        self.meaning_matrix = list()  #[meaning1, meaning2, ...] \n",
        "        self.meaning1 = list()  # 节点语义1\n",
        "        self.meaning2 = list()  # 节点语义2\n",
        "\n",
        "        self.label_dict = dict(zip(self.train_data[0].unique(), pd.get_dummies(self.train_data[0].unique()).values.tolist()))\n",
        "\n",
        "        # 将读取的数据转成数组\n",
        "        self.train_dataset, self.validation_dataset = random_split(self.train_data.to_numpy(), [int(len(self.train_data) * train_validation_split), len(self.train_data) - int(len(self.train_data) * train_validation_split)])\n",
        "        self.test_dataset = self.test_data.to_numpy()\n",
        "\n",
        "        self.build_vocab() # Based on train_dataset only. Updates self.stoi, self.itos, self.vocab_count and self.embedding_matrix\n",
        "\n",
        "        self.build_meaning_matrix()  # 获得训练集上每个词对应的2个meaning\n",
        "        \n",
        "        self.train_dataset, self.validation_dataset, self.test_dataset, self.edge_stat, self.public_edge_mask = self.prepare_dataset()\n",
        "\n",
        "    def build_vocab(self):\n",
        "        vocab_list = [sentence.split(' ') for _, sentence in self.train_dataset]\n",
        "        unique_vocab = []\n",
        "        for vocab in vocab_list:\n",
        "            unique_vocab.extend(vocab)\n",
        "        unique_vocab = list(set(unique_vocab))\n",
        "\n",
        "        for vocab in unique_vocab:\n",
        "            if vocab in self.tokenizer.stoi.keys():\n",
        "                self.stoi[vocab] = self.vocab_count\n",
        "                self.itos[self.vocab_count] = vocab\n",
        "                self.vocab_count += 1\n",
        "        self.embedding_matrix = self.tokenizer.embedding(self.tokenizer.encode(list(self.stoi.keys())))  # (|V|, d) 仅根据训练集构建的全局词嵌入向量\n",
        "       \n",
        "    def build_meaning_matrix(self):  # 构建训练集上每个节点对应的2个语义关系\n",
        "        OPTED = pickle.load(open(self.opted_filename, 'rb'))  # 打补丁的'pad'也在OPTED中！！！\n",
        "        del OPTED['pad']  # 删除'pad'这个特殊的词\n",
        "\n",
        "        Full_GloVe = pickle.load(open(self.full_glove_pth, 'rb'))\n",
        "\n",
        "        # meaning1\n",
        "        for vocab in self.stoi.keys():  # 遍历训练集字典下的每个词\n",
        "          m = asarray(np.zeros(300), dtype='float32')   # 初始化一个300维的meaning\n",
        "          j = 0\n",
        "          if vocab in OPTED.keys():  # 训练集中的词在OPTED中, 对于'unk'、'pad'和其他不在OPTED中的词均用一个全0的300维向量语义表示\n",
        "            for s in OPTED[vocab][0].split():  # 取第一个meaning, 后续可考虑取多个meaning(2, 3, 4...)\n",
        "              if s in Full_GloVe.keys():\n",
        "                j += 1\n",
        "                m += Full_GloVe[s]\n",
        "            if j>0:\n",
        "              m = m/j\n",
        "            self.meaning1.append(m)  # 添加到meaning_matrix中为了和embedded_neighbor_node结构对齐（三维）张量\n",
        "          else: # 训练集中的词不在OPTED中\n",
        "            self.meaning1.append(m)  # 没有解释的词，其meaning用0向量代替\n",
        "        self.meaning1 = np.array(self.meaning1).astype(np.float32)\n",
        "        self.meaning_matrix.append(self.meaning1)  # [meaning1, meaning2]\n",
        "        \n",
        "        # meaning2\n",
        "        for vocab in self.stoi.keys():\n",
        "          m = asarray(np.zeros(300), dtype='float32')\n",
        "          j = 0\n",
        "          if vocab in OPTED.keys():\n",
        "            if len(OPTED[vocab]) > 1:  # 如果词存在第二个语义\n",
        "              for s in OPTED[vocab][1].split():\n",
        "                if s in Full_GloVe.keys():\n",
        "                  j += 1\n",
        "                  m += Full_GloVe[s]\n",
        "              if j > 0:\n",
        "                m = m/j\n",
        "              self.meaning2.append(m)\n",
        "            else:  #没有第二个语义\n",
        "              self.meaning2.append(m)\n",
        "          else:  # 词不在OPTED中\n",
        "            self.meaning2.append(m)\n",
        "        \n",
        "        self.meaning2 = np.array(self.meaning2).astype(np.float32)\n",
        "        #print(f'len meaning2:\\n{len(self.meaning2)}')\n",
        "        \n",
        "        self.meaning_matrix.append(self.meaning2)  # meaning_matrix: [meaning1, meaning2]\n",
        "        # self.meaning_matrix = np.array(self.meaning_matrix).astype(np.float32)\n",
        "        \n",
        "          \n",
        "    # 本类中的主方法，负责返回结果\n",
        "    def prepare_dataset(self): # will also build self.edge_stat and self.public_edge_mask\n",
        "        # preparing self.train_dataset\n",
        "        node_sets = [[self.stoi.get(vocab, 0) for vocab in sentence.strip().split(' ')][:self.MAX_LENGTH] for _, sentence in self.train_dataset] # Only retrieve the first MAX_LENGTH words in each document\n",
        "        neighbor_sets = [create_neighbor_set(node_set, p=self.p) for node_set in node_sets]\n",
        "        labels = [self.label_dict[label] for label, _ in self.train_dataset]\n",
        "\n",
        "        # 调用类中的其他方法获取边的统计和标记信息\n",
        "        edge_stat, public_edge_mask = self.build_public_edge_mask(node_sets, neighbor_sets, min_freq=self.min_freq)\n",
        "        \n",
        "        train_dataset = TextLevelGNNDataset(node_sets, neighbor_sets, public_edge_mask, labels)  # 传给TextLevelGNNDataset类，返回每张图对应的torch.LongTensor(tensor0, tensor1, tensor2, tensor3)\n",
        "        \n",
        "        # preparing self.validation_dataset\n",
        "        node_sets = [[self.stoi.get(vocab, 0) for vocab in sentence.strip().split(' ')][:self.MAX_LENGTH] for _, sentence in self.validation_dataset] # Only retrieve the first MAX_LENGTH words in each document\n",
        "        neighbor_sets = [create_neighbor_set(node_set, p=self.p) for node_set in node_sets]\n",
        "        labels = [self.label_dict[label] for label, _ in self.validation_dataset]\n",
        "        validation_dataset = TextLevelGNNDataset(node_sets, neighbor_sets, public_edge_mask, labels)  # 传给TextLevelGNNDataset类，返回每张图对应的torch.LongTensor(tensor0, tensor1, tensor2, tensor3)\n",
        "\n",
        "        # preparing self.test_dataset\n",
        "        node_sets = [[self.stoi.get(vocab, 0) for vocab in sentence.strip().split(' ')][:self.MAX_LENGTH] for _, sentence in self.test_dataset] # Only retrieve the first MAX_LENGTH words in each document\n",
        "        neighbor_sets = [create_neighbor_set(node_set, p=self.p) for node_set in node_sets]\n",
        "        labels = [self.label_dict[label] for label, _ in self.test_dataset]\n",
        "        test_dataset = TextLevelGNNDataset(node_sets, neighbor_sets, public_edge_mask, labels)  # 传给TextLevelGNNDataset类，返回每张图对应的torch.LongTensor(tensor0, tensor1, tensor2, tensor3)\n",
        "\n",
        "        return train_dataset, validation_dataset, test_dataset, edge_stat, public_edge_mask  # 将训练集、验证集和测试集对应的每张图的信息封装后的长张量返回，以及边的统计和标记信息\n",
        "\n",
        "    def build_public_edge_mask(self, node_sets, neighbor_sets, min_freq=2):\n",
        "        edge_stat = torch.zeros(self.vocab_count, self.vocab_count)\n",
        "        for node_set, neighbor_set in zip(node_sets, neighbor_sets):\n",
        "            for neighbor in neighbor_set:\n",
        "                for to_node in neighbor:\n",
        "                    edge_stat[node_set, to_node] += 1\n",
        "        public_edge_mask = edge_stat < min_freq # mark True at uncommon edges\n",
        "        return edge_stat, public_edge_mask\n",
        "\n",
        "\n",
        "def create_neighbor_set(node_set, p=3):  # node_sets: 单个数据集(train/ val/ test)上的(batch_size，l), node_set = (max_length,)\n",
        "    if type(node_set[0]) != int:\n",
        "        raise ValueError('node_set should be a 1D list!')\n",
        "    if p < 0:\n",
        "        raise ValueError('p should be an integer >= 0!')\n",
        "    sequence_length = len(node_set)\n",
        "    neighbor_set = []\n",
        "    for i in range(sequence_length):\n",
        "        neighbor = []\n",
        "        for j in range(-p, p+1):\n",
        "            if 0 <= i + j < sequence_length:\n",
        "                neighbor.append(node_set[i+j])\n",
        "        neighbor_set.append(neighbor)\n",
        "    return neighbor_set\n",
        "\n",
        "\n",
        "def pad_custom_sequence(sequences):\n",
        "    '''\n",
        "    To pad different sequences into a padded tensor for training. The main purpose of this function is to separate different sequence, pad them in different ways and return padded sequences.\n",
        "    Input:\n",
        "        sequences <list>: A sequence with a length of 4, representing the node sets sequence in index 0, neighbor sets sequence in index 1, public edge mask sequence in index 2 and label sequence in index 3.\n",
        "                          And the length of each sequences are same as the batch size.\n",
        "                          sequences: [node_sets_sequence, neighbor_sets_sequence, public_edge_mask_sequence, label_sequence]\n",
        "    Return:\n",
        "        node_sets_sequence <torch.LongTensor>: The padded node sets sequence (works with batch_size >= 1).\n",
        "        neighbor_sets_sequence <torch.LongTensor>: The padded neighbor sets sequence (works with batch_size >= 1).\n",
        "        public_edge_mask_sequence <torch.BoolTensor>: The padded public edge mask sequence (works with batch_size >= 1).\n",
        "        label_sequence <torch.FloatTensor>: The padded label sequence (works with batch_size >= 1).\n",
        "    '''\n",
        "    node_sets_sequence = []  # list[tensor0, tensor1, tensor2, tensor3]\n",
        "    neighbor_sets_sequence = []\n",
        "    public_edge_mask_sequence = []\n",
        "    label_sequence = []\n",
        "    for node_sets, neighbor_sets, public_edge_mask, label in sequences:  # 依次从batch_size张图中取每张图的4个数据\n",
        "        node_sets_sequence.append(node_sets)\n",
        "        neighbor_sets_sequence.append(neighbor_sets)\n",
        "        public_edge_mask_sequence.append(public_edge_mask)\n",
        "        label_sequence.append(label) \n",
        "    node_sets_sequence = torch.nn.utils.rnn.pad_sequence(node_sets_sequence, batch_first=True, padding_value=1)  # 统一对齐batch_size下的句子节点为(max_sentence_length, d)（主要对句子长度小于max_sentence_length的句子补丁）\n",
        "    neighbor_sets_sequence, _ = padding_tensor(neighbor_sets_sequence)  # 统一对齐batch_size下的句子的每个节点对应的邻居节点，按max_sentence_length对齐\n",
        "    public_edge_mask_sequence, _ = padding_tensor(public_edge_mask_sequence)  # 统一对齐布尔型的标记张量\n",
        "    label_sequence = torch.nn.utils.rnn.pad_sequence(label_sequence, batch_first=True, padding_value=1)\n",
        "    return node_sets_sequence, neighbor_sets_sequence, public_edge_mask_sequence, label_sequence  # 返回对齐后的4个属性\n",
        "\n",
        "\n",
        "def padding_tensor(sequences, padding_idx=1):\n",
        "    '''\n",
        "    To pad tensor of different shape to be of the same shape, i.e. padding [tensor.rand(2, 3), tensor.rand(3, 5)] to a shape (2, 3, 5), where 0th dimension is batch_size, 1st and 2nd dimensions are padded.\n",
        "    Input:\n",
        "        sequences <list>: A list of tensors  # [tensor0, tensor1, tensor2, tensor3]\n",
        "        padding_idx <int>: The index that corresponds to the padding index\n",
        "    Return:\n",
        "        out_tensor <torch.tensor>: The padded tensor\n",
        "        mask <torch.tensor>: A boolean torch tensor where 1 (represents '<pad>') are marked as true\n",
        "    '''\n",
        "    num = len(sequences)\n",
        "    max_len_0 = max([s.shape[0] for s in sequences])\n",
        "    max_len_1 = max([s.shape[1] for s in sequences])\n",
        "    out_dims = (num, max_len_0, max_len_1)\n",
        "    out_tensor = sequences[0].data.new(*out_dims).fill_(padding_idx)\n",
        "    for i, tensor in enumerate(sequences):\n",
        "        len_0 = tensor.size(0)\n",
        "        len_1 = tensor.size(1)\n",
        "        out_tensor[i, :len_0, :len_1] = tensor\n",
        "    mask = out_tensor == padding_idx # Marking all places with padding_idx as mask\n",
        "    return out_tensor, mask\n",
        "\n",
        "\n",
        "class MessagePassing(nn.Module):  # 一张图上的处理\n",
        "    def __init__(self, vertice_count, input_size, out_size, dropout_rate=0, padding_idx=1):\n",
        "        super(MessagePassing, self).__init__()\n",
        "        self.vertice_count = vertice_count # |V|  r8语料库中有一万多个unique词\n",
        "        self.input_size = input_size # d  300维\n",
        "        self.out_size = out_size # c, r8, c=8\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.padding_idx = padding_idx  # 1\n",
        "        self.information_rate = nn.Parameter(torch.rand(self.vertice_count, 1)) # (|V|, 1), which means it is a column vector\n",
        "        self.linear = nn.Linear(self.input_size, self.out_size) # (d, c)  # 将300维映射到8维\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)  # 0\n",
        "\n",
        "    def forward(self, node_sets, embedded_node, embedded_meaning1, embedded_meaning2, edge_weight, embedded_neighbor_node):\n",
        "        # node_sets: (batch_size, l)  # 对应论文的N_i\n",
        "        # embedded_node: (batch_size, l, d)  # 对应论文的r_n\n",
        "        # edge_weight: (batch_size, max_sentence_length, max_neighbor_count)  # 对应论文e_an\n",
        "        # embedded_neighbor_node: (batch_size, max_sentence_length, max_neighbor_count, d)  对应论文的N_n^p\n",
        "        \n",
        "        ## meaning_embedded_neighbor: 每个节点的固定语义： m1, m2, m3，多的语义截取，不够的取0张量\n",
        "\n",
        "        tmp_tensor = (edge_weight.view(-1, 1) * embedded_neighbor_node.view(-1, self.input_size)).view(embedded_neighbor_node.shape) # (batch_size, max_sentence_length, max_neighbor_count, d)\n",
        "        tmp_tensor = tmp_tensor.masked_fill(tmp_tensor == 0, -1e18) # (batch_size, max_sentence_length, max_neighbor_count, d), mask for M such that masked places are marked as -1e18 # M不接收来自pad节点的邻居信息，如果是来自pad节点的则将其设置为无穷小\n",
        "        tmp_tensor = self.dropout(tmp_tensor)\n",
        "\n",
        "        # embedded_meaning1 # (batch_size, max_sentence_length, meaning_number, d)\n",
        "        embedded_meaning1 = embedded_meaning1.unsqueeze(2)\n",
        "        embedded_meaning2 = embedded_meaning2.unsqueeze(2)\n",
        "\n",
        "        tmp_tensor = torch.cat([tmp_tensor, embedded_meaning1, embedded_meaning2], dim=2)  # 将tmp_tensor在dim=2上拉长\n",
        "\n",
        "        M = tmp_tensor.max(dim=2)[0] # (batch_size, max_sentence_length, d), which is same shape as embedded_node (batch_size, l, d)\n",
        "        \n",
        "        information_rate = self.information_rate[node_sets] # (batch_size, l, 1)\n",
        "        information_rate = information_rate.masked_fill((node_sets == self.padding_idx).unsqueeze(-1), 1) # (batch_size, l, 1), Fill the information rate of the padding index as 1, such that new e_n = (1-i_r) * M + i_r * e_n = (1-1) * 0 + 1 * e_n = e_n (no update)\n",
        "        embedded_node = (1 - information_rate) * M + information_rate * embedded_node # (batch_size, l, d) \n",
        "        sum_embedded_node = embedded_node.sum(dim=1) # (batch_size, d)\n",
        "        x = F.relu(self.linear(sum_embedded_node)) # (batch_size, c)\n",
        "#         x = self.dropout(x) # if putting dropout with p=0.5 here, it is equivalent to wiping 4 choices out of 8 choices on the question sheet, which does not make sense. If a dropout layer is placed at here, it works the best when p=0 (disabled), followed by p=0.05, ..., p=0.5 (worst and does not even converge).\n",
        "        y = F.softmax(x, dim=1) # (batch_size, c) along the c dimension\n",
        "        return y\n",
        "\n",
        "\n",
        "class TextLevelGNN(nn.Module):\n",
        "    def __init__(self, pretrained_embeddings, pretrained_meaning_embeddings, out_size=8, dropout_rate=0, padding_idx=1):\n",
        "        super(TextLevelGNN, self).__init__()\n",
        "        self.out_size = out_size # c\n",
        "        self.padding_idx = padding_idx\n",
        "        self.weight_matrix = nn.Parameter(torch.randn(pretrained_embeddings.shape[0], pretrained_embeddings.shape[0])) # (|V|, |V|)        \n",
        "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False, padding_idx=self.padding_idx) # (|V|, d), # freeze=False（不冻结）,预训练的词向量值可以调整\n",
        "        ####################\n",
        "        self.meaning_embedding = nn.Embedding.from_pretrained(torch.tensor(pretrained_meaning_embeddings[0]), freeze=False, padding_idx=self.padding_idx)  # (|V|, d), # freeze=True（不冻结）,预训练的向量值可以调整\n",
        "        self.meaning_embedding2 = nn.Embedding.from_pretrained(torch.tensor(pretrained_meaning_embeddings[1]), freeze=False, padding_idx=self.padding_idx)  # 词的第二个语义\n",
        "        \n",
        "        self.message_passing = MessagePassing(vertice_count=pretrained_embeddings.shape[0], input_size=pretrained_embeddings.shape[1], out_size=self.out_size, dropout_rate=dropout_rate, padding_idx=self.padding_idx) # input_size: (d,); out_size: (c,)\n",
        "        self.public_edge_weight = nn.Parameter(torch.randn(1, 1)) # (1, 1)  二维张量\n",
        "\n",
        "    def forward(self, node_sets, neighbor_sets, public_edge_mask):\n",
        "        # node_sets: (batch_size, l)\n",
        "        # neighbor_sets: (batch_size, max_sentence_length, max_neighbor_count)\n",
        "        # neighbor_sets_mask: (batch_size, max_sentence_length, max_neighbor_count) (no need)\n",
        "        # public_edge_mask: (batch_size, max_sentence_length, max_neighbor_count)\n",
        "\n",
        "        ## meaning_embedded_neighbor: (batch_size, max_sentence_length, max_meaning_neighbor_count)  # 后续可以考虑为其增加权重操作\n",
        "        \n",
        "        embedded_node = self.embedding(node_sets) # (batch_size, l, d)  #将句子[12, 78,..., 987]转成各个词对应的Embedding向量\n",
        "        #############\n",
        "        embedded_meaning1 = self.meaning_embedding(node_sets)  # (batch_size, max_sentence, )根据节点id获得每个节点的一个meaning\n",
        "        embedded_meaning2 = self.meaning_embedding2(node_sets)\n",
        "\n",
        "        # 对应论文的e_an边权重\n",
        "        edge_weight = model.weight_matrix[node_sets.unsqueeze(2).repeat(1, 1, neighbor_sets.shape[-1]), neighbor_sets] # (batch_size, max_sentence_length, max_neighbor_count), neighbor_sets.shape[-1]: eg p=2, this expression=5; p=3, this expression=7. This is to first make node_sets to have same shape with neighbor_sets, then just do 1 query instead of 32*100 queries to speed up performance\n",
        "        a = edge_weight * ~public_edge_mask # (batch_size, max_sentence_length, max_neighbor_count)\n",
        "        b = self.public_edge_weight.unsqueeze(2).expand(1, public_edge_mask.shape[-2], public_edge_mask.shape[-1]) * public_edge_mask # (batch_size, max_sentence_length, max_neighbor_count)  # 偏置项\n",
        "        edge_weight = a + b # (batch_size, max_sentence_length, max_neighbor_count)\n",
        "        # 根据训练集上的embedding以及节点集合直接得出节点对应的邻居节点的词嵌入向量表示\n",
        "        embedded_neighbor_node = self.embedding(neighbor_sets) # (batch_size, max_sentece_length, max_neighbor_count, d)\n",
        "\n",
        "        # Apply mask to edge_weight, to mask and cut-off any relationships to the padding nodes  # 与'pad'节点相连的边不更新边权重e_an\n",
        "        edge_weight = edge_weight.masked_fill((node_sets.unsqueeze(2).repeat(1, 1, neighbor_sets.shape[-1]) == self.padding_idx) | (neighbor_sets == self.padding_idx), 0) # (batch_size, max_sentence_length, max_neighbor_count)\n",
        "\n",
        "        x = self.message_passing(node_sets, embedded_node, embedded_meaning1, embedded_meaning2, edge_weight, embedded_neighbor_node) # x: (batch_size, c)  # 每张图的预测标签\n",
        "        return x\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--cuda', default='0', type=str, required=False,\n",
        "                    help='Choosing which cuda to use')\n",
        "parser.add_argument('--embedding_size', default=300, type=int, required=False,\n",
        "                    help='Number of hidden units in each layer of the graph embedding part')\n",
        "parser.add_argument('--p', default=3, type=int, required=False,\n",
        "                    help='The window size')\n",
        "parser.add_argument('--min_freq', default=2, type=int, required=False,\n",
        "                    help='The minimum no. of occurrence for a word to be considered as a meaningful word. Words with less than this occurrence will be mapped to a globally shared embedding weight (to the <unk> token). It corresponds to the parameter k in the original paper.')\n",
        "parser.add_argument('--max_length', default=70, type=int, required=False,\n",
        "                    help='The max length of each document to be processed')\n",
        "parser.add_argument('--dropout', default=0, type=float, required=False,\n",
        "                    help='Dropout rate')\n",
        "parser.add_argument('--lr', default=1e-3, type=float, required=False,\n",
        "                    help='Initial learning rate')\n",
        "parser.add_argument('--lr_decay_factor', default=0.9, type=float, required=False,\n",
        "                    help='Multiplicative factor of learning rate decays')  # 学习率衰减的乘子\n",
        "parser.add_argument('--lr_decay_every', default=5, type=int, required=False,\n",
        "                    help='Decaying learning rate every ? epochs')\n",
        "parser.add_argument('--weight_decay', default=1e-4, type=float, required=False,\n",
        "                    help='Weight decay (L2 penalty)')\n",
        "parser.add_argument('--warm_up_epoch', default=0, type=int, required=False,\n",
        "                    help='Pretraining for ? epochs before early stopping to be in effect')\n",
        "parser.add_argument('--early_stopping_patience', default=10, type=int, required=False,\n",
        "                    help='Waiting for ? more epochs after the best epoch to see any further improvements')\n",
        "parser.add_argument('--early_stopping_criteria', default='loss', type=str, required=False,\n",
        "                    choices=['accuracy', 'loss'],\n",
        "                    help='Early stopping according to validation accuracy or validation loss')\n",
        "parser.add_argument(\"--epoch\", default=100, type=int, required=False,\n",
        "                    help='Number of epochs to train')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "tokenizer = Full_GloveTokenizer(filename='/content/drive/MyDrive/Colab_Notebooks/DATA/pretrained_model/Full_GloVe_dic.pkl')\n",
        "\n",
        "dataset = TextLevelGNNDatasetClass(train_filename='/content/drive/MyDrive/Colab_Notebooks/DATA/r8-train-all-terms.txt',\n",
        "                                   test_filename='/content/drive/MyDrive/Colab_Notebooks/DATA/r8-test-all-terms.txt',              \n",
        "                                   opted_filename='/content/drive/MyDrive/Colab_Notebooks/DATA/pretrained_model/OPTED_dic.pkl',\n",
        "                                   full_glove_pth='/content/drive/MyDrive/Colab_Notebooks/DATA/pretrained_model/Full_GloVe_dic.pkl',\n",
        "                                   train_validation_split=0.9,  # train = train*0.x\n",
        "                                   tokenizer=tokenizer,\n",
        "                                   p=args.p,\n",
        "                                   min_freq=args.min_freq,\n",
        "                                   MAX_LENGTH=args.max_length)\n",
        "\n",
        "train_loader = DataLoader(dataset.train_dataset, batch_size=128, shuffle=True, collate_fn=pad_custom_sequence)\n",
        "validation_loader = DataLoader(dataset.validation_dataset, batch_size=128, shuffle=True, collate_fn=pad_custom_sequence)\n",
        "test_loader = DataLoader(dataset.test_dataset, batch_size=128, shuffle=True, collate_fn=pad_custom_sequence)\n",
        "\n",
        "device = torch.device(f'cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# device = torch.device('cpu')  # GPU内存太小，直接用CPU计算\n",
        "if torch.cuda.is_available():\n",
        "  print(f'device: {device}')\n",
        "  print(f'name:{torch.cuda.get_device_name(0)}')\n",
        "  print(f'memory:{torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
        "\n",
        "model = TextLevelGNN(pretrained_embeddings=torch.tensor(dataset.embedding_matrix), pretrained_meaning_embeddings=dataset.meaning_matrix, dropout_rate=0).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "lr = args.lr\n",
        "lr_decay_factor = args.lr_decay_factor\n",
        "lr_decay_every = args.lr_decay_every\n",
        "weight_decay = args.weight_decay\n",
        "\n",
        "warm_up_epoch = args.warm_up_epoch\n",
        "early_stopping_patience = args.early_stopping_patience\n",
        "early_stopping_criteria = args.early_stopping_criteria\n",
        "best_epoch = 0 # Initialize\n",
        "\n",
        "training = {}  # {'accuracy':[], 'loss': []}\n",
        "validation = {}\n",
        "testing = {}\n",
        "training['accuracy'] = []\n",
        "training['loss'] = []\n",
        "validation['accuracy'] = []\n",
        "validation['loss'] = []\n",
        "testing['accuracy'] = []\n",
        "testing['loss'] = []\n",
        "\n",
        "for epoch in range(args.epoch):\n",
        "    model.train()  # 训练模式\n",
        "    train_loss = 0\n",
        "    train_correct_items = 0\n",
        "    previous_epoch_timestamp = time()\n",
        "\n",
        "    if epoch % lr_decay_every == 0: # Update optimizer for every lr_decay_every epochs\n",
        "        if epoch != 0: # When it is the first epoch, disable the lr_decay_factor\n",
        "            lr *= lr_decay_factor  # 乘以0.9的速度衰减\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # 更新优化器参数\n",
        "\n",
        "    for i, (node_sets, neighbor_sets, public_edge_masks, labels) in enumerate(train_loader):  # 批量取图数据\n",
        "        #print('Finished batch:', i)\n",
        "        node_sets = node_sets.to(device)\n",
        "        neighbor_sets = neighbor_sets.to(device)\n",
        "        public_edge_masks = public_edge_masks.to(device)\n",
        "        labels = labels.to(device)\n",
        "        prediction = model(node_sets, neighbor_sets, public_edge_masks)  # 调用TextLevelGNN类中的forward方法返回预测预测值，batch_size张图各自对应的预测结果\n",
        "        loss = criterion(prediction, labels).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_correct_items += (prediction.argmax(dim=1) == labels.argmax(dim=1)).sum().item()  # 获得训练集上总的预测正确的数量\n",
        "    train_accuracy = train_correct_items / len(dataset.train_dataset)  # 计算训练集上的准确率\n",
        "\n",
        "    model.eval()  # 评价模式\n",
        "    validation_loss = 0\n",
        "    validation_correct_items = 0\n",
        "    for i, (node_sets, neighbor_sets, public_edge_masks, labels) in enumerate(validation_loader):\n",
        "        node_sets = node_sets.to(device)\n",
        "        neighbor_sets = neighbor_sets.to(device)\n",
        "        public_edge_masks = public_edge_masks.to(device)\n",
        "        labels = labels.to(device)\n",
        "        prediction = model(node_sets, neighbor_sets, public_edge_masks)\n",
        "        loss = criterion(prediction, labels).to(device)\n",
        "        validation_loss += loss.item()\n",
        "        validation_correct_items += (prediction.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
        "    validation_accuracy = validation_correct_items / len(dataset.validation_dataset)\n",
        "\n",
        "#     model.eval()\n",
        "    test_loss = 0\n",
        "    test_correct_items = 0\n",
        "    for i, (node_sets, neighbor_sets, public_edge_masks, labels) in enumerate(test_loader):\n",
        "        node_sets = node_sets.to(device)\n",
        "        neighbor_sets = neighbor_sets.to(device)\n",
        "        public_edge_masks = public_edge_masks.to(device)\n",
        "        labels = labels.to(device)\n",
        "        prediction = model(node_sets, neighbor_sets, public_edge_masks)\n",
        "        loss = criterion(prediction, labels).to(device)\n",
        "        test_loss += loss.item()\n",
        "        test_correct_items += (prediction.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
        "    test_accuracy = test_correct_items / len(dataset.test_dataset)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}, Testing Loss: {test_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {validation_accuracy:.4f}, Testing Accuracy: {test_accuracy:.4f}, Time Used: {time()-previous_epoch_timestamp:.2f}s')\n",
        "    training['accuracy'].append(train_accuracy)\n",
        "    training['loss'].append(train_loss)\n",
        "    validation['accuracy'].append(validation_accuracy)\n",
        "    validation['loss'].append(validation_loss)\n",
        "    testing['accuracy'].append(test_accuracy)\n",
        "    testing['loss'].append(test_loss)\n",
        "\n",
        "    # add warmup mechanism for warm_up_epoch epochs\n",
        "    if epoch >= warm_up_epoch:\n",
        "        best_epoch = warm_up_epoch\n",
        "        # early stopping\n",
        "        if early_stopping_criteria == 'accuracy':\n",
        "            if validation['accuracy'][epoch] > validation['accuracy'][best_epoch]:\n",
        "                best_epoch = epoch\n",
        "            elif epoch >= best_epoch + early_stopping_patience:\n",
        "                print(f'Early stopping... (No further increase in validation accuracy) for consecutive {early_stopping_patience} epochs.')\n",
        "                break\n",
        "        if early_stopping_criteria == 'loss':\n",
        "            if validation['loss'][epoch] < validation['loss'][best_epoch]:\n",
        "                best_epoch = epoch\n",
        "            elif epoch >= best_epoch + early_stopping_patience:\n",
        "                print(f'Early stopping... (No further decrease in validation loss) for consecutive {early_stopping_patience} epochs.')\n",
        "                break\n",
        "    elif epoch + 1 == warm_up_epoch:\n",
        "        print('--- Warm up finished ---')\n",
        "\n",
        "# 保存结果至表格\n",
        "df = pd.concat([pd.DataFrame(training), pd.DataFrame(validation), pd.DataFrame(testing)], axis=1)\n",
        "df.columns = ['Training Accuracy', 'Training Loss', 'Validation Accuracy', 'Validation Loss', 'Testing Accuracy', 'Testing Loss']\n",
        "df.to_csv(f'/content/drive/MyDrive/Colab_Notebooks/DATA/r8-eb{args.embedding_size}-p{args.p}-k{args.min_freq}-max_l{args.max_length}-dp{args.dropout}-epoch{args.epoch}-n.csv') # Logging\n",
        "\n",
        "\n",
        "# 保存结果图像\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(8,4))\n",
        "\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax1.plot(training['loss'], label='Training Loss')\n",
        "ax1.plot(validation['loss'], label='Validation Loss')\n",
        "ax1.plot(testing['loss'], label='Testing Loss')\n",
        "ax1.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.plot(training['accuracy'], label='Training Accuracy')\n",
        "ax2.plot(validation['accuracy'], label='Validation Accuracy')\n",
        "ax2.plot(testing['accuracy'], label='Testing Accuracy')\n",
        "ax2.legend()\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.ylabel('Accuracy')\n",
        "plt.savefig(f'/content/drive/MyDrive/Colab_Notebooks/DATA/r8-eb{args.embedding_size}-p{args.p}-k{args.min_freq}-max_l{args.max_length}-dp{args.dropout}-epoch{args.epoch}-n.png', dpi=400, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "调参"
      ],
      "metadata": {
        "id": "HhOwQyfazPp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!CUDA_LAUNCH_BLOCKING=1 python parsing.py --cuda=0 --embedding_size=300 --p=3 --min_freq=2 --max_length=70 --dropout=0 --epoch=80\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uL15K1MzPAS",
        "outputId": "a46cf96c-cb15-4c6f-d907-ed60434f7981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcmalloc: large alloc 1115734016 bytes == 0x55aa6d736000 @  0x7f59da46d1e7 0x7f59d7ead46e 0x7f59d7efdc7b 0x7f59d7f00e83 0x7f59d7f0107b 0x7f59d7fa2761 0x55aa36c554b0 0x55aa36c55240 0x55aa36cc90f3 0x55aa36cc39ee 0x55aa36c5748c 0x55aa36c98159 0x55aa36c950a4 0x55aa36c55d49 0x55aa36cc994f 0x55aa36cc39ee 0x55aa36cc36f3 0x55aa36d8d4c2 0x55aa36d8d83d 0x55aa36d8d6e6 0x55aa36d65163 0x55aa36d64e0c 0x7f59d9257bf7 0x55aa36d64cea\n",
            "device: cuda:0\n",
            "name:Tesla K80\n",
            "memory:11.996954624\n",
            "Epoch: 1, Training Loss: 39.5684, Validation Loss: 4.9096, Testing Loss: 14.9265, Training Accuracy: 0.6526, Validation Accuracy: 0.7741, Testing Accuracy: 0.8090, Time Used: 26.35s\n",
            "Epoch: 2, Training Loss: 28.1148, Validation Loss: 4.3516, Testing Loss: 13.1242, Training Accuracy: 0.8367, Validation Accuracy: 0.7869, Testing Accuracy: 0.8228, Time Used: 26.15s\n",
            "Epoch: 3, Training Loss: 22.6802, Validation Loss: 3.3564, Testing Loss: 10.1777, Training Accuracy: 0.8655, Validation Accuracy: 0.8342, Testing Accuracy: 0.8483, Time Used: 26.14s\n",
            "Epoch: 4, Training Loss: 17.7708, Validation Loss: 2.5784, Testing Loss: 8.0779, Training Accuracy: 0.8859, Validation Accuracy: 0.8470, Testing Accuracy: 0.8593, Time Used: 26.13s\n",
            "Epoch: 5, Training Loss: 13.6079, Validation Loss: 1.9423, Testing Loss: 6.4530, Training Accuracy: 0.8963, Validation Accuracy: 0.8434, Testing Accuracy: 0.8520, Time Used: 26.06s\n",
            "Epoch: 6, Training Loss: 6.6627, Validation Loss: 0.7267, Testing Loss: 2.5520, Training Accuracy: 0.8930, Validation Accuracy: 0.8889, Testing Accuracy: 0.8872, Time Used: 26.08s\n",
            "Epoch: 7, Training Loss: 3.7020, Validation Loss: 0.6241, Testing Loss: 2.3067, Training Accuracy: 0.9374, Validation Accuracy: 0.9107, Testing Accuracy: 0.9045, Time Used: 26.02s\n",
            "Epoch: 8, Training Loss: 3.2435, Validation Loss: 0.6158, Testing Loss: 2.2139, Training Accuracy: 0.9435, Validation Accuracy: 0.9035, Testing Accuracy: 0.9105, Time Used: 26.09s\n",
            "Epoch: 9, Training Loss: 2.8633, Validation Loss: 0.5649, Testing Loss: 2.0778, Training Accuracy: 0.9473, Validation Accuracy: 0.9071, Testing Accuracy: 0.9169, Time Used: 26.07s\n",
            "Epoch: 10, Training Loss: 2.6919, Validation Loss: 0.5343, Testing Loss: 1.9680, Training Accuracy: 0.9514, Validation Accuracy: 0.9107, Testing Accuracy: 0.9251, Time Used: 26.05s\n",
            "Epoch: 11, Training Loss: 2.4639, Validation Loss: 0.4106, Testing Loss: 1.6827, Training Accuracy: 0.9678, Validation Accuracy: 0.9508, Testing Accuracy: 0.9497, Time Used: 26.35s\n",
            "Epoch: 12, Training Loss: 1.9725, Validation Loss: 0.4028, Testing Loss: 1.6693, Training Accuracy: 0.9765, Validation Accuracy: 0.9563, Testing Accuracy: 0.9456, Time Used: 26.42s\n",
            "Epoch: 13, Training Loss: 1.9388, Validation Loss: 0.3719, Testing Loss: 1.6023, Training Accuracy: 0.9718, Validation Accuracy: 0.9526, Testing Accuracy: 0.9484, Time Used: 26.18s\n",
            "Epoch: 14, Training Loss: 1.6095, Validation Loss: 0.3685, Testing Loss: 1.6722, Training Accuracy: 0.9812, Validation Accuracy: 0.9526, Testing Accuracy: 0.9456, Time Used: 26.22s\n",
            "Epoch: 15, Training Loss: 1.5213, Validation Loss: 0.4011, Testing Loss: 1.5891, Training Accuracy: 0.9826, Validation Accuracy: 0.9599, Testing Accuracy: 0.9497, Time Used: 26.24s\n",
            "Epoch: 16, Training Loss: 1.5492, Validation Loss: 0.4019, Testing Loss: 1.5989, Training Accuracy: 0.9824, Validation Accuracy: 0.9472, Testing Accuracy: 0.9493, Time Used: 26.22s\n",
            "Epoch: 17, Training Loss: 1.2755, Validation Loss: 0.4606, Testing Loss: 1.7838, Training Accuracy: 0.9872, Validation Accuracy: 0.9399, Testing Accuracy: 0.9447, Time Used: 26.15s\n",
            "Epoch: 18, Training Loss: 1.1541, Validation Loss: 0.3678, Testing Loss: 1.4498, Training Accuracy: 0.9899, Validation Accuracy: 0.9472, Testing Accuracy: 0.9534, Time Used: 26.17s\n",
            "Epoch: 19, Training Loss: 1.0196, Validation Loss: 0.3776, Testing Loss: 1.4472, Training Accuracy: 0.9907, Validation Accuracy: 0.9508, Testing Accuracy: 0.9534, Time Used: 26.27s\n",
            "Epoch: 20, Training Loss: 0.9197, Validation Loss: 0.3672, Testing Loss: 1.4290, Training Accuracy: 0.9923, Validation Accuracy: 0.9526, Testing Accuracy: 0.9561, Time Used: 26.35s\n",
            "Epoch: 21, Training Loss: 1.1730, Validation Loss: 0.3778, Testing Loss: 1.5288, Training Accuracy: 0.9876, Validation Accuracy: 0.9526, Testing Accuracy: 0.9511, Time Used: 26.22s\n",
            "Epoch: 22, Training Loss: 0.9571, Validation Loss: 0.3340, Testing Loss: 1.4906, Training Accuracy: 0.9909, Validation Accuracy: 0.9508, Testing Accuracy: 0.9539, Time Used: 26.21s\n",
            "Epoch: 23, Training Loss: 0.8182, Validation Loss: 0.3212, Testing Loss: 1.3439, Training Accuracy: 0.9951, Validation Accuracy: 0.9581, Testing Accuracy: 0.9625, Time Used: 26.17s\n",
            "Epoch: 24, Training Loss: 0.7085, Validation Loss: 0.3379, Testing Loss: 1.4257, Training Accuracy: 0.9955, Validation Accuracy: 0.9563, Testing Accuracy: 0.9557, Time Used: 26.24s\n",
            "Epoch: 25, Training Loss: 0.6720, Validation Loss: 0.3580, Testing Loss: 1.4130, Training Accuracy: 0.9959, Validation Accuracy: 0.9545, Testing Accuracy: 0.9566, Time Used: 26.27s\n",
            "Epoch: 26, Training Loss: 1.0390, Validation Loss: 0.3530, Testing Loss: 1.4047, Training Accuracy: 0.9925, Validation Accuracy: 0.9526, Testing Accuracy: 0.9593, Time Used: 26.42s\n",
            "Epoch: 27, Training Loss: 0.8159, Validation Loss: 0.3804, Testing Loss: 1.3049, Training Accuracy: 0.9913, Validation Accuracy: 0.9545, Testing Accuracy: 0.9625, Time Used: 26.32s\n",
            "Epoch: 28, Training Loss: 0.6052, Validation Loss: 0.3527, Testing Loss: 1.3203, Training Accuracy: 0.9968, Validation Accuracy: 0.9508, Testing Accuracy: 0.9593, Time Used: 26.40s\n",
            "Epoch: 29, Training Loss: 0.5913, Validation Loss: 0.3527, Testing Loss: 1.3357, Training Accuracy: 0.9962, Validation Accuracy: 0.9545, Testing Accuracy: 0.9607, Time Used: 26.23s\n",
            "Epoch: 30, Training Loss: 0.5435, Validation Loss: 0.3302, Testing Loss: 1.3000, Training Accuracy: 0.9970, Validation Accuracy: 0.9508, Testing Accuracy: 0.9612, Time Used: 26.51s\n",
            "Epoch: 31, Training Loss: 0.7387, Validation Loss: 0.3421, Testing Loss: 1.4455, Training Accuracy: 0.9937, Validation Accuracy: 0.9545, Testing Accuracy: 0.9566, Time Used: 26.38s\n",
            "Epoch: 32, Training Loss: 0.5776, Validation Loss: 0.3531, Testing Loss: 1.3316, Training Accuracy: 0.9957, Validation Accuracy: 0.9545, Testing Accuracy: 0.9607, Time Used: 26.39s\n",
            "Epoch: 33, Training Loss: 0.5215, Validation Loss: 0.3339, Testing Loss: 1.3212, Training Accuracy: 0.9966, Validation Accuracy: 0.9508, Testing Accuracy: 0.9593, Time Used: 26.43s\n",
            "Epoch: 34, Training Loss: 0.4595, Validation Loss: 0.3303, Testing Loss: 1.3323, Training Accuracy: 0.9974, Validation Accuracy: 0.9508, Testing Accuracy: 0.9589, Time Used: 26.47s\n",
            "Epoch: 35, Training Loss: 0.4293, Validation Loss: 0.3586, Testing Loss: 1.3762, Training Accuracy: 0.9982, Validation Accuracy: 0.9526, Testing Accuracy: 0.9566, Time Used: 26.34s\n",
            "Epoch: 36, Training Loss: 0.5866, Validation Loss: 0.3563, Testing Loss: 1.3216, Training Accuracy: 0.9955, Validation Accuracy: 0.9508, Testing Accuracy: 0.9612, Time Used: 26.35s\n",
            "Epoch: 37, Training Loss: 0.4927, Validation Loss: 0.3054, Testing Loss: 1.2803, Training Accuracy: 0.9968, Validation Accuracy: 0.9526, Testing Accuracy: 0.9589, Time Used: 25.88s\n",
            "Epoch: 38, Training Loss: 0.4124, Validation Loss: 0.3140, Testing Loss: 1.2958, Training Accuracy: 0.9978, Validation Accuracy: 0.9490, Testing Accuracy: 0.9598, Time Used: 25.87s\n",
            "Epoch: 39, Training Loss: 0.3832, Validation Loss: 0.3323, Testing Loss: 1.3431, Training Accuracy: 0.9976, Validation Accuracy: 0.9545, Testing Accuracy: 0.9571, Time Used: 25.87s\n",
            "Epoch: 40, Training Loss: 0.3556, Validation Loss: 0.3412, Testing Loss: 1.2590, Training Accuracy: 0.9980, Validation Accuracy: 0.9526, Testing Accuracy: 0.9607, Time Used: 25.82s\n",
            "Epoch: 41, Training Loss: 0.5353, Validation Loss: 0.3386, Testing Loss: 1.2927, Training Accuracy: 0.9957, Validation Accuracy: 0.9508, Testing Accuracy: 0.9616, Time Used: 26.22s\n",
            "Epoch: 42, Training Loss: 0.4130, Validation Loss: 0.3173, Testing Loss: 1.3080, Training Accuracy: 0.9976, Validation Accuracy: 0.9490, Testing Accuracy: 0.9603, Time Used: 26.29s\n",
            "Epoch: 43, Training Loss: 0.3592, Validation Loss: 0.3439, Testing Loss: 1.3512, Training Accuracy: 0.9978, Validation Accuracy: 0.9508, Testing Accuracy: 0.9589, Time Used: 26.32s\n",
            "Epoch: 44, Training Loss: 0.3347, Validation Loss: 0.2943, Testing Loss: 1.2850, Training Accuracy: 0.9984, Validation Accuracy: 0.9508, Testing Accuracy: 0.9612, Time Used: 26.32s\n",
            "Epoch: 45, Training Loss: 0.3223, Validation Loss: 0.3110, Testing Loss: 1.2530, Training Accuracy: 0.9978, Validation Accuracy: 0.9526, Testing Accuracy: 0.9584, Time Used: 26.28s\n",
            "Epoch: 46, Training Loss: 0.4722, Validation Loss: 0.3042, Testing Loss: 1.2968, Training Accuracy: 0.9968, Validation Accuracy: 0.9526, Testing Accuracy: 0.9616, Time Used: 26.37s\n",
            "Epoch: 47, Training Loss: 0.3602, Validation Loss: 0.3120, Testing Loss: 1.2836, Training Accuracy: 0.9980, Validation Accuracy: 0.9526, Testing Accuracy: 0.9612, Time Used: 26.19s\n",
            "Epoch: 48, Training Loss: 0.3179, Validation Loss: 0.3049, Testing Loss: 1.2692, Training Accuracy: 0.9980, Validation Accuracy: 0.9581, Testing Accuracy: 0.9598, Time Used: 26.27s\n",
            "Epoch: 49, Training Loss: 0.2964, Validation Loss: 0.3506, Testing Loss: 1.2520, Training Accuracy: 0.9986, Validation Accuracy: 0.9563, Testing Accuracy: 0.9593, Time Used: 26.27s\n",
            "Epoch: 50, Training Loss: 0.2961, Validation Loss: 0.3092, Testing Loss: 1.2606, Training Accuracy: 0.9980, Validation Accuracy: 0.9545, Testing Accuracy: 0.9593, Time Used: 26.31s\n",
            "Epoch: 51, Training Loss: 0.4261, Validation Loss: 0.3153, Testing Loss: 1.3536, Training Accuracy: 0.9972, Validation Accuracy: 0.9526, Testing Accuracy: 0.9598, Time Used: 26.24s\n",
            "Epoch: 52, Training Loss: 0.3268, Validation Loss: 0.3312, Testing Loss: 1.3298, Training Accuracy: 0.9980, Validation Accuracy: 0.9545, Testing Accuracy: 0.9598, Time Used: 26.32s\n",
            "Epoch: 53, Training Loss: 0.2926, Validation Loss: 0.3434, Testing Loss: 1.3288, Training Accuracy: 0.9982, Validation Accuracy: 0.9563, Testing Accuracy: 0.9616, Time Used: 26.13s\n",
            "Epoch: 54, Training Loss: 0.2732, Validation Loss: 0.3215, Testing Loss: 1.2627, Training Accuracy: 0.9984, Validation Accuracy: 0.9581, Testing Accuracy: 0.9607, Time Used: 26.31s\n",
            "Epoch: 55, Training Loss: 0.2593, Validation Loss: 0.2743, Testing Loss: 1.2371, Training Accuracy: 0.9986, Validation Accuracy: 0.9617, Testing Accuracy: 0.9635, Time Used: 26.21s\n",
            "Epoch: 56, Training Loss: 0.4069, Validation Loss: 0.3022, Testing Loss: 1.2990, Training Accuracy: 0.9974, Validation Accuracy: 0.9526, Testing Accuracy: 0.9580, Time Used: 26.31s\n",
            "Epoch: 57, Training Loss: 0.2991, Validation Loss: 0.3205, Testing Loss: 1.2385, Training Accuracy: 0.9980, Validation Accuracy: 0.9581, Testing Accuracy: 0.9612, Time Used: 26.24s\n",
            "Epoch: 58, Training Loss: 0.2726, Validation Loss: 0.3356, Testing Loss: 1.2795, Training Accuracy: 0.9986, Validation Accuracy: 0.9581, Testing Accuracy: 0.9616, Time Used: 26.21s\n",
            "Epoch: 59, Training Loss: 0.2478, Validation Loss: 0.3400, Testing Loss: 1.2671, Training Accuracy: 0.9986, Validation Accuracy: 0.9599, Testing Accuracy: 0.9598, Time Used: 26.18s\n",
            "Epoch: 60, Training Loss: 0.2418, Validation Loss: 0.3384, Testing Loss: 1.2497, Training Accuracy: 0.9988, Validation Accuracy: 0.9581, Testing Accuracy: 0.9621, Time Used: 26.29s\n",
            "Epoch: 61, Training Loss: 0.3704, Validation Loss: 0.3260, Testing Loss: 1.2398, Training Accuracy: 0.9976, Validation Accuracy: 0.9581, Testing Accuracy: 0.9598, Time Used: 26.17s\n",
            "Epoch: 62, Training Loss: 0.2772, Validation Loss: 0.3143, Testing Loss: 1.2907, Training Accuracy: 0.9986, Validation Accuracy: 0.9599, Testing Accuracy: 0.9630, Time Used: 26.26s\n",
            "Epoch: 63, Training Loss: 0.2553, Validation Loss: 0.2994, Testing Loss: 1.2933, Training Accuracy: 0.9986, Validation Accuracy: 0.9599, Testing Accuracy: 0.9616, Time Used: 26.12s\n",
            "Epoch: 64, Training Loss: 0.2391, Validation Loss: 0.3081, Testing Loss: 1.2354, Training Accuracy: 0.9988, Validation Accuracy: 0.9581, Testing Accuracy: 0.9630, Time Used: 26.20s\n",
            "Epoch: 65, Training Loss: 0.2295, Validation Loss: 0.3078, Testing Loss: 1.3079, Training Accuracy: 0.9988, Validation Accuracy: 0.9581, Testing Accuracy: 0.9621, Time Used: 26.13s\n",
            "Epoch: 66, Training Loss: 0.3420, Validation Loss: 0.3140, Testing Loss: 1.2585, Training Accuracy: 0.9988, Validation Accuracy: 0.9581, Testing Accuracy: 0.9625, Time Used: 26.20s\n",
            "Epoch: 67, Training Loss: 0.2667, Validation Loss: 0.2978, Testing Loss: 1.2603, Training Accuracy: 0.9988, Validation Accuracy: 0.9581, Testing Accuracy: 0.9616, Time Used: 26.01s\n",
            "Epoch: 68, Training Loss: 0.2417, Validation Loss: 0.2935, Testing Loss: 1.2376, Training Accuracy: 0.9990, Validation Accuracy: 0.9581, Testing Accuracy: 0.9616, Time Used: 26.00s\n",
            "Epoch: 69, Training Loss: 0.2292, Validation Loss: 0.2850, Testing Loss: 1.2713, Training Accuracy: 0.9984, Validation Accuracy: 0.9581, Testing Accuracy: 0.9607, Time Used: 26.07s\n",
            "Epoch: 70, Training Loss: 0.2150, Validation Loss: 0.2925, Testing Loss: 1.2412, Training Accuracy: 0.9988, Validation Accuracy: 0.9599, Testing Accuracy: 0.9616, Time Used: 26.02s\n",
            "Epoch: 71, Training Loss: 0.3067, Validation Loss: 0.2798, Testing Loss: 1.2044, Training Accuracy: 0.9982, Validation Accuracy: 0.9617, Testing Accuracy: 0.9630, Time Used: 26.02s\n",
            "Epoch: 72, Training Loss: 0.2546, Validation Loss: 0.3180, Testing Loss: 1.2258, Training Accuracy: 0.9988, Validation Accuracy: 0.9581, Testing Accuracy: 0.9630, Time Used: 26.15s\n",
            "Epoch: 73, Training Loss: 0.2270, Validation Loss: 0.3079, Testing Loss: 1.3105, Training Accuracy: 0.9988, Validation Accuracy: 0.9599, Testing Accuracy: 0.9625, Time Used: 26.05s\n",
            "Epoch: 74, Training Loss: 0.2185, Validation Loss: 0.2980, Testing Loss: 1.2141, Training Accuracy: 0.9984, Validation Accuracy: 0.9563, Testing Accuracy: 0.9630, Time Used: 25.98s\n",
            "Epoch: 75, Training Loss: 0.2125, Validation Loss: 0.2894, Testing Loss: 1.2183, Training Accuracy: 0.9988, Validation Accuracy: 0.9581, Testing Accuracy: 0.9635, Time Used: 26.16s\n",
            "Epoch: 76, Training Loss: 0.2951, Validation Loss: 0.2884, Testing Loss: 1.2165, Training Accuracy: 0.9984, Validation Accuracy: 0.9545, Testing Accuracy: 0.9635, Time Used: 26.09s\n",
            "Epoch: 77, Training Loss: 0.2436, Validation Loss: 0.2865, Testing Loss: 1.2134, Training Accuracy: 0.9988, Validation Accuracy: 0.9599, Testing Accuracy: 0.9662, Time Used: 25.97s\n",
            "Epoch: 78, Training Loss: 0.2237, Validation Loss: 0.3110, Testing Loss: 1.2221, Training Accuracy: 0.9990, Validation Accuracy: 0.9599, Testing Accuracy: 0.9639, Time Used: 26.06s\n",
            "Epoch: 79, Training Loss: 0.2269, Validation Loss: 0.2970, Testing Loss: 1.2352, Training Accuracy: 0.9990, Validation Accuracy: 0.9581, Testing Accuracy: 0.9639, Time Used: 25.92s\n",
            "Epoch: 80, Training Loss: 0.2021, Validation Loss: 0.2994, Testing Loss: 1.4098, Training Accuracy: 0.9986, Validation Accuracy: 0.9599, Testing Accuracy: 0.9616, Time Used: 26.28s\n"
          ]
        }
      ]
    }
  ]
}