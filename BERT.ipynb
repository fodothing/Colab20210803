{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hshuai97/Colab20210803/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYs7t2A383i9"
      },
      "source": [
        "Using BERT or RoBERTa model to classification several datasets (20ng, r8, r52, oh, mr)\n",
        "\n",
        "Reference:\n",
        "\n",
        "1. [Huggingface](https://github.com/huggingface/transformers/blob/main/README_zh-hans.md)\n",
        "\n",
        "2. [Colab1: Huggingface pytorch transformer](https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/huggingface_pytorch-transformers.ipynb)\n",
        "\n",
        "3. [Colab2: Sentiment analysis using roberta](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb#scrollTo=HMqQTafXEaei)\n",
        "\n",
        "4. [Text classification with BERT in PyTorch:](https://github.com/nlptown/nlp-notebooks/blob/master/Text%20classification%20with%20BERT%20in%20PyTorch.ipynb)\n",
        "\n",
        "5. [Distilbert for multilabel text classification](https://github.comDhavalTaunk08/NLP_scripts/blob/master/Transformers_multilabel_distilbert.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFEO-za3_frf"
      },
      "source": [
        "# Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nv_hQfp8-Ta6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50510e7a-ed61-455a-af1f-d9783da5e590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 4.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 35.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 43.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 24.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=1415e8fce7f80ccc2600a8984f38f39eeb187a519fd5d18e066f5b23ea95958c\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import transformers\n",
        "except ModuleNotFoundError:\n",
        "  !pip install transformers\n",
        "\n",
        "try:\n",
        "  import sentencepiece\n",
        "except ModuleNotFoundError:\n",
        "  !pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlNAXV9i_n4r"
      },
      "source": [
        "# Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PKqFftY0sQF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a1dfd8e-91fa-4c0a-b698-a90acae81baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing parsing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile parsing.py\n",
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def data(dataset_name):\n",
        "    NAME = dataset_name\n",
        "    if NAME not in ['20ng', 'r8', 'r52', 'oh', 'mr']:\n",
        "      raise ValueError('The dataset is not support')\n",
        "\n",
        "    PATH = '/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/data/'+NAME\n",
        "\n",
        "    train_texts = []\n",
        "    train_labels = []\n",
        "    with open(os.path.join(PATH, NAME+'-train-stemmed.txt'), 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        for line in data:\n",
        "          t = line.split('\\t')\n",
        "          train_texts.append(t[1])\n",
        "          train_labels.append(t[0])\n",
        "\n",
        "    dev_texts = []\n",
        "    dev_labels = []\n",
        "    with open(os.path.join(PATH, NAME+'-dev-stemmed.txt'), 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        for line in data:\n",
        "          t = line.split('\\t')\n",
        "          dev_texts.append(t[1])\n",
        "          dev_labels.append(t[0])\n",
        "\n",
        "    test_texts = []\n",
        "    test_labels = []\n",
        "    with open(os.path.join(PATH, NAME+'-test-stemmed.txt'), 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        for line in data:\n",
        "          t = line.split('\\t')\n",
        "          test_texts.append(t[1])\n",
        "          test_labels.append(t[0])\n",
        "\n",
        "    target_names = list(set(train_labels))\n",
        "    label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
        "\n",
        "    print(\"Train size:\", len(train_texts))\n",
        "    print(\"Dev size:\", len(dev_texts))\n",
        "    print(\"Test size:\", len(test_texts))\n",
        "    print(f'labels: {label2idx}')\n",
        "\n",
        "    return train_texts,  train_labels, dev_texts, dev_labels, test_texts, test_labels, label2idx\n",
        "\n",
        "def batch(inputs, y, batch_size, shuffle=True):\n",
        "  input_ids = inputs.input_ids\n",
        "  attention_mask = inputs.attention_mask\n",
        "\n",
        "  data = TensorDataset(input_ids, attention_mask, y)\n",
        "\n",
        "  dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
        "  \n",
        "  return dataloader\n",
        "\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self, num_class, MODEL):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = AutoModel.from_pretrained(MODEL)\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.4)\n",
        "        self.classifier = torch.nn.Linear(768, num_class)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "def train(model, tr_inputs, dev_inputs, epoch):\n",
        "    loss_func = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4, lr=LEARNING_RATE)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    no_improv = 0  # No improvement on dev set\n",
        "    PATIENCE = 8  # Patience on dev set to finish training\n",
        "    for e in range(epoch):\n",
        "      improved = ''\n",
        "      model.train()\n",
        "\n",
        "      for s, ba in enumerate(tr_inputs):\n",
        "        b = tuple(t.to(device) for t in ba)\n",
        "        input_ids, attention_mask, y = b\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = loss_func(outputs, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      if e % 2 == 0:\n",
        "        val_acc = dev(model, dev_inputs)\n",
        "        if val_acc > best_acc:\n",
        "          best_acc = val_acc\n",
        "          no_improve = 0\n",
        "          improved = '*'\n",
        "          torch.save(model, f'/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/model/bert_model.pkl')\n",
        "        else:\n",
        "          no_improve += 2\n",
        "        print(f'Epoch:{e}, train loss:{loss.item():.6f}, val acc: {val_acc:.4f}, {improved}')\n",
        "        if no_improve >=PATIENCE:\n",
        "          print('No improvement on development set. Early stop training.')\n",
        "          break\n",
        "\n",
        "    return model\n",
        "\n",
        "def dev(model, dev_inputs):\n",
        "  model.eval()\n",
        "\n",
        "  total_pred = 0.0\n",
        "  correct = 0.0\n",
        "\n",
        "  for s, ba in enumerate(dev_inputs):\n",
        "      b = tuple(t.to(device) for t in ba)\n",
        "      input_ids, attention_mask, y = b\n",
        "\n",
        "      with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        \n",
        "        correct_pred = torch.sum(pred==y)\n",
        "        correct += correct_pred\n",
        "        total_pred += len(y)\n",
        "  \n",
        "  return torch.div(correct, total_pred)  # Acc on dev set\n",
        "\n",
        "def test(model, te_inputs):\n",
        "  model.eval()\n",
        "\n",
        "  total_pred = 0.0\n",
        "  correct = 0.0\n",
        "\n",
        "  for s, ba in enumerate(te_inputs):\n",
        "    b = tuple(t.to(device) for t in ba)\n",
        "    input_ids, attention_mask, y = b\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(input_ids, attention_mask)\n",
        "      pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "      correct_pred = torch.sum(pred==y)\n",
        "      correct += correct_pred\n",
        "      total_pred += len(y)\n",
        "\n",
        "  return torch.div(correct, total_pred)  # Test set acc.\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dataset', required=True, type=str, default='mr',help='dataset name')\n",
        "parser.add_argument('--max_len', required=True, type=int)\n",
        "parser.add_argument('--epoch', required=True, type=int, default=50)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "\n",
        "MAX_LEN = args.max_len\n",
        "EPOCH = args.epoch\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 1e-04\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MODEL = 'bert-base-uncased'  # 'roberta-base',  'bert-base-uncased', \n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "t_texts, t_labels, d_texts, d_labels, te_texts, te_labels, label2idx = data(args.dataset)\n",
        "\n",
        "tr_y = torch.tensor([label2idx[t] for t in t_labels])\n",
        "d_y = torch.tensor([label2idx[t] for t in d_labels])\n",
        "te_y = torch.tensor([label2idx[t] for t in te_labels])\n",
        "\n",
        "train_inputs = tokenizer(t_texts, max_length=MAX_LEN, truncation=True, padding=True, return_tensors='pt')\n",
        "dev_inputs= tokenizer(d_texts, max_length=MAX_LEN, truncation=True, padding=True, return_tensors='pt')\n",
        "test_inputs = tokenizer(te_texts, max_length=MAX_LEN, truncation=True, padding=True, return_tensors='pt')\n",
        "print(train_inputs.keys())\n",
        "\n",
        "inputs_tr= batch(train_inputs, tr_y, BATCH_SIZE, shuffle=True)  # Batching for training\n",
        "inputs_dev = batch(dev_inputs, d_y, BATCH_SIZE, shuffle=False)\n",
        "inputs_te = batch(test_inputs, te_y, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model = BERTClass(len(label2idx), MODEL)\n",
        "model.to(device)\n",
        "\n",
        "final_model= train(model, inputs_tr, inputs_dev, epoch=EPOCH)\n",
        "\n",
        "res = test(final_model, inputs_te)\n",
        "print(f'Test accuracy: {res.cpu().numpy():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "xw9q75JFeTDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python parsing.py --dataset='oh' --max_len=135 --epoch=50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxOWFxsz9nQw",
        "outputId": "aafa1769-c018-4ced-a273-8600364ce0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 14.4kB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 460kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 427kB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 516kB/s]\n",
            "Train size: 3021\n",
            "Dev size: 336\n",
            "Test size: 4043\n",
            "labels: {'C04': 0, 'C15': 1, 'C14': 2, 'C22': 3, 'C18': 4, 'C23': 5, 'C06': 6, 'C13': 7, 'C03': 8, 'C16': 9, 'C07': 10, 'C17': 11, 'C12': 12, 'C05': 13, 'C08': 14, 'C10': 15, 'C20': 16, 'C02': 17, 'C09': 18, 'C11': 19, 'C01': 20, 'C19': 21, 'C21': 22}\n",
            "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
            "Downloading: 100% 420M/420M [00:09<00:00, 46.0MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fFEO-za3_frf"
      ],
      "name": "BERT",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "130dCViOFwOJQNvynOEXASb0JpJvyW3qo",
      "authorship_tag": "ABX9TyPjDISBpkqiUmSc966NVZRE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}