{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextLevelGNN_v4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1AZ7Wmc4Wi9pFiwLJT8WPPAOqEcts3B77",
      "authorship_tag": "ABX9TyM88iGM8S76RJJHIT9js7sw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hshuai97/Colab20210803/blob/main/TextLevelGNN_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Lianzhe Huang 源代码地址: [source link](https://github.com/mojave-pku/TextLevelGCN)\n",
        "2.  v1版本：[source link](https://colab.research.google.com/drive/1i5ySiMxus-pV_fCbk6yxAof6JL9qyeIQ#scrollTo=ww9wY6vugDpl)\n",
        "3. v2版本：[source link]( https://colab.research.google.com/drive/1GVaPym3UMeBLSCGFJ5CQyc_t7Ra1wzgy#scrollTo=lwY3NTaygDor)\n",
        "\n",
        "4. 在v2的版本上，使用softmax计算每个语义的重要程度: $\\alpha_i=\\frac{exp(S_i)}{\\sum_jexp(S_j)}$ 即GCN+GAT"
      ],
      "metadata": {
        "id": "HtT1D0Pm5RVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install libraries"
      ],
      "metadata": {
        "id": "g_jdf-wyHwNF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KZAbumQ5IcO",
        "outputId": "2777202f-abbc-4c71-9d12-cbeb5db49094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl-cu111\n",
            "  Downloading https://data.dgl.ai/wheels/dgl_cu111-0.8.0.post1-cp37-cp37m-manylinux1_x86_64.whl (252.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 252.7 MB 52 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (4.63.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.6.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.21.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2.10)\n",
            "Installing collected packages: dgl-cu111\n",
            "Successfully installed dgl-cu111-0.8.0.post1\n",
            "Collecting word2vec\n",
            "  Downloading word2vec-0.11.1.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 872 kB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.21.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.1.0)\n",
            "Building wheels for collected packages: word2vec\n",
            "  Building wheel for word2vec (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2vec: filename=word2vec-0.11.1-py2.py3-none-any.whl size=156420 sha256=574b2d03bddd4f28216fa604336128b1659d6f95e85c0ae23127564cdedfb944\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/c0/d4/29d797817e268124a32b6cf8beb8b8fe87b86f099d5a049e61\n",
            "Successfully built word2vec\n",
            "Installing collected packages: word2vec\n",
            "Successfully installed word2vec-0.11.1\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n"
          ]
        }
      ],
      "source": [
        "# install deep graph labrary: https://www.dgl.ai/pages/start.html\n",
        "import torch\n",
        "try:\n",
        "  import dgl\n",
        "except ModuleNotFoundError:\n",
        "  # Installing dgl package with specific CUDA\n",
        "  CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "  !pip install dgl-{CUDA} -f https://data.dgl.ai/wheels/repo.html\n",
        "\n",
        "# install word2vec\n",
        "try:\n",
        "  import word2vec  # type: ignore\n",
        "except ModuleNotFoundError:\n",
        "  !pip install word2vec # type: ignore\n",
        "\n",
        "# install torch_scatter  refer link: https://colab.research.google.com/drive/1PfoLSjr4TID_ogKEIV4Jusl2NromfgL3#scrollTo=mMoeNLSF38C-\n",
        "try:\n",
        "  import torch_scatter\n",
        "except ModuleNotFoundError:\n",
        "  TORCH = torch.__version__.split('+')[0]\n",
        "  CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "  !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# parsing"
      ],
      "metadata": {
        "id": "QMf2qWavH7Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile parsing.py\n",
        "# build graph\n",
        "import torch\n",
        "import dgl\n",
        "\n",
        "# dada_helper\n",
        "import os\n",
        "import csv\n",
        "import re\n",
        "\n",
        "# model\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import word2vec\n",
        "from torch_scatter import scatter_sum, scatter_max, scatter_mean  # 图计算需要\n",
        "import math\n",
        "\n",
        "# train\n",
        "import tqdm\n",
        "import sys, random\n",
        "import argparse\n",
        "from time import time\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------buildGraph.py------------------------------------------------------------------------\n",
        "class GraphBuilder(object):\n",
        "    def __init__(self, words, hiddenSizeNode):\n",
        "        self.graph = dgl.DGLGraph()\n",
        "        self.word2id = dict(zip(words, range(len(words))))\n",
        "        self.graph.add_nodes(len(words))\n",
        "\n",
        "        # add hidden para for nodes.\n",
        "        self.graph.ndata['h'] = torch.nn.Parameter(\n",
        "            torch.Tensor(len(words), hiddenSizeNode)\n",
        "        )\n",
        "\n",
        "        # all node are supposed to connected.\n",
        "        # warning: self-connected enabled.\n",
        "        for i in range(len(words)):\n",
        "            self.graph.add_edges(i, range(0, len(words)))\n",
        "\n",
        "        # add hidden para for edges. Only edge weight (size = 1 )\n",
        "        self.graph.edata['h'] = torch.nn.Parameter(\n",
        "            torch.Tensor(self.graph.number_of_edges(), 1)\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------data_helper.py------------------------------------------------------------------------\n",
        "class DataHelper(object):\n",
        "    def __init__(self, dataset, mode='train', vocab=None):\n",
        "        allowed_data = ['r8', '20ng', 'r52', 'mr', 'oh', 'agnews']\n",
        "        tmp_data = dataset.split('/')\n",
        "        dataset = tmp_data[len(tmp_data)-1]\n",
        "\n",
        "        if dataset not in allowed_data:\n",
        "            raise ValueError('currently allowed data: %s' % ','.join(allowed_data))\n",
        "        else:\n",
        "            self.dataset = dataset\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        self.base = os.path.join('/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/data', self.dataset)  # 'data/r8'\n",
        "\n",
        "        self.current_set = os.path.join(self.base, '%s-%s-stemmed.txt' % (self.dataset, self.mode))\n",
        "\n",
        "        with open(os.path.join(self.base, 'label.txt')) as f:\n",
        "            labels = f.read()\n",
        "        self.labels_str = labels.split('\\n')\n",
        "\n",
        "        content, label = self.get_content()  # 获得数据集中的content和label\n",
        "        self.label = self.label_to_onehot(label)  # 转1-hot编码\n",
        "        \n",
        "        if vocab is None:\n",
        "            self.vocab = []\n",
        "            try:\n",
        "                self.get_vocab()\n",
        "            except FileNotFoundError:\n",
        "                self.build_vocab(content, min_count=30)\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "\n",
        "        self.d = dict(zip(self.vocab, range(len(self.vocab))))  # 将词转字典\n",
        "#-----------------------------self.content-----------------------------\n",
        "        temp = []\n",
        "        for doc in content:  # 取一个样本\n",
        "            doc_tmp = []\n",
        "            sentence = doc.split('[sep]')  # 根据' * '标记划分为句子\n",
        "            for sen in sentence:  # 取一个句子\n",
        "                sen = sen.strip()  # 删除可能的空格\n",
        "                words = list(map(lambda x: self.word2id(x), sen.split()))\n",
        "                if len(words)>0:  # 删除短的子句\n",
        "                  doc_tmp.append(words)\n",
        "            temp.append(doc_tmp)\n",
        "        \n",
        "        self.content = temp  # 此时content三维\n",
        "\n",
        "        #self.content = [list(map(lambda x: self.word2id(x), doc.split(' '))) for doc in content]  # id型content\n",
        "\n",
        "\n",
        "    def label_to_onehot(self, label_str):\n",
        "        return [self.labels_str.index(l) for l in label_str]\n",
        "\n",
        "\n",
        "    def get_content(self):\n",
        "        with open(self.current_set) as f:  # open dataset\n",
        "            all = f.read()\n",
        "            all = all.split('\\n')\n",
        "            content = [line.split('\\t') for line in all]  # 2 elements:(label, str_text)\n",
        "        if self.dataset == '20ng' or 'r52' or 'mr':\n",
        "            cleaned = []\n",
        "            for i, pair in enumerate(content):\n",
        "                if len(pair) < 2:  # remove some exceptional sentences\n",
        "                    #print(f'i, pair: {i, pair}')\n",
        "                    pass\n",
        "                else:\n",
        "                    cleaned.append(pair)\n",
        "        else:\n",
        "            cleaned = content\n",
        "\n",
        "        label, content = zip(*cleaned)  # '*' means unpack a list\n",
        "\n",
        "        return content, label\n",
        "\n",
        "\n",
        "    def word2id(self, word):\n",
        "        try:\n",
        "            result = self.d[word]\n",
        "        except KeyError:\n",
        "            result = self.d['UNK']\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def get_vocab(self):\n",
        "        with open(os.path.join(self.base, 'vocab-30.txt')) as f:\n",
        "            vocab = f.read()\n",
        "            self.vocab = vocab.split('\\n')\n",
        "            #print(f'self.vocab: {self.vocab}')\n",
        "\n",
        "\n",
        "    def build_vocab(self, content, min_count=2):\n",
        "        vocab = []\n",
        "\n",
        "        for c in content:\n",
        "            words = c.split(' ')\n",
        "            for word in words:\n",
        "                if word not in vocab and word !='[sep]' and word!='':\n",
        "                    vocab.append(word)\n",
        "\n",
        "        freq = dict(zip(vocab, [0 for i in range(len(vocab))]))\n",
        "\n",
        "        for c in content:\n",
        "            words = c.split()\n",
        "            for word in words:\n",
        "              if  word !='[sep]' and word!='':\n",
        "                freq[word] += 1\n",
        "\n",
        "        results = []\n",
        "        for word in freq.keys():\n",
        "            if freq[word] < min_count:\n",
        "                continue\n",
        "            else:\n",
        "                results.append(word)\n",
        "\n",
        "        results.insert(0, 'UNK')\n",
        "        with open(os.path.join(self.base, 'vocab-30.txt'), 'w') as f:\n",
        "            f.write('\\n'.join(results))\n",
        "\n",
        "        self.vocab = results\n",
        "\n",
        "\n",
        "    def batch_iter(self, batch_size, num_epoch):\n",
        "        for i in range(num_epoch):\n",
        "            num_per_epoch = int(len(self.content) / batch_size)\n",
        "            for batch_id in range(num_per_epoch):\n",
        "                start = batch_id * batch_size\n",
        "                end = min((batch_id + 1) * batch_size, len(self.content))\n",
        "\n",
        "                content = self.content[start:end]\n",
        "                label = self.label[start:end]\n",
        "\n",
        "                device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "                y = torch.tensor(label).to(device)  # gup内存上的label\n",
        "\n",
        "                yield content, y, i \n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------model.py------------------------------------------------------------------------\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 class_num,\n",
        "                 hidden_size_node,\n",
        "                 vocab,\n",
        "                 n_gram,\n",
        "                 drop_out,\n",
        "                 edges_num,  # 边的数量\n",
        "                 edges_matrix,  # 边的连接关系\n",
        "                 max_length=50,  # 350是文档限制，100对应句子限制\n",
        "                 trainable_edges=True,\n",
        "                 pmi=None,  # 边的权重\n",
        "                 ):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.vocab = vocab\n",
        "        # print(len(vocab))\n",
        "        # self.seq_edge_w = torch.nn.Embedding(edges_num, 1)  # 模型参数：边的权重\n",
        "        print(f'edges_num: {edges_num}')\n",
        "        print(f'pmi.shape: {pmi.shape}')\n",
        "\n",
        "        self.node_hidden = torch.nn.Embedding(len(vocab), hidden_size_node)  # 模型参数： 隐藏层节点\n",
        "        \n",
        "        #self.seq_edge_w = torch.nn.Embedding.from_pretrained(pmi, freeze=True)  # 加载预训练的边的权重\n",
        "            \n",
        "        self.edges_num = edges_num\n",
        "        if trainable_edges:\n",
        "            self.seq_edge_w = torch.nn.Embedding.from_pretrained(torch.ones(edges_num, 1), freeze=False)\n",
        "        else:\n",
        "            self.seq_edge_w = torch.nn.Embedding.from_pretrained(pmi, freeze=True)\n",
        "\n",
        "        self.hidden_size_node = hidden_size_node\n",
        "\n",
        "        self.node_hidden.weight.data.copy_(torch.tensor(self.load_word2vec('/content/drive/MyDrive/Colab_Notebooks/DATA/glove.6B/glove.6B.300d.w2vformat.txt')))\n",
        "        self.node_hidden.weight.requires_grad = True\n",
        "\n",
        "        self.len_vocab = len(vocab)\n",
        "\n",
        "        self.ngram = n_gram\n",
        "\n",
        "        self.d = dict(zip(self.vocab, range(len(self.vocab))))\n",
        "\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.eta = torch.nn.Parameter(torch.tensor([0.2]), requires_grad=True)  # 设置可训练的eta参数并初始化eta\n",
        "\n",
        "        self.edges_matrix = edges_matrix\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(p=drop_out)\n",
        "\n",
        "        self.activation = torch.nn.ReLU()\n",
        "\n",
        "        self.Linear = torch.nn.Linear(hidden_size_node, class_num, bias=True)  # 一个MLP层\n",
        "\n",
        "#----------------------------------------------------start-------------------------------------------------------------\n",
        "        self.max_L = 30   # 每个样本最多的子图数量\n",
        "        self.query = torch.nn.Parameter(torch.rand(32, hidden_size_node))  # (batch_size, 300)\n",
        "\n",
        "    def size_splits(self, tensor, split_sizes, dim=0):   # refers to:  https://github.com/pytorch/pytorch/issues/3223\n",
        "        \"\"\"Splits the tensor according to chunks of split_sizes.\n",
        "        # source link:  https://github.com/pytorch/pytorch/issues/3223\n",
        "        \n",
        "        Arguments:\n",
        "            tensor (Tensor): tensor to split.\n",
        "            split_sizes (list(int)): sizes of chunks\n",
        "            dim (int): dimension along which to split the tensor.\n",
        "        \"\"\"\n",
        "        if dim < 0:\n",
        "            dim += tensor.dim()\n",
        "        \n",
        "        dim_size = tensor.size(dim)\n",
        "        if dim_size != torch.sum(torch.Tensor(split_sizes)):\n",
        "            raise KeyError(\"Sum of split sizes exceeds tensor dim\")\n",
        "        \n",
        "        splits = torch.cumsum(torch.Tensor([0] + split_sizes), dim=0)[:-1]\n",
        "\n",
        "        return tuple(tensor.narrow(int(dim), int(start), int(length)) \n",
        "            for start, length in zip(splits, split_sizes))\n",
        "        \n",
        "    \n",
        "    def scaled_dot_product(self, q, k, v):  # refers to tutorial 6 written by Lippe (source link is in paper)\n",
        "      d_k = q.size()[-1]\n",
        "      attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "      attn_logits = attn_logits / math.sqrt(d_k)\n",
        "      attention = F.softmax(attn_logits, dim=-1)\n",
        "      values = torch.matmul(attention, v)\n",
        "\n",
        "      return values\n",
        "        \n",
        "    def Sen_Attn(self, v_s, graph_id):\n",
        "      # v_s: feature vector of sentence level node\n",
        "      # id_split: [[0,4], [5,11], ..., [266, 269]]  mark the node belongs to which document\n",
        "      #alpha = torch.ones(len(v_s), 1)  # attention value\n",
        "      q = self.query[graph_id].unsqueeze(0)  # (1, hidden_size_node)  二维张量\n",
        "      k = torch.tanh(v_s)\n",
        "      h = self.scaled_dot_product(q, k ,v_s)\n",
        "\n",
        "      return h  # return the vector multipled by \\alpha_{i}\n",
        "\n",
        "#----------------------------------------------------end-------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def load_word2vec(self, word2vec_file):\n",
        "        model = word2vec.load(word2vec_file)\n",
        "\n",
        "        embedding_matrix = []\n",
        "\n",
        "        for word in self.vocab:\n",
        "            try:\n",
        "                embedding_matrix.append(model[word])\n",
        "            except KeyError:\n",
        "                # print(f'Line 269. The word not in vocab:{word}')\n",
        "                #unk_word=np.zeros(300)\n",
        "                #print(f\"line 271, model['the].shape: {len(model['the'])}\")  # [,300]\n",
        "                embedding_matrix.append(np.zeros(len(model['the'])))  # 其他词用0向量代替\n",
        "\n",
        "        embedding_matrix = np.array(embedding_matrix)\n",
        "\n",
        "        return embedding_matrix\n",
        "\n",
        "\n",
        "    def add_seq_edges(self, doc_ids: list, old_to_new: dict):\n",
        "        edges = []\n",
        "        old_edge_id = []\n",
        "        for index, src_word_old in enumerate(doc_ids):\n",
        "            src = old_to_new[src_word_old]\n",
        "            for i in range(max(0, index - self.ngram), min(index + self.ngram + 1, len(doc_ids))):\n",
        "                dst_word_old = doc_ids[i]\n",
        "                dst = old_to_new[dst_word_old]\n",
        "\n",
        "                # - first connect the new sub_graph\n",
        "                edges.append([src, dst])\n",
        "                # - then get the hidden from parent_graph\n",
        "                old_edge_id.append(self.edges_matrix[src_word_old, dst_word_old])\n",
        "\n",
        "            # self circle\n",
        "            edges.append([src, src])  # 二维列表存储边的关系\n",
        "            old_edge_id.append(self.edges_matrix[src_word_old, src_word_old])\n",
        "\n",
        "        return edges, old_edge_id\n",
        "\n",
        "\n",
        "    def seq_to_graph(self, doc_ids: list) -> dgl.DGLGraph():  # function annotation\n",
        "        if len(doc_ids) > self.max_length:\n",
        "            doc_ids = doc_ids[:self.max_length]  # 截取max_length长度的句子\n",
        "\n",
        "        local_vocab = set(doc_ids)\n",
        "\n",
        "        old_to_new = dict(zip(local_vocab, range(len(local_vocab))))\n",
        "\n",
        "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        local_vocab = torch.tensor(list(local_vocab)).to(device)\n",
        "\n",
        "        sub_graph = dgl.DGLGraph()  # 定义子图, sub_graph = dgl.DGLGraph()\n",
        "        sub_graph = sub_graph.to(device)  # 将cpu上的图也放到device上\n",
        "\n",
        "        sub_graph.add_nodes(len(local_vocab))  # 为图添加节点\n",
        "        local_node_hidden = self.node_hidden(local_vocab)  # 词ID转词嵌入向量\n",
        "\n",
        "        sub_graph.ndata['h'] = local_node_hidden  # Return a node data view for setting/getting node features\n",
        "\n",
        "        seq_edges, seq_old_edges_id = self.add_seq_edges(doc_ids, old_to_new)\n",
        "\n",
        "        edges, old_edge_id = [], []\n",
        "        # edges = []\n",
        "\n",
        "        edges.extend(seq_edges)\n",
        "\n",
        "        old_edge_id.extend(seq_old_edges_id)\n",
        "\n",
        "        old_edge_id = torch.LongTensor(old_edge_id).to(device)\n",
        "\n",
        "        srcs, dsts = zip(*edges)  # '*' denotes unpack\n",
        "        sub_graph.add_edges(srcs, dsts)  # 图添加边\n",
        "        try:\n",
        "            seq_edges_w = self.seq_edge_w(old_edge_id)\n",
        "        except RuntimeError:\n",
        "            print(f'old_edge_id: {old_edge_id}')\n",
        "        sub_graph.edata['w'] = seq_edges_w  # 图的边添加权重\n",
        "\n",
        "        return sub_graph  #  返回一个文本对应的一张图，节点特征和边的权重已被初始化\n",
        "\n",
        "    \n",
        "    def forward(self, doc_ids):  # 一次传一个batch_size数量的样本\n",
        "        \n",
        "        graphs = [self.seq_to_graph(doc_ids[i][j])  for i in range(len(doc_ids)) for j in range(len(doc_ids[i])) if j<self.max_L]  # 一个句子一个图, 至多取前20个子图\n",
        "        graphs_id = [i for i in range(len(doc_ids)) for j in range(len(doc_ids[i])) if j < self.max_L]  # 句子所属文档的标记  [0,0,0,1,1,1,1, ...]\n",
        "\n",
        "#--------------------------------------------------------------------Start-------------------------------------------------------------------\n",
        "        id = torch.tensor([i for i in range(len(graphs_id))])\n",
        "        split_size = [graphs_id.count(i) for i in range(len(doc_ids))]  # 将id按样本切分\n",
        "        id_mapping = self.size_splits(tensor=id, split_sizes=split_size, dim=0)  # 按样本切分 [[0,1,2],[3,4,5,6], ...]\n",
        "\n",
        "        id_split = torch.tensor([[tmp[0], tmp[len(tmp)-1]] for tmp in id_mapping])   # [[0,2], [3,6], ..., [266,269]]\n",
        "        doc_id = torch.tensor([i for i in range(len(doc_ids))])  # (0, 1, 2, 3, ...., 31)\n",
        "\n",
        "       \n",
        "#--------------------------------------------------------------------End-------------------------------------------------------------------\n",
        "\n",
        "        batch_graph = dgl.batch(graphs)\n",
        "        batch_graph.update_all(\n",
        "            message_func=dgl.function.src_mul_edge('h', 'w', 'weighted_message'),  # src_mul_edge(): u_mul_v()\n",
        "\n",
        "            reduce_func=dgl.function.max('weighted_message', 'M')  # M_n\n",
        "\n",
        "        )\n",
        "\n",
        "        # 更新节点r_n^'\n",
        "        eta = self.eta\n",
        "        batch_graph.ndata['h'] = batch_graph.ndata['M']*(1-eta) + batch_graph.ndata['h']*eta\n",
        "        out1 = dgl.sum_nodes(batch_graph, feat='h')  # (269, 300)\n",
        "#--------------------------------------------------------------------start-------------------------------------------------------------------\n",
        "        id_all = zip(id_split, doc_id)\n",
        "        out2 = [self.Sen_Attn(out1[id_spl[0]: id_spl[1]+1], d_id)  for id_spl,  d_id in id_all]  # id_split: [[0,4], [5,11], ..., [266, 269]] 划分的下标，用于求Attention: \\alpha_{ij}\n",
        "        out2 = torch.cat(out2, dim=0)  # (32, 300)\n",
        "\n",
        "#----------------------------------------------------------------end----------------------------------------------------------------------\n",
        "        \n",
        "        drop1 = self.dropout(out2)\n",
        "        act1 = self.activation(drop1)\n",
        "\n",
        "        l = self.Linear(act1)\n",
        "\n",
        "        return l\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------pmi.py------------------------------------------------------------------------\n",
        "def cal_PMI(dataset: str, window_size=20):  # point wise mutual information\n",
        "    helper = DataHelper(dataset=dataset, mode=\"train\")\n",
        "    content, _ = helper.get_content()  # function returns (content, label)\n",
        "    pair_count_matrix = np.zeros((len(helper.vocab), len(helper.vocab)), dtype=int)  # p(i,j)\n",
        "    word_count =np.zeros(len(helper.vocab), dtype=int)  # 公式中的#W(i)\n",
        "    print(f'len of word_count: {len(helper.vocab)}')\n",
        "    \n",
        "    for sentence in content:  # one  sentence per document\n",
        "        sentence = sentence.split()  # get the words in a sentence\n",
        "        for i, word in enumerate(sentence):\n",
        "            try:\n",
        "                word_count[helper.d[word]] += 1\n",
        "            except KeyError:\n",
        "                continue\n",
        "            start_index = max(0, i - window_size)\n",
        "            end_index = min(len(sentence), i + window_size)\n",
        "            for j in range(start_index, end_index):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                else:\n",
        "                    target_word = sentence[j]\n",
        "                    try:\n",
        "                        pair_count_matrix[helper.d[word], helper.d[target_word]] += 1  # p(i,j)\n",
        "                    except KeyError:\n",
        "                        continue\n",
        "        \n",
        "    total_count = np.sum(word_count)\n",
        "    # print(f'line 408:total_count: {total_count}')\n",
        "    word_count = word_count / total_count\n",
        "    # print(f'word_count: {word_count}')\n",
        "    pair_count_matrix = pair_count_matrix / total_count\n",
        "    \n",
        "    pmi_matrix = np.zeros((len(helper.vocab), len(helper.vocab)), dtype=float)\n",
        "    for i in range(len(helper.vocab)):\n",
        "        for j in range(len(helper.vocab)):\n",
        "            pmi_matrix[i, j] = np.log(\n",
        "                pair_count_matrix[i, j] / (word_count[i] * word_count[j])  # 正则化后值很小，被取整为0，发生除0错误, 得到NaN值\n",
        "            )\n",
        "    \n",
        "    pmi_matrix = np.nan_to_num(pmi_matrix)  # replace NaN with zero and infinity with large finite number\n",
        "    \n",
        "    pmi_matrix = np.maximum(pmi_matrix, 0.0)  # 大于0的元素不变，小于0的元素取0\n",
        "\n",
        "    edges_weights = [0.0]\n",
        "    count = 1  # number of edges\n",
        "    edges_mappings = np.zeros((len(helper.vocab), len(helper.vocab)), dtype=int)\n",
        "    for i in range(len(helper.vocab)):\n",
        "        for j in range(len(helper.vocab)):\n",
        "            if pmi_matrix[i, j] != 0:  # the value of PMI, >0 positive correlation\n",
        "                edges_weights.append(pmi_matrix[i, j])\n",
        "                edges_mappings[i, j] = count\n",
        "                count += 1\n",
        "\n",
        "    edges_weights = np.array(edges_weights)\n",
        "\n",
        "    edges_weights = edges_weights.reshape(-1, 1)\n",
        "    # print(edges_weights.shape)\n",
        "    edges_weights = torch.Tensor(edges_weights)\n",
        "    \n",
        "    return edges_weights, edges_mappings, count\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------train.py------------------------------------------------------------------------\n",
        "NUM_ITER_EVAL = 100\n",
        "EARLY_STOP_EPOCH = 10\n",
        "\n",
        "\n",
        "def dev(model, dataset):\n",
        "    data_helper = DataHelper(dataset, mode='dev')\n",
        "\n",
        "    total_pred = 0\n",
        "    correct = 0\n",
        "    iter = 0\n",
        "    for content, label, _ in data_helper.batch_iter(batch_size=32, num_epoch=1):\n",
        "        iter += 1\n",
        "        model.eval()\n",
        "\n",
        "        logits = model(content)\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "        correct_pred = torch.sum(pred == label)\n",
        "\n",
        "        correct += correct_pred\n",
        "        total_pred += len(content)\n",
        "\n",
        "    total_pred = float(total_pred)\n",
        "    correct = correct.float()\n",
        "    # print(torch.div(correct, total_pred))\n",
        "    return torch.div(correct, total_pred)\n",
        "\n",
        "\n",
        "def test(model_name, dataset):\n",
        "    model = torch.load(os.path.join('/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/model', model_name + '.pkl'))\n",
        "\n",
        "    data_helper = DataHelper(dataset, mode='test')\n",
        "\n",
        "    total_pred = 0\n",
        "    correct = 0\n",
        "    iter = 0\n",
        "    for content, label, _ in data_helper.batch_iter(batch_size=32, num_epoch=1):\n",
        "        iter += 1\n",
        "        model.eval()\n",
        "\n",
        "        logits = model(content)\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "        correct_pred = torch.sum(pred == label)\n",
        "\n",
        "        correct += correct_pred\n",
        "        total_pred += len(content)\n",
        "\n",
        "    total_pred = float(total_pred)\n",
        "    #correct = correct.float()\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    x = torch.div(correct, total_pred)\n",
        "    x = x.to(device)\n",
        "    return x\n",
        "\n",
        "\n",
        "def train(ngram, model_name, drop_out, dataset, edges=True, total_epoch=1):\n",
        "    print('load data helper.')\n",
        "    data_helper = DataHelper(dataset, mode='train')\n",
        "\n",
        "    tmp_data = dataset.split('/')\n",
        "    dataset_name = tmp_data[len(tmp_data)-1]\n",
        "    if os.path.exists(os.path.join('/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/model', model_name+'.pkl')) and model_name != 'temp_model':\n",
        "        print('load model from file.')\n",
        "        model = torch.load(os.path.join('/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/model', model_name+'.pkl'))\n",
        "    else:\n",
        "        print('new model.')\n",
        "        if model_name == 'temp_model':\n",
        "            model_name = f'temp_model_{dataset_name}'\n",
        "        # edges_num, edges_matrix = edges_mapping(len(data_helper.vocab), data_helper.content, ngram)\n",
        "        edges_weights, edges_mappings, count = cal_PMI(dataset=dataset)\n",
        "        \n",
        "        model = Model(class_num=len(data_helper.labels_str), hidden_size_node=300,\n",
        "                      vocab=data_helper.vocab, n_gram=ngram, drop_out=drop_out, edges_matrix=edges_mappings, edges_num=count,\n",
        "                      trainable_edges=edges, pmi=edges_weights)\n",
        "\n",
        "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "    loss_func = torch.nn.CrossEntropyLoss()\n",
        "    optim = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
        "\n",
        "    iter = 0\n",
        "\n",
        "    best_acc = 0.0\n",
        "    last_best_epoch = 0\n",
        "    # start_time = time()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "    for content, label, epoch in data_helper.batch_iter(batch_size=32, num_epoch=total_epoch):\n",
        "        improved = ''\n",
        "        model.train()\n",
        "\n",
        "        logits = model(content)  # doc_ids传给forward()方法\n",
        "        loss = loss_func(logits, label)\n",
        "\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "        correct = torch.sum(pred == label)\n",
        "\n",
        "        total_correct += correct\n",
        "        total += len(label)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        iter += 1\n",
        "        if iter % NUM_ITER_EVAL == 0:\n",
        "\n",
        "            val_acc = dev(model, dataset=dataset)\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                last_best_epoch = epoch\n",
        "                improved = '*'\n",
        "\n",
        "                torch.save(model, f'/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/model/{model_name}.pkl')\n",
        "\n",
        "            if epoch - last_best_epoch >= EARLY_STOP_EPOCH:\n",
        "                print('Early stopping...')\n",
        "                return model_name\n",
        "            print(f'Epoch:{epoch}, iter:{iter}, train loss:{total_loss/ NUM_ITER_EVAL :.4f}, train acc: {float(total_correct) / float(total) :.4f}, val acc: {val_acc:.4f},{improved}')\n",
        "\n",
        "            total_loss = 0.0\n",
        "            total_correct = 0\n",
        "            total = 0\n",
        "\n",
        "    return model_name\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--ngram', required=False, type=int, default=4, help='ngram number')\n",
        "parser.add_argument('--name', required=False, type=str, default='temp_model', help='project name')\n",
        "parser.add_argument('--dropout', required=False, type=float, default=0.5, help='dropout rate')\n",
        "parser.add_argument('--dataset', required=True, type=str, help='dataset')\n",
        "parser.add_argument('--edges', required=False, type=int, default=1, help='trainable edges')\n",
        "parser.add_argument('--rand', required=False, type=int, default=42, help='rand_seed')\n",
        "parser.add_argument('--epoch', required=False, type=int, default=1, help='training epoch')\n",
        "parser.add_argument('--lr', default=1e-3, type=float, required=False, help='Initial learning rate')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "device = torch.device(f'cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "  print(f'device: {device}')\n",
        "  print(f'name:{torch.cuda.get_device_name(0)}')\n",
        "  print(f'memory:{torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
        "print(f'*'*50)\n",
        "\n",
        "print('ngram: %d' % args.ngram)\n",
        "print('project_name: %s' % args.name)\n",
        "print('dataset: %s' % args.dataset)\n",
        "print('trainable_edges: %s' % args.edges)\n",
        "print(f'*'*50)\n",
        "# #\n",
        "SEED = args.rand\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "if args.edges == 1:\n",
        "    edges = True\n",
        "    print('trainable edges is True')\n",
        "else:\n",
        "    edges = False\n",
        "\n",
        "model = train(args.ngram, args.name, args.dropout, dataset=args.dataset, edges=edges, total_epoch=args.epoch)\n",
        "print(f'test acc: {test(model, args.dataset).cpu().numpy():.4f}')"
      ],
      "metadata": {
        "id": "CGyeJFGUH82e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab420c1-d1ca-45b0-a874-4d223e43b924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing parsing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run"
      ],
      "metadata": {
        "id": "JKcGnw8RH-a7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python parsing.py --dataset='/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/data/r52' --name='temp_model' --edges=1 --ngram=3 --dropout=0.5 --epoch=100"
      ],
      "metadata": {
        "id": "euBkeLfWIBwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9299d38-54ba-4aa8-b5dd-0b31187ab7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
            "  return warnings.warn(message, category=category, stacklevel=1)\n",
            "**************************************************\n",
            "ngram: 3\n",
            "project_name: temp_model\n",
            "dataset: /content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/data/r52\n",
            "trainable_edges: 1\n",
            "**************************************************\n",
            "trainable edges is True\n",
            "load data helper.\n",
            "new model.\n",
            "len of word_count: 1552\n",
            "parsing.py:457: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  pair_count_matrix[i, j] / (word_count[i] * word_count[j])  # 正则化后值很小，被取整为0，发生除0错误, 得到NaN值\n",
            "parsing.py:457: RuntimeWarning: divide by zero encountered in log\n",
            "  pair_count_matrix[i, j] / (word_count[i] * word_count[j])  # 正则化后值很小，被取整为0，发生除0错误, 得到NaN值\n",
            "edges_num: 1025075\n",
            "pmi.shape: torch.Size([1025075, 1])\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "Epoch:0, iter:100, train loss:1.8600, train acc: 0.6141, val acc: 0.6391,*\n",
            "Epoch:1, iter:200, train loss:1.0356, train acc: 0.7816, val acc: 0.8078,*\n",
            "Epoch:1, iter:300, train loss:0.6861, train acc: 0.8716, val acc: 0.8641,*\n",
            "Epoch:2, iter:400, train loss:0.4867, train acc: 0.9066, val acc: 0.8938,*\n",
            "Epoch:2, iter:500, train loss:0.4061, train acc: 0.9266, val acc: 0.8953,*\n",
            "Epoch:3, iter:600, train loss:0.3165, train acc: 0.9384, val acc: 0.9031,*\n",
            "Epoch:3, iter:700, train loss:0.2590, train acc: 0.9556, val acc: 0.9016,\n",
            "Epoch:4, iter:800, train loss:0.2008, train acc: 0.9603, val acc: 0.9156,*\n",
            "Epoch:4, iter:900, train loss:0.1967, train acc: 0.9659, val acc: 0.9141,\n",
            "Epoch:5, iter:1000, train loss:0.1560, train acc: 0.9672, val acc: 0.9266,*\n",
            "Epoch:6, iter:1100, train loss:0.1576, train acc: 0.9706, val acc: 0.9172,\n",
            "Epoch:6, iter:1200, train loss:0.1101, train acc: 0.9800, val acc: 0.9250,\n",
            "Epoch:7, iter:1300, train loss:0.1165, train acc: 0.9778, val acc: 0.9250,\n",
            "Epoch:7, iter:1400, train loss:0.1029, train acc: 0.9781, val acc: 0.9281,*\n",
            "Epoch:8, iter:1500, train loss:0.0807, train acc: 0.9831, val acc: 0.9266,\n",
            "Epoch:8, iter:1600, train loss:0.0907, train acc: 0.9812, val acc: 0.9281,\n",
            "Epoch:9, iter:1700, train loss:0.0758, train acc: 0.9841, val acc: 0.9250,\n",
            "Epoch:9, iter:1800, train loss:0.0711, train acc: 0.9888, val acc: 0.9234,\n",
            "Epoch:10, iter:1900, train loss:0.0635, train acc: 0.9856, val acc: 0.9297,*\n",
            "Epoch:10, iter:2000, train loss:0.0645, train acc: 0.9881, val acc: 0.9234,\n",
            "Epoch:11, iter:2100, train loss:0.0541, train acc: 0.9881, val acc: 0.9328,*\n",
            "Epoch:12, iter:2200, train loss:0.0608, train acc: 0.9884, val acc: 0.9156,\n",
            "Epoch:12, iter:2300, train loss:0.0474, train acc: 0.9888, val acc: 0.9266,\n",
            "Epoch:13, iter:2400, train loss:0.0519, train acc: 0.9900, val acc: 0.9219,\n",
            "Epoch:13, iter:2500, train loss:0.0441, train acc: 0.9925, val acc: 0.9281,\n",
            "Epoch:14, iter:2600, train loss:0.0375, train acc: 0.9925, val acc: 0.9250,\n",
            "Epoch:14, iter:2700, train loss:0.0480, train acc: 0.9922, val acc: 0.9250,\n",
            "Epoch:15, iter:2800, train loss:0.0436, train acc: 0.9909, val acc: 0.9219,\n",
            "Epoch:15, iter:2900, train loss:0.0445, train acc: 0.9909, val acc: 0.9250,\n",
            "Epoch:16, iter:3000, train loss:0.0390, train acc: 0.9906, val acc: 0.9234,\n",
            "Epoch:16, iter:3100, train loss:0.0417, train acc: 0.9925, val acc: 0.9266,\n",
            "Epoch:17, iter:3200, train loss:0.0351, train acc: 0.9938, val acc: 0.9219,\n",
            "Epoch:18, iter:3300, train loss:0.0390, train acc: 0.9912, val acc: 0.9219,\n",
            "Epoch:18, iter:3400, train loss:0.0291, train acc: 0.9944, val acc: 0.9172,\n",
            "Epoch:19, iter:3500, train loss:0.0352, train acc: 0.9925, val acc: 0.9094,\n",
            "Epoch:19, iter:3600, train loss:0.0305, train acc: 0.9950, val acc: 0.9156,\n",
            "Epoch:20, iter:3700, train loss:0.0266, train acc: 0.9947, val acc: 0.9156,\n",
            "Epoch:20, iter:3800, train loss:0.0379, train acc: 0.9928, val acc: 0.9109,\n",
            "Early stopping...\n",
            "test acc: 0.9320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# result"
      ],
      "metadata": {
        "id": "YKbBS65yIFnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " oh: (min_count5, words8140)\n",
        "\n",
        " 1. edge=0, ngram=1,dp=0.5: acc0.6778, cpu=3h48mins\n",
        "\n",
        " 2. edge=0, ngram=4,dp=0.5: acc0.662, gpu34mins\n",
        " \n",
        " 3. edge=1, ngram=1,dp=0.5: acc"
      ],
      "metadata": {
        "id": "tTLfy3hOIKOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20ng: (min_count=15,words12095)  min_count=5, words25782; min_count=10, words16016; \n",
        "\n",
        "1. edge=0,ngram=2,dp=0.5: acc0.8234,cpu6h\n",
        "\n",
        "2. edge=0,ngram=3,dp=0.5: acc"
      ],
      "metadata": {
        "id": "ZP-MG4rtoR3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "mr: (min_count=5, words2934)\n",
        "\n",
        "1. edge=0,ngram=2,dp=0.5:  acc0.7033, cpu34mins;\n",
        "\n",
        "2. edge=0,ngram=1,dp=0.5: acc0.6948, cpu14mins\n",
        "\n",
        "3. edge=0,ngram=2,dp=0.7: acc0.7097, cpu14mins\n",
        "\n",
        "4. edge=1,ngram=2,dp=0.7: acc0.7365, cpu18mins\n",
        "\n",
        "5. edge=1,ngram=3,dp=0.7: acc0.7342, cpu18mins"
      ],
      "metadata": {
        "id": "-Fy2ScAkI0kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "r8: (min_count=5, words4380)\n",
        "\n",
        "1. edge=1,ngram=2,dp=0.7: acc0.9733, cpu17mins\n",
        "\n",
        "2. edge=1,ngram=3,dp=0.7: acc0.9747, cpu15mins\n",
        "\n",
        "3. edge=1,ngram=3,dp=0.5: acc0.9756, cpu18mins\n",
        "\n",
        "4. edge=1,ngram=4,dp=0.5: acc0.9756, cpu19mins\n",
        "\n",
        "5. edge=1,ngram=3,dp=0.4: acc0.9743, cpu18mins"
      ],
      "metadata": {
        "id": "MA5rkeDH37mL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "r52: (min_coun=5, words5019) min_coun=30, words1552\n",
        "\n",
        "1. edge=1,ngram=2,dp=0.7: acc0.9344, cpu33mins\n",
        "\n",
        "2.  edge=1,ngram=3,dp=0.7: acc0.9355, cpu27mins\n",
        "\n",
        "3. edge=1,ngram=3,dp=0.5: acc0.9320, cpu20mins, (min_coun=30, words1552)"
      ],
      "metadata": {
        "id": "2TeEECiWHx2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "agnews: (min_count=30, words9690) min_count=15, words14404； min_count=20, words12223\n",
        "\n",
        "1. edge=1, ngram=2, dp=0.7: acc"
      ],
      "metadata": {
        "id": "92mFyCwWwmM0"
      }
    }
  ]
}