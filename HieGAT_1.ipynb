{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hshuai97/Colab20210803/blob/main/HieGAT_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYQGYFllWh65"
      },
      "source": [
        "1. 改写于官方DGL库的GAT实现代码，[GAT代码链接](https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html)；[训练代码](https://github.com/dmlc/dgl/blob/master/examples/pytorch/gat/train.py)\n",
        "\n",
        "2. 使用纯净的GAT，在sen-level上使用GAT,\n",
        "\n",
        "3. 其中word-level将输出结果不分顺序直接相加表示所在样本的向量表示；sen-level则将word-level的输出再进行一次GAT，将输出结果作为所在样本的向量表示；doc-level直接在原样本上进行GAT，并将输出结果作为样本的向量表示\n",
        "\n",
        "4. 使用geometric做图分类的colab教程[link](https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing), 根据节点分类教程[link2](https://colab.research.google.com/drive/14OvFnAXggxB8vM4e8vSURUp1TaKnovzX?usp=sharing#scrollTo=p3TAi69zI1bO)可知，GCNConv可以在节点分类和图分类上共用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar8ervFTXa_f"
      },
      "source": [
        "# Install labraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "P1lGFW2BWa9J",
        "outputId": "e25102c9-9836-4c7a-c531-c41b0265cb94"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 200})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl-cu113\n",
            "  Downloading https://data.dgl.ai/wheels/dgl_cu113-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (220.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 220.6 MB 40 kB/s \n",
            "\u001b[?25hCollecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[K     |████████████████████████████████| 281 kB 9.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (2.23.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (2.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (4.64.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (3.0.4)\n",
            "Installing collected packages: psutil, dgl-cu113\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed dgl-cu113-0.8.2 psutil-5.9.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting word2vec\n",
            "  Downloading word2vec-0.11.1.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.21.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.1.0)\n",
            "Building wheels for collected packages: word2vec\n",
            "  Building wheel for word2vec (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2vec: filename=word2vec-0.11.1-py2.py3-none-any.whl size=164801 sha256=d2c951bc8475bb43b596e525994c16ab55f4e9460c0215c33d5b0a840637a4b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/c0/d4/29d797817e268124a32b6cf8beb8b8fe87b86f099d5a049e61\n",
            "Successfully built word2vec\n",
            "Installing collected packages: word2vec\n",
            "Successfully installed word2vec-0.11.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 16.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 58.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 73.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 200})'''))\n",
        "\n",
        "import torch\n",
        "try:\n",
        "  import dgl\n",
        "except ModuleNotFoundError:\n",
        "  CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "  !pip install dgl-{CUDA} -f https://data.dgl.ai/wheels/repo.html\n",
        "\n",
        "try:\n",
        "  import word2vec\n",
        "except ModuleNotFoundError:\n",
        "  !pip install word2vec\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "try:\n",
        "  import transformers\n",
        "except ModuleNotFoundError:\n",
        "  !pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x-SjdWTXfaR"
      },
      "source": [
        "# Main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twbAMVo4XhkJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af468f9-07bf-4e7d-a697-020111c583e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda:0\n",
            "name: Tesla P100-PCIE-16GB\n",
            "**************************************************\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "import word2vec\n",
        "\n",
        "import dgl\n",
        "from dgl.nn import GATConv\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "def Data(dataset_name):\n",
        "    NAME = dataset_name\n",
        "    if NAME not in ['20ng', 'r8', 'r52', 'oh', 'mr']:\n",
        "      raise ValueError('The dataset is not support')\n",
        "\n",
        "    PATH = '/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/data/'+NAME\n",
        "\n",
        "    train_texts = []\n",
        "    train_labels = []\n",
        "    with open(os.path.join(PATH, NAME+'-train-stemmed.txt'), 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        for line in data:\n",
        "          line = line.strip()\n",
        "          t = line.split('\\t')\n",
        "          train_texts.append(t[1])\n",
        "          train_labels.append(t[0])\n",
        "\n",
        "    dev_texts = []\n",
        "    dev_labels = []\n",
        "    with open(os.path.join(PATH, NAME+'-dev-stemmed.txt'), 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        for line in data:\n",
        "          line = line.strip()\n",
        "          t = line.split('\\t')\n",
        "          dev_texts.append(t[1])\n",
        "          dev_labels.append(t[0])\n",
        "\n",
        "    test_texts = []\n",
        "    test_labels = []\n",
        "    with open(os.path.join(PATH, NAME+'-test-stemmed.txt'), 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        for line in data:\n",
        "          line = line.strip()\n",
        "          t = line.split('\\t')\n",
        "          test_texts.append(t[1])\n",
        "          test_labels.append(t[0])\n",
        "\n",
        "    target_names = list(set(train_labels))\n",
        "    label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
        "\n",
        "    print(f'Dataset: {NAME}, Total train: {len(train_texts)+len(dev_texts)}, Train size: {len(train_texts)}, Dev size: {len(dev_texts)}, Test size: {len(test_texts)}, Num_class: {len(label2idx)}')\n",
        "    print(f'labels: {label2idx}')\n",
        "    print('*'*50)\n",
        "\n",
        "    return train_texts,  train_labels, dev_texts, dev_labels, test_texts, test_labels, label2idx\n",
        "\n",
        "class GATLayer(nn.Module):  # word-level or doc-level\n",
        "    def __init__(self,\n",
        "                 num_layers=1,\n",
        "                 in_dim=300,\n",
        "                 num_hidden=300,\n",
        "                 num_classes=8,\n",
        "                 heads=[1],\n",
        "                 activation=F.elu,\n",
        "                 feat_drop=.6,\n",
        "                 attn_drop=.6,\n",
        "                 negative_slope=.2,\n",
        "                 residual=False):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        if num_layers > 1:\n",
        "        # input projection (no residual)\n",
        "            self.gat_layers.append(GATConv(\n",
        "                in_dim, num_hidden, heads[0],\n",
        "                feat_drop, attn_drop, negative_slope, False, self.activation))\n",
        "            # hidden layers\n",
        "            for l in range(1, num_layers-1):\n",
        "                # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "                self.gat_layers.append(GATConv(\n",
        "                    num_hidden * heads[l-1], num_hidden, heads[l],\n",
        "                    feat_drop, attn_drop, negative_slope, residual, self.activation))\n",
        "            # output projection\n",
        "            self.gat_layers.append(GATConv(\n",
        "                num_hidden * heads[-2], num_classes, heads[-1],\n",
        "                feat_drop, attn_drop, negative_slope, residual, None))\n",
        "        else:\n",
        "            self.gat_layers.append(GATConv(\n",
        "                in_dim, num_classes, heads[0],\n",
        "                feat_drop, attn_drop, negative_slope, residual, None))\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        for l in range(self.num_layers):\n",
        "            h = self.gat_layers[l](g, h)\n",
        "            h = h.flatten(1) if l != self.num_layers - 1 else h.mean(1)  # last ouput layer is 'mean' operation\n",
        "        return h  # (num_nodes, num_classes)\n",
        "\n",
        "class HieGAT(torch.nn.Module):\n",
        "  def __init__(self, vocab, in_dim, num_classes, sen_max_len, num_layers, num_heads, device):\n",
        "    super(HieGAT, self).__init__()\n",
        "    self.vocab_size = len(vocab)\n",
        "    self.vocab = vocab\n",
        "\n",
        "    self.node_hidden = torch.nn.Embedding(self.vocab_size, 300)  # (num_vocab+1, num_hidden), include 'unk\n",
        "    self.node_hidden.weight.data.copy_(torch.tensor(self.load_w2v('/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/glove.6B/glove.6B.300d.w2vformat.txt')))\n",
        "    self.node_hidden.weight.requires_grad = False\n",
        "\n",
        "    self.gram = 1\n",
        "    self.SEN_GRAM = 1\n",
        "\n",
        "    self.DEVICE = device\n",
        "    self.sen_max_len = sen_max_len\n",
        "\n",
        "    self.lam_d = torch.nn.Parameter(torch.tensor(0.5)).to(device)\n",
        "    self.lam_s= torch.nn.Parameter(torch.tensor(0.5)).to(device)\n",
        "    self.lam_w = torch.nn.Parameter(torch.tensor(0.5)).to(device)\n",
        "\n",
        "    self.gat_w = GATLayer(in_dim=in_dim, num_classes=num_classes, num_layers=num_layers, heads=[num_heads]*(num_layers-1)+[1], residual=False)\n",
        "    self.gat_s = GATLayer(in_dim=num_classes, num_classes=num_classes, num_layers=num_layers, heads=[num_heads]*(num_layers-1)+[1], residual=False)\n",
        "    self.gat_d = GATLayer(in_dim=in_dim, num_classes=num_classes, num_layers=num_layers, heads=[num_heads]*(num_layers-1)+[1], residual=False)\n",
        "    \n",
        "    #self.linear = torch.nn.Linear(out_dim, num_classes)\n",
        "\n",
        "  def load_w2v(self, path):\n",
        "    w2v = word2vec.load(path)\n",
        "    embedding_matrix = []\n",
        "    for word in self.vocab:\n",
        "      try:\n",
        "        embedding_matrix.append(w2v[word])\n",
        "      except KeyError:\n",
        "        embedding_matrix.append(np.zeros(len(w2v['the'])))\n",
        "    \n",
        "    embedding_matrix = np.array(embedding_matrix)\n",
        "    \n",
        "    return embedding_matrix\n",
        "\n",
        "  def add_edges(self, sample,  local_vocab_id):  # add edges for word-level\n",
        "    edges = []\n",
        "    for i, src in enumerate(sample):\n",
        "      u = local_vocab_id[src]\n",
        "      for j in range(max(0, i-self.gram), min(i+self.gram +1, len(sample))):\n",
        "        dst = sample[j]\n",
        "        v = local_vocab_id[dst]\n",
        "\n",
        "        edges.append([u, v])\n",
        "    return edges\n",
        "\n",
        "  def sample2graph(self, sample):  # sample: [78, 63, 63, 33, 78,  ...], Type: list\n",
        "    if len(sample) == 0:\n",
        "      raise Exception('sample length is equal 0')\n",
        "    if len(sample)>self.sen_max_len:\n",
        "      sample = sample[:self.sen_max_len]\n",
        "\n",
        "    local_vocab = set(sample)  # {78, 63, 33, ...}\n",
        "\n",
        "    n = len(local_vocab)\n",
        "    local_vocab_id = dict(zip(local_vocab, range(n)))  # {78:0, 63:1, 33:2, ...}\n",
        "    u, v = zip(*self.add_edges(sample, local_vocab_id))\n",
        "    u = torch.tensor(u).to(self.DEVICE)\n",
        "    v = torch.tensor(v).to(self.DEVICE)\n",
        "\n",
        "    g = dgl.graph((u, v), num_nodes=n).to(self.DEVICE)\n",
        "\n",
        "    local_vocab_tensor = torch.tensor(list(local_vocab)).to(self.DEVICE)\n",
        "\n",
        "    g.ndata['z'] =  self.node_hidden(local_vocab_tensor)\n",
        "\n",
        "    return g\n",
        "\n",
        "  def add_edges_sen(self, n): # add edges for sentence-level\n",
        "    edges = []\n",
        "    for i in range(n):\n",
        "        u = i\n",
        "        for j in range(max(0, i-self.SEN_GRAM), min(i+self.SEN_GRAM+1, n)):\n",
        "            v = j\n",
        "            # - first connect the new sub_graph\n",
        "            edges.append([u, v])\n",
        "    return edges\n",
        "\n",
        "  def sentence2graph(self, inputs):\n",
        "    n = len(inputs)  # Number of sentence in each sample\n",
        "    edges = self.add_edges_sen(n) \n",
        "    u, v = zip(*edges)\n",
        "    \n",
        "    u = torch.tensor(u).to(self.DEVICE)\n",
        "    v = torch.tensor(v).to(self.DEVICE)\n",
        "\n",
        "    g = dgl.graph((u, v), num_nodes=n).to(self.DEVICE)\n",
        "    g.ndata['z'] = inputs.to(self.DEVICE)\n",
        "\n",
        "    return g\n",
        "\n",
        "  def forward(self, inputs):  # inputs: (batch_size, num_sentence, tokens)\n",
        "    senlev_g = []\n",
        "    worlev_out = []\n",
        "    for token,_ in inputs:  # one sample\n",
        "      list_g = [self.sample2graph(sen)  for sen in token]\n",
        "      tg = dgl.batch(list_g)\n",
        "      w_h = self.gat_w(tg, tg.ndata['z'] )  # (N, C)\n",
        "      tg.ndata['z'] = w_h.to(self.DEVICE)  # Update h\n",
        "\n",
        "      out_word_lev = dgl.mean_nodes(tg, feat='z')  # word-level output: (num_sentence, C)\n",
        "      worlev_out.append(torch.mean(out_word_lev, dim=0).unsqueeze(dim=0))\n",
        "      \n",
        "      s_g = self.sentence2graph(out_word_lev)  # sen-level graph\n",
        "      senlev_g.append(s_g)\n",
        "    sen_g = dgl.batch(senlev_g)\n",
        "    sen_h = self.gat_s(sen_g, sen_g.ndata['z'])\n",
        "    sen_g.ndata['z'] = sen_h.to(self.DEVICE)  # (batch_size, num_classes)\n",
        "\n",
        "\n",
        "    doc_inputs = [] # inputs: (batch_size, tokens)\n",
        "    for x,_ in inputs:\n",
        "      temp = [w for t in x for w in t]\n",
        "      doc_inputs.append(temp)\n",
        "    doc_g = [self.sample2graph(d)  for d in doc_inputs]\n",
        "    batch_dg = dgl.batch(doc_g)\n",
        "    d_h = self.gat_d(batch_dg, batch_dg.ndata['z'])\n",
        "    batch_dg.ndata['z'] = d_h\n",
        "\n",
        "\n",
        "    out_wor = torch.cat(worlev_out, dim=0)  # word-level output: (batch_size, C)\n",
        "    out_sen = dgl.mean_nodes(sen_g, feat='z')  # sen-level output (batch_size, C)\n",
        "    out_doc = dgl.mean_nodes(batch_dg, feat='z')  # doc-level output: (batch_size, C)\n",
        "\n",
        "    total = self.lam_w*F.log_softmax(out_wor, dim=-1) + self.lam_s*F.log_softmax(out_sen, dim=-1) + self.lam_d*F.log_softmax(out_doc, dim=-1)  # (batch_size, C)\n",
        "    \n",
        "    return total\n",
        "\n",
        "def train(model, epoch, input, dev_input, DEVICE, DATASET):\n",
        "  num_train_steps = len(input) * epoch\n",
        "  num_warmup_steps = int(0.1 * num_train_steps)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4, lr=1e-3)\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps = num_train_steps)\n",
        "\n",
        "  PATIENCE = 7  # Patience on dev set to finish training\n",
        "  no_improve = 0  # No improvement on dev set\n",
        "\n",
        "  best_acc = 0.0\n",
        "  dur = []\n",
        "\n",
        "  for e in range(epoch):\n",
        "    t0 = time.time()\n",
        "    improved = ''\n",
        "    model.train()\n",
        "\n",
        "    for i, ba in enumerate(input):\n",
        "      y = []\n",
        "      for _,label in ba:\n",
        "        y.append(label)\n",
        "      y = torch.tensor(y).to(DEVICE)\n",
        "\n",
        "      outputs = model(ba)\n",
        "      loss = F.nll_loss(outputs, y)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "\n",
        "    val_acc = dev(model, dev_input, DEVICE)\n",
        "    if val_acc>best_acc:\n",
        "      best_acc = val_acc\n",
        "      no_improve = 0\n",
        "      improved = '*'\n",
        "      torch.save(model, f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/hiegat1_{DATASET}.pkl')\n",
        "    else:\n",
        "      no_improve+=1\n",
        "    dur.append(time.time()-t0)\n",
        "    print(f'Epoch: {e}, Train loss:{loss.item():.4f}, Val acc: {val_acc:.4f}, Times: {np.mean(dur):.4f}s, {improved}')\n",
        "\n",
        "    if no_improve>=PATIENCE:\n",
        "      print(f'No improvement on dev set, early stopping')\n",
        "      break\n",
        "\n",
        "def dev(model, input, DEVICE):\n",
        "  model.eval()\n",
        "  total_pred = 0.0\n",
        "  correct = 0.0\n",
        "  for i, ba in enumerate(input):\n",
        "    y = []\n",
        "    for _,label in ba:\n",
        "      y.append(label)\n",
        "    y = torch.tensor(y).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(ba)\n",
        "      pred = torch.argmax(outputs, dim=1)\n",
        "      \n",
        "      correct_pred = torch.sum(pred==y)\n",
        "      correct += correct_pred\n",
        "      total_pred += len(y)\n",
        "  \n",
        "  return torch.div(correct, total_pred)  # Acc on dev set\n",
        "\n",
        "def test(model, input, DEVICE):\n",
        "  model.eval()\n",
        "  total_pred = 0.0\n",
        "  correct = 0.0\n",
        "  for i, ba in enumerate(input):\n",
        "    y = []\n",
        "    for _,label in ba:\n",
        "      y.append(label)\n",
        "    y = torch.tensor(y).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(ba)\n",
        "      pred = torch.argmax(outputs, dim=1)\n",
        "      \n",
        "      correct_pred = torch.sum(pred==y)\n",
        "      correct += correct_pred\n",
        "      total_pred += len(y)\n",
        "  \n",
        "  return torch.div(correct, total_pred)  # Acc on dev set\n",
        "\n",
        "def  buildvocab(sample, min_count=5):\n",
        "  '''\n",
        "  sample: ['wo xihuan ziran yuyan chuli', 'wo ai shengdu xuexi',  'wo xihuan jiqi xuexi']\n",
        "\n",
        "  '''\n",
        "\n",
        "  MIN_COUNT = min_count\n",
        "\n",
        "  freq = {}\n",
        "  for i in sample:\n",
        "    for t in word_tokenize(i):\n",
        "      if t not in freq:\n",
        "        freq[t] = 0\n",
        "\n",
        "  for i in sample:\n",
        "    for t in word_tokenize(i):\n",
        "      freq[t] +=1\n",
        "\n",
        "  del_key = []\n",
        "  for i in freq:\n",
        "    if freq[i]<MIN_COUNT:\n",
        "      del_key.append(i)\n",
        "  \n",
        "  for i in del_key:\n",
        "    freq.pop(i)\n",
        "\n",
        "  vocab_id = {}\n",
        "  for i, key in enumerate(freq):\n",
        "    vocab_id[key] = i\n",
        "  print(f'vocab_id size: {len(vocab_id)}')\n",
        "  print('*'*50)\n",
        "  \n",
        "  return vocab_id\n",
        "\n",
        "#vocab = {'this':0, 'is':1, 'first':2, 'sentence':3, 'however':4, 'could':5, 'be':6, '<unk>':7, '<pad>':100}\n",
        "def w2id(input, vocab):\n",
        "  ids = []\n",
        "  for w in input:\n",
        "    if w in [',', '.', '!', '?']:\n",
        "      continue\n",
        "    if w in vocab:\n",
        "      ids.append(vocab[w])\n",
        "    else: \n",
        "      ids.append(vocab['<unk>'])\n",
        "  return ids\n",
        "\n",
        "def batch_hiegnn(texts, labels, batch_size, label2idx, vocab_id):\n",
        "  x = texts\n",
        "  y = [label2idx[t] for t in labels]\n",
        "  data = [(x[i], y[i]) for i in range(len(y))]\n",
        "  random.shuffle(data)\n",
        "\n",
        "  input = [(sent_tokenize(x), y)  for x, y in data]\n",
        "\n",
        "  input1 = []\n",
        "  for sample, y in input:\n",
        "    t = []\n",
        "    for s in sample:\n",
        "      temp = w2id(word_tokenize(s), vocab_id)\n",
        "      if len(temp) >0:\n",
        "        t.append(temp)\n",
        "    input1.append((t, y))\n",
        "\n",
        "  input2 = [input1[i: i+batch_size] for i in range(0, len(input1), batch_size)]\n",
        "\n",
        "  return input2\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "  print(f'device: {DEVICE}')\n",
        "  print(f'name: {torch.cuda.get_device_name(0)}')\n",
        "  print(f'*'*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh4LyB-NXrD4"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oAKUGPmXnlw",
        "outputId": "673b5478-40da-4a60-a574-f67fcc861576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: mr, Total train: 7108, Train size: 6397, Dev size: 711, Test size: 3554, Num_class: 2\n",
            "labels: {'1': 0, '0': 1}\n",
            "**************************************************\n",
            "vocab_id size: 14322\n",
            "**************************************************\n",
            "Epoch: 0, Train loss:1.0274, Val acc: 0.7243, Times: 67.6502s, *\n",
            "Epoch: 1, Train loss:0.9790, Val acc: 0.7328, Times: 68.5062s, *\n",
            "Epoch: 2, Train loss:0.9583, Val acc: 0.7384, Times: 68.0826s, *\n",
            "Epoch: 3, Train loss:0.9138, Val acc: 0.7314, Times: 67.8515s, \n",
            "Epoch: 4, Train loss:0.8999, Val acc: 0.7384, Times: 67.7234s, \n",
            "Epoch: 5, Train loss:0.9260, Val acc: 0.7173, Times: 67.7241s, \n",
            "Epoch: 6, Train loss:0.9101, Val acc: 0.7581, Times: 67.6443s, *\n",
            "Epoch: 7, Train loss:0.9007, Val acc: 0.7440, Times: 67.5873s, \n",
            "Epoch: 8, Train loss:0.9461, Val acc: 0.7665, Times: 67.5509s, *\n",
            "Epoch: 9, Train loss:0.9310, Val acc: 0.7722, Times: 67.6131s, *\n",
            "Epoch: 10, Train loss:0.9347, Val acc: 0.7707, Times: 67.5563s, \n",
            "Epoch: 11, Train loss:0.9176, Val acc: 0.7722, Times: 67.4834s, \n",
            "Epoch: 12, Train loss:0.9509, Val acc: 0.7679, Times: 67.4025s, \n",
            "Epoch: 13, Train loss:0.9206, Val acc: 0.7693, Times: 67.3928s, \n",
            "Epoch: 14, Train loss:0.9128, Val acc: 0.7679, Times: 67.3600s, \n",
            "Epoch: 15, Train loss:0.8964, Val acc: 0.7595, Times: 67.3159s, \n",
            "Epoch: 16, Train loss:0.9146, Val acc: 0.7707, Times: 67.2773s, \n",
            "No improvement on dev set, early stopping\n",
            "Test accuracy: 0.7389\n",
            "Run 1/1, Time: 19.9m, min_count=1, num_head=9, num_layer=2\n",
            "**************************************************\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "\n",
        "DATASET = 'mr'\n",
        "EPOCH = 50\n",
        "SEN_MAX_LEN = 128\n",
        "\n",
        "NUM_HEADS = [9]\n",
        "NUM_LAYER = [2]\n",
        "\n",
        "\n",
        "if DATASET=='mr':\n",
        "  MIN_COUNT = [1]\n",
        "\n",
        "elif DATASET=='20ng':\n",
        "  MIN_COUNT = [10,20]\n",
        "\n",
        "else:\n",
        "   MIN_COUNT = [5]\n",
        "\n",
        "\n",
        "def ParaTuning(DATASET, SEN_MAX_LEN, DEVICE, EPOCH, p1, p2, p3):  # Parameters tuning\n",
        "  NUM_HEADS = p1\n",
        "  NUM_LAYER = p2\n",
        "  MIN_COUNT = p3\n",
        "\n",
        "  comp_time = 0\n",
        "  total_comp = len(p1) * len(p2) * len(p3) \n",
        "\n",
        "  para = []\n",
        "  acc = []\n",
        "  dur = []\n",
        "\n",
        "  train_texts,  train_labels, dev_texts, dev_labels, test_texts, test_labels, label2idx = Data(DATASET)\n",
        "  NUM_CLASS = len(label2idx)\n",
        "  \n",
        "\n",
        "  for min_count in MIN_COUNT:\n",
        "    vocab_id = buildvocab(train_texts, min_count=min_count)\n",
        "    vocab_id['<unk>']=len(vocab_id)  # for OOV\n",
        "\n",
        "    for num_head in NUM_HEADS:\n",
        "        BATCH_SIZE=128\n",
        "        train_batch = batch_hiegnn(train_texts, train_labels, BATCH_SIZE, label2idx, vocab_id)\n",
        "        dev_batch = batch_hiegnn(dev_texts, dev_labels, BATCH_SIZE, label2idx, vocab_id)\n",
        "        test_batch = batch_hiegnn(test_texts, test_labels, BATCH_SIZE, label2idx, vocab_id)\n",
        "\n",
        "        for num_layer in NUM_LAYER:\n",
        "          tt = time.time()\n",
        "          model = HieGAT(vocab_id,  in_dim=300,  num_classes=NUM_CLASS, sen_max_len=SEN_MAX_LEN, num_layers=num_layer, num_heads=num_head, device=DEVICE)\n",
        "          model.to(DEVICE)\n",
        "\n",
        "          train(model, EPOCH, train_batch, dev_batch, DEVICE, DATASET)\n",
        "\n",
        "          best_model = torch.load(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/hiegat1_{DATASET}.pkl')\n",
        "          res = test(best_model, test_batch, DEVICE)\n",
        "          print(f'Test accuracy: {res.cpu().numpy():.4f}')\n",
        "\n",
        "          comp_time+=1\n",
        "          print(f'Run {comp_time}/{total_comp}, Time: {(time.time()-tt)/60:.1f}m, min_count={min_count}, num_head={num_head}, num_layer={num_layer}')\n",
        "          print(f'*'*50)\n",
        "\n",
        "          acc.append(res.cpu().numpy())\n",
        "          dur.append((time.time()-tt)/60)\n",
        "          para.append(f'min_count={min_count}, num_head={num_head}, num_layer={num_layer}')\n",
        "\n",
        "    \n",
        "  df = pd.concat([pd.DataFrame({'para': para}), pd.DataFrame({'acc': acc}), pd.DataFrame({'dur': dur})], axis=1)\n",
        "\n",
        "  df.to_csv(f'/content/drive/MyDrive/Colab_Notebooks/TextLevelGNN/model/3-type-levels-{DATASET}.csv')\n",
        "\n",
        "ParaTuning(DATASET, SEN_MAX_LEN, DEVICE, EPOCH, NUM_HEADS, NUM_LAYER,  MIN_COUNT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def visualize(h, color):\n",
        "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "    plt.show()\n",
        "\n",
        "h = torch.ones(4,10)\n",
        "y = [0,1,0,2]\n",
        "visualize(h, y)"
      ],
      "metadata": {
        "id": "LmS_SUx-GgK0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "a8e04e12-8b42-4ccf-a9b6-f8f9178e136d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET= 'mr'\n",
        "\n",
        "if DATASET=='mr':\n",
        "  MIN_COUNT = [1]\n",
        "\n",
        "elif DATASET=='20ng':\n",
        "  MIN_COUNT = [10,20]\n",
        "\n",
        "else:\n",
        "   MIN_COUNT = [5]\n",
        "\n",
        "print(MIN_COUNT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOf6K_Rd3Arp",
        "outputId": "28b6afe9-fcb2-4aab-a965-9625d4d0bde5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HieGAT_1",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1RSGemL2Dz9Zpa6SOjKdU6rFWMFC_OUK8",
      "authorship_tag": "ABX9TyOPbfMaOicqqy8XcF6DNYqc",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}