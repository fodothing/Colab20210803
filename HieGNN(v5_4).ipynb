{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hshuai97/Colab20210803/blob/main/HieGNN(v5_4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpWgfCxSlHR"
      },
      "source": [
        "word-level: GAT or GCN\n",
        "\n",
        "sentence-level: GAT or GCN\n",
        "\n",
        "Reference:\n",
        "\n",
        "1. [GAT and GCN inplementation in Deep Graph Library](https://github.com/dmlc/dgl/blob/master/examples/pytorch/gat/gat.py)\n",
        "\n",
        "2.  [Lianzhe Huang source code: ](https://github.com/mojave-pku/TextLevelGCN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l7KBFy-dO_K"
      },
      "source": [
        "# Install labraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV0W4YE9SjBD"
      },
      "outputs": [],
      "source": [
        "# Install deep graph labrary: https://www.dgl.ai/pages/start.html\n",
        "import torch\n",
        "try:\n",
        "  import dgl\n",
        "except ModuleNotFoundError:\n",
        "  # Installing dgl package with specific CUDA\n",
        "  CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "  !pip install dgl-{CUDA} -f https://data.dgl.ai/wheels/repo.html\n",
        "\n",
        "# Install word2vec\n",
        "try:\n",
        "  import word2vec  # type: ignore\n",
        "except ModuleNotFoundError:\n",
        "  !pip install word2vec # type: ignore\n",
        "\n",
        "try:\n",
        "  import torch_scatter\n",
        "except ModuleNotFoundError:\n",
        "  TORCH = torch.__version__.split('+')[0]\n",
        "  CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "  !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv9DuJzudx5W"
      },
      "source": [
        "# Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyg3fhHUdzo0",
        "outputId": "ee397dc3-a718-4a1a-a9dc-474d48e2f0f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting parsing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile parsing.py\n",
        "# build graph\n",
        "import torch\n",
        "import dgl\n",
        "\n",
        "# dada_helper\n",
        "import os\n",
        "import re\n",
        "\n",
        "# model\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import word2vec\n",
        "import math\n",
        "from torch_scatter import scatter_add, scatter_max, scatter_mean\n",
        "\n",
        "# train\n",
        "import random\n",
        "import argparse\n",
        "from time import time\n",
        "\n",
        "\n",
        "class DataHelper(object):  # Preprocess dataset. (Almost the same code as Huang, see reference)\n",
        "    def __init__(self, dataset, mode='train', vocab=None):\n",
        "        allowed_data = ['20ng', 'r8', 'r52', 'oh', 'mr', 'agnews']  # six datasets\n",
        "        tmp_data = dataset.split('/')\n",
        "        dataset = tmp_data[len(tmp_data)-1]\n",
        "\n",
        "        if dataset not in allowed_data:\n",
        "            raise ValueError('currently allowed data: %s' % ','.join(allowed_data))\n",
        "        else:\n",
        "            self.dataset = dataset\n",
        "\n",
        "        if self.dataset =='20ng':  # The 20ng  dataset is large.\n",
        "          self.min_count = 20\n",
        "        elif self.dataset == 'agnews':  # The agnews  dataset is large.\n",
        "          self.min_count = 35\n",
        "        elif dataset == 'mr':\n",
        "          self.min_count =2  # The 'mr' dataset is small.\n",
        "        else:\n",
        "          self.min_count = 5\n",
        "\n",
        "        self.mode = mode\n",
        "        self.base = os.path.join('/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/data', self.dataset)  # 'data/r8'\n",
        "        self.current_set = os.path.join(self.base, '%s-%s-stemmed.txt' % (self.dataset, self.mode))\n",
        "\n",
        "        with open(os.path.join(self.base, 'label.txt')) as f:\n",
        "            labels = f.read()\n",
        "        self.labels_str = labels.split('\\n')\n",
        "\n",
        "        content, label = self.get_content()  # Get content and label from dataset.\n",
        "        self.label = self.label_to_onehot(label)  # Convert to 1-hot encode.\n",
        "        \n",
        "        if vocab is None:\n",
        "            self.vocab = []\n",
        "            try:\n",
        "                self.get_vocab()\n",
        "            except FileNotFoundError:\n",
        "                self.build_vocab(content, self.min_count)\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "\n",
        "        self.d = dict(zip(self.vocab, range(len(self.vocab))))\n",
        "\n",
        "        temp = []  # Word-level\n",
        "        for doc in content:\n",
        "            doc_tmp = []\n",
        "            sentence = doc.split('[sep]')  # Split document according to '[sep]' symbol.\n",
        "            for sen in sentence:  # Take sentence from document\n",
        "                sen = sen.strip()  # Remove space.\n",
        "                if len(sen)>0:  # Remove blank exception.\n",
        "                    words = list(map(lambda x: self.word2id(x), sen.split()))\n",
        "                    doc_tmp.append(words)\n",
        "            temp.append(doc_tmp)\n",
        "        \n",
        "        self.content = temp  # Word-level content split, 3-d.\n",
        "\n",
        "        self.content_doc = [list(map(lambda x: self.word2id(x), doc.split())) for doc in content]  # Doc-level content split, 2-d.\n",
        "\n",
        "    def label_to_onehot(self, label_str):\n",
        "        return [self.labels_str.index(l) for l in label_str]\n",
        "\n",
        "    def get_content(self):\n",
        "        with open(self.current_set) as f:  # open dataset\n",
        "            all = f.read()\n",
        "            all = all.split('\\n')\n",
        "            content = [line.split('\\t') for line in all]  # 2 elements:(label, str_text)\n",
        "        if self.dataset in ['r8', '20ng', 'r52', 'mr', 'oh','agnews']:\n",
        "            cleaned = []\n",
        "            for i, pair in enumerate(content):\n",
        "                if len(pair) < 2 or len(pair[1]) <5:  # # remove the sample that lack of 'str_text' or 'label'; or remove short text\n",
        "                    # print(i, pair)\n",
        "                    pass\n",
        "                else:\n",
        "                    cleaned.append(pair)\n",
        "        else:\n",
        "            cleaned = content\n",
        "\n",
        "        label, content = zip(*cleaned)  # '*' means unpack a list\n",
        "\n",
        "        return content, label\n",
        "\n",
        "    def word2id(self, word):\n",
        "        try:\n",
        "            result = self.d[word]\n",
        "        except KeyError:\n",
        "            result = self.d['UNK']\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_vocab(self):\n",
        "        with open(os.path.join(self.base, 'vocab-'+str(self.min_count)+'.txt')) as f:  # For example, vocab-5.txt\n",
        "            vocab = f.read()\n",
        "            self.vocab = vocab.split('\\n')\n",
        "            #print(f'self.vocab: {self.vocab}')\n",
        "\n",
        "    def build_vocab(self, content, min_count=10):\n",
        "        vocab = []\n",
        "\n",
        "        for c in content:\n",
        "            words = c.split()\n",
        "            for word in words:\n",
        "                if word not in vocab and word !='[sep]' and word!='':\n",
        "                    vocab.append(word)\n",
        "\n",
        "        freq = dict(zip(vocab, [0 for i in range(len(vocab))]))\n",
        "\n",
        "        for c in content:\n",
        "            words = c.split()\n",
        "            for word in words:\n",
        "              if  word !='[sep]' and word!='':\n",
        "                freq[word] += 1\n",
        "\n",
        "        results = []\n",
        "        for word in freq.keys():\n",
        "            if freq[word] < min_count:\n",
        "                continue\n",
        "            else:\n",
        "                results.append(word)\n",
        "\n",
        "        results.insert(0, 'UNK')\n",
        "        with open(os.path.join(self.base, 'vocab-'+str(self.min_count)+'.txt'), 'w') as f:\n",
        "            f.write('\\n'.join(results))\n",
        "\n",
        "        self.vocab = results\n",
        "\n",
        "    def batch_iter(self, batch_size, num_epoch):\n",
        "        for i in range(num_epoch):\n",
        "            num_per_epoch = int(len(self.content) / batch_size)  # Split content by batch size.\n",
        "            for batch_id in range(num_per_epoch):\n",
        "                start = batch_id * batch_size\n",
        "                end = min((batch_id + 1) * batch_size, len(self.content))\n",
        "\n",
        "                content = self.content[start:end]  # Word-Level: (batch_size, num_sen, num_words) \n",
        "                label = self.label[start:end]\n",
        "\n",
        "                content_doc = self.content_doc[start:end]  # Sentence-Level: (batch_size, num_words)\n",
        "\n",
        "                device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "                \n",
        "                y = torch.tensor(label).to(device)\n",
        "\n",
        "                yield content, content_doc, y, i   # Return content, label and epoch.\n",
        "\n",
        "\n",
        "class WordLevelGCN(torch.nn.Module): # GCN for word-level\n",
        "    def __init__(self, hidden_size_node, vocab, n_gram, edges_matrix, edges_num, max_length=300):\n",
        "        super(WordLevelGCN, self).__init__()\n",
        "\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.vocab = vocab  # Prepare for other methods in the same class.\n",
        "\n",
        "        self.node_hidden = torch.nn.Embedding(len(vocab), hidden_size_node)  # Word-level node hidden (embedding).\n",
        "        self.node_hidden.weight.data.copy_(torch.tensor(self.load_word2vec('/content/drive/MyDrive/Colab_Notebooks/DATA/glove.6B/glove.6B.100d.w2vformat.txt')))\n",
        "        self.node_hidden.weight.requires_grad = True\n",
        "\n",
        "        self.node_eta = torch.nn.Embedding.from_pretrained(torch.rand(len(vocab), 1), freeze=False)  # Word-level node save itself feature according to eta percent.\n",
        "\n",
        "        self.edges_matrix = edges_matrix  # All global word-level edges are stored in the matrix.\n",
        "        self.edges_num = edges_num  # The number of word-level edges.\n",
        "        self.node_edge_w = torch.nn.Embedding.from_pretrained(torch.ones(edges_num, 1), freeze=False)  # Setting trainable parameters for word-level edges.\n",
        "\n",
        "        self.len_vocab = len(vocab)\n",
        "        self.d = dict(zip(vocab, range(len(vocab))))  # Set new index for vocab.\n",
        "\n",
        "        self.ngram = n_gram\n",
        "        self.max_length = max_length  # Maximum number of word-level nodes.\n",
        "\n",
        "    def load_word2vec(self, word2vec_file):  # Loading embeddings representation from w2v.\n",
        "      model = word2vec.load(word2vec_file)\n",
        "\n",
        "      embedding_matrix = []\n",
        "\n",
        "      for word in self.vocab:\n",
        "          try:\n",
        "              embedding_matrix.append(model[word])\n",
        "          except KeyError:\n",
        "              # print(f'Line 269. The word not in vocab:{word}')\n",
        "              #unk_word=np.zeros(300)\n",
        "              #print(f\"line 271, model['the].shape: {len(model['the'])}\")  # [,300]\n",
        "              embedding_matrix.append(np.zeros(len(model['the'])))  # other unknow words use zeros vector\n",
        "\n",
        "      embedding_matrix = np.array(embedding_matrix)\n",
        "\n",
        "      return embedding_matrix\n",
        "\n",
        "    def build_graph(self, word_ids):  # Build word level graph.\n",
        "        if len(word_ids) > self.max_length:\n",
        "            word_ids = word_ids[ : self.max_length]\n",
        "        \n",
        "        local_vocab = set(word_ids)\n",
        "        old_to_new = dict(zip(local_vocab, range(len(local_vocab))))\n",
        "\n",
        "        local_vocab = torch.tensor(list(local_vocab)).to(self.device)  # Graph on device, so need local_vocab on same device.\n",
        "\n",
        "        graph = dgl.DGLGraph()\n",
        "        graph = graph.to(self.device)\n",
        "\n",
        "        graph.add_nodes(len(local_vocab)) # Add nodes for graph.\n",
        "        graph.ndata['h'] = self.node_hidden(local_vocab)  # Add node features for graph.\n",
        "        \n",
        "        eta = torch.sigmoid(self.node_eta(local_vocab))  # Limit to [0-1].\n",
        "        graph.ndata['eta'] = eta  # Add node eta for graph\n",
        "\n",
        "        edges, edges_id = self.add_edges(word_ids, old_to_new)\n",
        "        edges_id = torch.LongTensor(edges_id).to(self.device)  # The edges_id is a 1-d list.\n",
        "        srcs, dsts = zip(*edges)  # get source and destination nodes id (local id).\n",
        "        \n",
        "        graph.add_edges(srcs, dsts)  # Add edges for graph.\n",
        "        graph.edata['w'] = self.node_edge_w(edges_id) # Add weight for edges.\n",
        "\n",
        "        return graph\n",
        "    \n",
        "    def add_edges(self, word_ids: list, old_to_new:dict):  # Add word-level edges for word-level graph.\n",
        "        edges = []\n",
        "        old_edge_id = []\n",
        "        for index, src_word_old in enumerate(word_ids):\n",
        "            src = old_to_new[src_word_old]\n",
        "            for i in range(max(0, index - self.ngram), min(index + self.ngram + 1, len(word_ids))):\n",
        "                dst_word_old = word_ids[i]\n",
        "                dst = old_to_new[dst_word_old]\n",
        "\n",
        "                # - first connect the new sub_graph\n",
        "                edges.append([src, dst])\n",
        "                # - then get the hidden from parent_graph\n",
        "                old_edge_id.append(self.edges_matrix[src_word_old, dst_word_old])\n",
        "\n",
        "            # self circle\n",
        "            edges.append([src, src])  # All edges.\n",
        "            old_edge_id.append(self.edges_matrix[src_word_old, src_word_old])\n",
        "\n",
        "        return edges, old_edge_id\n",
        "    \n",
        "    def forward(self, word_id):  # Update word-level graph\n",
        "      graphs = []\n",
        "      graphs_id = {}  # Dictionary data formar, such as {0:[0,3]}\n",
        "\n",
        "      count=0\n",
        "      for i in range(len(word_id)):\n",
        "        t_index = [count]\n",
        "        if len(word_id[i])>0:\n",
        "          for j in range(len(word_id[i])):\n",
        "            graphs.append(self.build_graph(word_id[i][j]))\n",
        "            count+=1\n",
        "          t_index.append(count)  # [start, end]\n",
        "          graphs_id[i]=t_index\n",
        "        else:\n",
        "          print('Error! Sentence length must be greater than 0.')\n",
        "          break\n",
        "\n",
        "\n",
        "      batch_graph = dgl.batch(graphs)  # batching update\n",
        "      batch_graph.update_all(\n",
        "          message_func = dgl.function.src_mul_edge('h', 'w', 'weighted_message'),\n",
        "          reduce_func = dgl.function.max('weighted_message','M')\n",
        "      )\n",
        "      batch_graph.ndata['h'] = batch_graph.ndata['eta']*batch_graph.ndata['h'] + batch_graph.ndata['M'] * (1 - batch_graph.ndata['eta'])  # Update word-level node feature.\n",
        "      out_w = dgl.sum_nodes(batch_graph, feat='h')  # Convert word-level graph to one vector (num_graph, 300)\n",
        "\n",
        "      return out_w, graphs_id  # (num_sen, hidden_size_node); graphs_id is  dictionary data format\n",
        "\n",
        "\n",
        "class SentenceLevelGCN(torch.nn.Module):  # GCN for sentence-level\n",
        "    def __init__(self, hidden_size_node, vocab, n_gram, edges_matrix, edges_num):\n",
        "      super(SentenceLevelGCN, self).__init__()\n",
        "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class WordGATLayer(torch.nn.Module):  # GAT for word-level\n",
        "  def __init__(self, in_dim, out_dim, hidden_size_node, vocab, n_gram, edges_matrix, edges_num, max_length=300):\n",
        "      super(WordGATLayer, self).__init__()\n",
        "\n",
        "      self.fc = torch.nn.Linear(in_dim, out_dim, bias=False)\n",
        "      self.attn_fc = torch.nn.Linear(2*out_dim, 1, bias=False)\n",
        "      self.reset_parameters()\n",
        "\n",
        "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "      self.vocab = vocab\n",
        "      \n",
        "      self.word_ngram = n_gram   # n-gram for word-level\n",
        "      self.edges_matrix = edges_matrix\n",
        "      self.max_length = max_length\n",
        "\n",
        "      self.node_hidden = torch.nn.Embedding(len(vocab), hidden_size_node)  # Word-level node hidden (embedding).\n",
        "      self.node_hidden.weight.data.copy_(torch.tensor(self.load_word2vec('/content/drive/MyDrive/Colab_Notebooks/DATA/glove.6B/glove.6B.100d.w2vformat.txt')))\n",
        "      self.node_hidden.weight.requires_grad = True\n",
        "  \n",
        "  def reset_parameters(self):\n",
        "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
        "        gain = torch.nn.init.calculate_gain('relu')\n",
        "        torch.nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        torch.nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
        "\n",
        "  def load_word2vec(self, word2vec_file):  # Loading embeddings representation from w2v.\n",
        "      model = word2vec.load(word2vec_file)\n",
        "\n",
        "      embedding_matrix = []\n",
        "\n",
        "      for word in self.vocab:\n",
        "          try:\n",
        "              embedding_matrix.append(model[word])\n",
        "          except KeyError:\n",
        "              embedding_matrix.append(np.zeros(len(model['the'])))  # other unknow words use zeros vector\n",
        "\n",
        "      embedding_matrix = np.array(embedding_matrix)\n",
        "\n",
        "      return embedding_matrix\n",
        "\n",
        "  def edge_attention(self, edges):  # func = lambda edges : F.leaky_relu(self.attn_fc(z2))\n",
        "        # edge UDF for equation (2)\n",
        "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
        "        a = self.attn_fc(z2)  # (2*out_dim, 1)\n",
        "        return {'e': F.leaky_relu(a)}\n",
        "\n",
        "  def message_func(self, edges):  # Passing two tensors: the transformed z embedding of source node and un-normalized attention score e\n",
        "        # message UDF for equation (3) & (4)\n",
        "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
        "\n",
        "  def reduce_func(self, nodes):  # Performing two tasks: 1 normalize the attention score e, 2 Aggregate neighbor embeddings weighted by the attention scores\n",
        "        # reduce UDF for equation (3) & (4)\n",
        "        # equation (3)\n",
        "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "        # equation (4)\n",
        "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
        "        return {'h': h}  # New embeddings of nodes h'\n",
        "\n",
        "  def a_edges(self, word_ids: list, old_to_new:dict):  # Add edges for graph.\n",
        "        edges = []\n",
        "        old_edge_id = []\n",
        "        for index, src_word_old in enumerate(word_ids):\n",
        "            src = old_to_new[src_word_old]\n",
        "            for i in range(max(0, index - self.word_ngram), min(index + self.word_ngram + 1, len(word_ids))):\n",
        "                dst_word_old = word_ids[i]\n",
        "                dst = old_to_new[dst_word_old]\n",
        "\n",
        "                # - first connect the new sub_graph\n",
        "                edges.append([src, dst])\n",
        "                # - then get the hidden from parent_graph\n",
        "                old_edge_id.append(self.edges_matrix[src_word_old, dst_word_old])\n",
        "\n",
        "            #self circle\n",
        "            edges.append([src, src])  # All edges.\n",
        "            old_edge_id.append(self.edges_matrix[src_word_old, src_word_old])\n",
        "\n",
        "        return edges, old_edge_id\n",
        "\n",
        "  def build_graph(self, word_ids):  # Build GAT graph\n",
        "    if len(word_ids) > self.max_length:\n",
        "        word_ids = word_ids[ : self.max_length]\n",
        "    \n",
        "    local_vocab = set(word_ids)\n",
        "    old_to_new = dict(zip(local_vocab, range(len(local_vocab))))\n",
        "    local_vocab = torch.tensor(list(local_vocab)).to(self.device)  # Graph on device, so need local_vocab on same device.\n",
        "\n",
        "    graph = dgl.DGLGraph()\n",
        "    graph = graph.to(self.device)\n",
        "\n",
        "    graph.add_nodes(len(local_vocab)) # Add nodes for graph.\n",
        "    z = self.fc(self.node_hidden(local_vocab))  # equation (1)\n",
        "    graph.ndata['z'] = z  # Add node embeddings for graph\n",
        "\n",
        "    edges, edges_id = self.a_edges(word_ids, old_to_new)\n",
        "    srcs, dsts = zip(*edges)  # get source and destination nodes id (local id).\n",
        "    \n",
        "    graph.add_edges(srcs, dsts)  # Add edges for graph.\n",
        "    graph.apply_edges(self.edge_attention)  # Update the features of the specified edges, graph.edata['e] = F.leaky_relu(a)\n",
        "\n",
        "    return graph\n",
        "\n",
        "  def forward(self, word_id):  # word_id: 3d\n",
        "      graphs = []  # word-level graphs\n",
        "      graphs_id = {}  # Dictionary data formar, such as {0:[0,3], 1:[3:6], ...}\n",
        "      g_id = []  # [0,0,0,1,1,1,...]\n",
        "\n",
        "      count=0\n",
        "      for i in range(len(word_id)):\n",
        "        t_index = [count]\n",
        "        if len(word_id[i])>0:\n",
        "          for j in range(len(word_id[i])):\n",
        "            g_id.append(i)\n",
        "            graphs.append(self.build_graph(word_id[i][j]))\n",
        "            count+=1\n",
        "          t_index.append(count)  # [start, end]\n",
        "          graphs_id[i]=t_index\n",
        "        else:\n",
        "          print('Error! Sentence length must be greater than 0.')\n",
        "          break\n",
        "\n",
        "      batch_graph = dgl.batch(graphs)  # batching update\n",
        "\n",
        "      batch_graph.update_all(self.message_func, self.reduce_func)\n",
        "\n",
        "      out_w = dgl.sum_nodes(batch_graph, feat='h')  # (num_sen, hidden_size_node)\n",
        "      \n",
        "      g_id = torch.tensor(g_id, device=self.device) \n",
        "      output_sum = scatter_add(out_w, g_id, dim=0)  # (batch_size, hidden_size_node)\n",
        "\n",
        "      return out_w, graphs_id, output_sum\n",
        "\n",
        "\n",
        "class SentenceGATLayer(torch.nn.Module):  # GAT for sentence-level\n",
        "  def __init__(self, in_dim, out_dim, hidden_size_node, vocab, n_gram, edges_matrix, edges_num):\n",
        "    super(SentenceGATLayer, self).__init__()\n",
        "\n",
        "    self.fc = torch.nn.Linear(in_dim, out_dim, bias=False)  # equation (1)\n",
        "    self.attn_fc = torch.nn.Linear(2 * out_dim, 1, bias=False)  # equation (2)\n",
        "    self.reset_parameters()\n",
        "\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.vocab = vocab  # Prepare for other methods in this class.\n",
        "    self.dim = hidden_size_node\n",
        "\n",
        "    self.sen_ngram = 2  # For R8 and R52, ngram=1, because ithey only have 1 sentence, and for 20NG and Ohsumed MR, ngram is 2 or 3...\n",
        "\n",
        "    self.word_gat = WordGATLayer(self.dim, self.dim, self.dim, vocab, n_gram, edges_matrix, edges_num)  # GCN for wprd-level\n",
        "\n",
        "  def reset_parameters(self):\n",
        "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
        "        gain = torch.nn.init.calculate_gain('relu')\n",
        "        torch.nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        torch.nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
        "  \n",
        "  def edge_attention(self, edges):  # func = lambda edges : F.leaky_relu(self.attn_fc(z2))\n",
        "        # edge UDF for equation (2)\n",
        "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
        "        a = self.attn_fc(z2)  # (2*out_dim, 1)\n",
        "        return {'e': F.leaky_relu(a)}\n",
        "\n",
        "  def message_func(self, edges):  # Passing two tensors: the transformed z embedding of source node and un-normalized attention score e\n",
        "        # message UDF for equation (3) & (4)\n",
        "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
        "\n",
        "  def reduce_func(self, nodes):  # Performing two tasks: 1 normalize the attention score e, 2 Aggregate neighbor embeddings weighted by the attention scores\n",
        "        # reduce UDF for equation (3) & (4)\n",
        "        # equation (3)\n",
        "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "        # equation (4)\n",
        "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
        "        return {'h': h}  # New embeddings of nodes h'\n",
        "  \n",
        "  def a_edges(self, num_sen):  # Add edges for graph.\n",
        "        edges = []\n",
        "        for i in range(num_sen):\n",
        "            src = i\n",
        "            for j in range(num_sen):\n",
        "              if abs(i-j) <=self.sen_ngram:\n",
        "                dst = j\n",
        "                # - first connect the new sub_graph\n",
        "                edges.append([src, dst])\n",
        "\n",
        "        return torch.tensor(edges).to(self.device)  # If using gpu, the edges tensor should be in the same divice\n",
        "  \n",
        "  def build_graph(self, word_hidden):  # Build GAT graph\n",
        "    \n",
        "    n_sen = len(word_hidden)  # Number of sentence in each sample\n",
        "\n",
        "    graph = dgl.DGLGraph()\n",
        "    graph = graph.to(self.device)\n",
        "\n",
        "    graph.add_nodes(n_sen) # Add sentence-level nodes for graph.\n",
        "    z = self.fc(word_hidden)  # equation (1)\n",
        "    graph.ndata['z'] = z  # Add node embeddings for graph\n",
        "\n",
        "    edges = self.a_edges(n_sen)\n",
        "    srcs, dsts = zip(*edges)  # get source and destination nodes id (local id).\n",
        "    \n",
        "    graph.add_edges(srcs, dsts)  # Add edges for graph.\n",
        "    graph.apply_edges(self.edge_attention)  # Update the features of the specified edges, graph.edata['e] = F.leaky_relu(a)\n",
        "\n",
        "    return graph\n",
        "\n",
        "  def forward(self, word_id):\n",
        "    out_sen, g_id, out_sum = self.word_gat(word_id)  # Get w & s\n",
        "\n",
        "    s_id = list(g_id.keys())  # Sample id: (batch_size)\n",
        "\n",
        "    graphs = [self.build_graph(out_sen[g_id[i][0]: g_id[i][1]]) for i in s_id ]  # Doc-level graphs.\n",
        "\n",
        "    batch_graph = dgl.batch(graphs)  # batching update\n",
        "\n",
        "    batch_graph.update_all(self.message_func, self.reduce_func)  # Using GAT updates graphs\n",
        "\n",
        "    out_sen = dgl.sum_nodes(batch_graph, feat='h')  # (batch_zise, hidden_size_node)\n",
        "    \n",
        "    return out_sen, out_sum  # Sen-level and word-level\n",
        "\n",
        "\n",
        "class MultiHeadGATLayer(torch.nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, num_heads, hidden_size_node, vocab, n_gram, edges_matrix, edges_num, merge='mean'):\n",
        "    super(MultiHeadGATLayer, self).__init__()\n",
        "    self.heads = torch.nn.ModuleList()\n",
        "    for i in range(num_heads):\n",
        "      self.heads.append(SentenceGATLayer(in_dim, out_dim, hidden_size_node, vocab, n_gram, edges_matrix, edges_num))\n",
        "    self.merge = merge\n",
        "  \n",
        "  def forward(self, doc_id):\n",
        "    head_outs = [attn_head(doc_id) for attn_head in self.heads]\n",
        "    if self.merge == 'cat':  # Hidden layer\n",
        "      return torch.cat(head_outs, dim=1)  # (batch_size, hidden_size_node*num_heads)\n",
        "    else:\n",
        "      return torch.mean(torch.stack(head_outs, dim=1), dim=1)  #  (batch_size, hidden_size_node)\n",
        "\n",
        "\n",
        "class DocLevelGCN(torch.nn.Module): # The same as Huang lianzhe, 'Model' parts of paper code, see References.\n",
        "    def __init__(self, hidden_size_node, vocab, n_gram, edges_matrix, edges_num, max_length=400):\n",
        "      super(DocLevelGCN, self).__init__()\n",
        "\n",
        "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "      self.vocab = vocab  # Prepare for other methods in this class.\n",
        "\n",
        "      self.node_hidden = torch.nn.Embedding(len(vocab), hidden_size_node)  # Word-level node hidden (embedding).\n",
        "      self.node_hidden.weight.data.copy_(torch.tensor(self.load_word2vec('/content/drive/MyDrive/Colab_Notebooks/DATA/glove.6B/glove.6B.100d.w2vformat.txt')))\n",
        "      self.node_hidden.weight.requires_grad = True\n",
        "\n",
        "      self.node_eta = torch.nn.Embedding.from_pretrained(torch.rand(len(vocab), 1), freeze=False)  # Word-level node save itself feature according to eta percent.\n",
        "\n",
        "      self.edges_matrix = edges_matrix  # All global doc-level edges are stored in the matrix.\n",
        "      self.edges_num = edges_num  # The number of doc-level edges.\n",
        "      self.node_edge_w = torch.nn.Embedding.from_pretrained(torch.ones(edges_num, 1), freeze=False)  # Setting trainable parameters for word-level edges.\n",
        "\n",
        "      self.len_vocab = len(vocab)\n",
        "      self.d = dict(zip(vocab, range(len(vocab))))  # Set new index for vocab.\n",
        "\n",
        "      self.ngram = n_gram\n",
        "      self.max_length = max_length  # Maximum number of doc-level nodes.\n",
        "\n",
        "    def doc_level_edges(self, word_ids: list, old_to_new:dict):  # Add doc-level edges for word-level graph.\n",
        "        edges = []\n",
        "        old_edge_id = []\n",
        "        for index, src_word_old in enumerate(word_ids):\n",
        "            src = old_to_new[src_word_old]\n",
        "            for i in range(max(0, index - self.ngram), min(index + self.ngram + 1, len(word_ids))):\n",
        "                dst_word_old = word_ids[i]\n",
        "                dst = old_to_new[dst_word_old]\n",
        "\n",
        "                # - first connect the new sub_graph\n",
        "                edges.append([src, dst])\n",
        "                # - then get the hidden from parent_graph\n",
        "                old_edge_id.append(self.edges_matrix[src_word_old, dst_word_old])\n",
        "\n",
        "            # self circle\n",
        "            #edges.append([src, src])  # All edges.\n",
        "            #old_edge_id.append(self.edges_matrix[src_word_old, src_word_old])\n",
        "\n",
        "        return edges, old_edge_id\n",
        "    \n",
        "    def load_word2vec(self, word2vec_file):  # Loading embeddings representation from w2v.\n",
        "      model = word2vec.load(word2vec_file)\n",
        "\n",
        "      embedding_matrix = []\n",
        "\n",
        "      for word in self.vocab:\n",
        "          try:\n",
        "              embedding_matrix.append(model[word])\n",
        "          except KeyError:\n",
        "              # print(f'Line 269. The word not in vocab:{word}')\n",
        "              #unk_word=np.zeros(300)\n",
        "              #print(f\"line 271, model['the].shape: {len(model['the'])}\")  # [,300]\n",
        "              embedding_matrix.append(np.zeros(len(model['the'])))  # other unknow words use zeros vector\n",
        "\n",
        "      embedding_matrix = np.array(embedding_matrix)\n",
        "\n",
        "      return embedding_matrix\n",
        "\n",
        "    def doc_level_graph(self, word_ids):  # Build doc level graph.\n",
        "        if len(word_ids) > self.max_length:\n",
        "            word_ids = word_ids[ : self.max_length]\n",
        "        \n",
        "        local_vocab = set(word_ids)\n",
        "        old_to_new = dict(zip(local_vocab, range(len(local_vocab))))\n",
        "\n",
        "        local_vocab = torch.tensor(list(local_vocab)).to(self.device)  #Graph on device, so need local_vocab on same device.\n",
        "\n",
        "        graph = dgl.DGLGraph()\n",
        "        graph = graph.to(self.device)\n",
        "\n",
        "        graph.add_nodes(len(local_vocab)) # Add nodes for graph.\n",
        "        graph.ndata['h'] = self.node_hidden(local_vocab)  # Add node features for graph.\n",
        "        \n",
        "        eta = torch.sigmoid(self.node_eta(local_vocab))  # Limit to [0-1].\n",
        "        graph.ndata['eta'] = eta  # Add node eta for graph\n",
        "\n",
        "        edges, edges_id = self.doc_level_edges(word_ids, old_to_new)\n",
        "        edges_id = torch.LongTensor(edges_id).to(self.device)  # The edges_id is a 1-d list.\n",
        "        srcs, dsts = zip(*edges)  # get source and destination nodes id (local id).\n",
        "        \n",
        "        graph.add_edges(srcs, dsts)  # Add edges for graph.\n",
        "        graph.edata['w'] = self.node_edge_w(edges_id) # Add weight for edges.\n",
        "\n",
        "        return graph\n",
        "\n",
        "    def doc_level_edges(self, word_ids: list, old_to_new:dict):  # Add doc-level edges for word-level graph.\n",
        "        edges = []\n",
        "        old_edge_id = []\n",
        "        for index, src_word_old in enumerate(word_ids):\n",
        "            src = old_to_new[src_word_old]\n",
        "            for i in range(max(0, index - self.ngram), min(index + self.ngram + 1, len(word_ids))):\n",
        "                dst_word_old = word_ids[i]\n",
        "                dst = old_to_new[dst_word_old]\n",
        "\n",
        "                # - first connect the new sub_graph\n",
        "                edges.append([src, dst])\n",
        "                # - then get the hidden from parent_graph\n",
        "                old_edge_id.append(self.edges_matrix[src_word_old, dst_word_old])\n",
        "\n",
        "            # self circle\n",
        "            edges.append([src, src])  # All edges.\n",
        "            old_edge_id.append(self.edges_matrix[src_word_old, src_word_old])\n",
        "\n",
        "        return edges, old_edge_id\n",
        "\n",
        "    def forward(self, doc_id):  # Update doc-level graph\n",
        "      graphs = [self.doc_level_graph(doc) for doc in doc_id ]  # Doc-level graphs.\n",
        "\n",
        "      batch_graph = dgl.batch(graphs)  # batching update\n",
        "      batch_graph.update_all(\n",
        "          message_func = dgl.function.src_mul_edge('h', 'w', 'weighted_message'),\n",
        "          reduce_func = dgl.function.max('weighted_message','M')\n",
        "      )\n",
        "      batch_graph.ndata['h'] = batch_graph.ndata['eta']*batch_graph.ndata['h'] + batch_graph.ndata['M'] * (1 - batch_graph.ndata['eta'])  # Update word-level node feature.\n",
        "      out_doc = dgl.sum_nodes(batch_graph, feat='h')  # Convert doc-level graph to one vector (batch_size, 300)\n",
        "\n",
        "      return out_doc  # Return doc-level graph representation.\n",
        "\n",
        "\n",
        "class DocLevelGAT(torch.nn.Module):  # GAT for doc-level graph\n",
        "  def __init__(self, in_dim, out_dim, hidden_size_node, vocab, n_gram, edges_matrix, max_length=400):\n",
        "    super(DocLevelGAT, self).__init__()\n",
        "\n",
        "    self.fc = torch.nn.Linear(in_dim, out_dim, bias=False)\n",
        "    self.attn_fc = torch.nn.Linear(out_dim*2, 1, bias=False)\n",
        "    self.reset_parameters()\n",
        "    \n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.vocab = vocab\n",
        "    self.word_ngram = n_gram\n",
        "    self.edges_matrix = edges_matrix\n",
        "    self.max_length = max_length\n",
        "\n",
        "    self.node_hidden = torch.nn.Embedding(len(vocab), hidden_size_node)\n",
        "    self.node_hidden.weight.data.copy_(torch.tensor(self.load_word2vec('/content/drive/MyDrive/Colab_Notebooks/DATA/glove.6B/glove.6B.100d.w2vformat.txt')))\n",
        "    self.node_hidden.weight.requires_grad = True\n",
        "  \n",
        "  def load_word2vec(self, w2v_file):\n",
        "    model = word2vec.load(w2v_file)\n",
        "    embedding_matrix = []\n",
        "    for word in self.vocab:\n",
        "      try:\n",
        "        embedding_matrix.append(model[word])\n",
        "      except KeyError:\n",
        "        embedding_matrix.append(np.zeros(len(model['the'])))\n",
        "    embedding_matrix = np.array(embedding_matrix)\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    gain = torch.nn.init.calculate_gain('relu')\n",
        "    torch.nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "    torch.nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
        "\n",
        "  def edge_attention(self, edges):\n",
        "    # edge UDF for equation (2)\n",
        "    z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
        "    a = self.attn_fc(z2)\n",
        "    return {'e':F.leaky_relu(a)}\n",
        "\n",
        "  def message_func(self, edges):\n",
        "    # message UDF for equation (3) & (4)\n",
        "    return {'z': edges.src['z'], 'e':edges.data['e']}\n",
        "\n",
        "  def reduce_func(self, nodes):\n",
        "    alpha = F.softmax(nodes.mailbox['e'], dim=1)  # Normalizing the attention score e\n",
        "    h =torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
        "    return {'h': h}\n",
        "\n",
        "  def a_edges(self, word_ids: list, old_to_new:dict):  # Add edges for graph\n",
        "    edges = []\n",
        "    old_edge_id = []\n",
        "    for index, src_word_old in enumerate(word_ids):\n",
        "      src = old_to_new[src_word_old]\n",
        "      for i in range(max(0, index - self.word_ngram), min(index + self.word_ngram + 1, len(word_ids))):\n",
        "        dst_word_old = word_ids[i]\n",
        "        dst = old_to_new[dst_word_old]\n",
        "        # - first connect the new sub_graph\n",
        "        edges.append([src, dst])\n",
        "        # - then get the hidden from parent_graph\n",
        "        old_edge_id.append(self.edges_matrix[src_word_old, dst_word_old])    \n",
        "      # self circle\n",
        "      edges.append([src, src])  # All edges.\n",
        "      old_edge_id.append(self.edges_matrix[src_word_old, src_word_old])  \n",
        "\n",
        "    return edges, old_edge_id\n",
        "\n",
        "  def build_graph(self, word_ids):  # Build GAT graph\n",
        "    if len(word_ids) > self.max_length:\n",
        "        word_ids = word_ids[ : self.max_length]\n",
        "    \n",
        "    local_vocab = set(word_ids)\n",
        "    old_to_new = dict(zip(local_vocab, range(len(local_vocab))))\n",
        "    local_vocab = torch.tensor(list(local_vocab)).to(self.device)  # Graph on device, so need local_vocab on same device.\n",
        "\n",
        "    graph = dgl.DGLGraph()\n",
        "    graph = graph.to(self.device)\n",
        "\n",
        "    graph.add_nodes(len(local_vocab)) # Add nodes for graph.\n",
        "    z = self.fc(self.node_hidden(local_vocab))  # equation (1)\n",
        "    graph.ndata['z'] = z  # Add node embeddings for graph\n",
        "\n",
        "    edges, edges_id = self.a_edges(word_ids, old_to_new)\n",
        "    srcs, dsts = zip(*edges)  # get source and destination nodes id (local id).\n",
        "    \n",
        "    graph.add_edges(srcs, dsts)  # Add edges for graph.\n",
        "    graph.apply_edges(self.edge_attention)  # Update the features of the specified edges, graph.edata['e] = F.leaky_relu(a)\n",
        "\n",
        "    return graph\n",
        "\n",
        "  def forward(self, doc_id):  # word_id: 2d, GAT for doc-level\n",
        "      graphs = [self.build_graph(doc) for doc in doc_id ]  # word-level graphs\n",
        "\n",
        "      batch_graph = dgl.batch(graphs)  # batching update\n",
        "\n",
        "      batch_graph.update_all(self.message_func, self.reduce_func)\n",
        "\n",
        "      out_w = dgl.sum_nodes(batch_graph, feat='h')  # (batch_zise, hidden_size_node)\n",
        "\n",
        "      return out_w\n",
        "\n",
        "\n",
        "class HieGNN(torch.nn.Module):  # Paper model\n",
        "  def __init__(self, class_num, hidden_size_node,vocab, n_gram, drop_out, edges_matrix, edges_num):\n",
        "    super(HieGNN, self).__init__()\n",
        "\n",
        "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    self.dim = hidden_size_node\n",
        "\n",
        "    self.alpha = torch.nn.Parameter(torch.sigmoid(torch.randn(1)))\n",
        "    self.beta = torch.nn.Parameter(torch.sigmoid(torch.randn(1)))\n",
        "    self.gamma = torch.nn.Parameter(torch.sigmoid(torch.randn(1)))\n",
        "    \n",
        "\n",
        "    #self.docgcn = DocLevelGCN(self.dim, vocab, n_gram, edges_matrix, edges_num)\n",
        "    self.sengat = SentenceGATLayer(self.dim, self.dim, self.dim, vocab, n_gram, edges_matrix, edges_num)  # GAT for sen-level --> word-level\n",
        "    self.doclevelgat = DocLevelGAT(self.dim, self.dim, self.dim, vocab, n_gram, edges_matrix)\n",
        "\n",
        "\n",
        "    self.dropout = torch.nn.Dropout(p=drop_out)  # Output layer.\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    self.Linear = torch.nn.Linear(hidden_size_node, class_num)  # y = Wx + b\n",
        "  \n",
        "  def forward(self, content, content_doc):  # conten:3-d, content_doc: 2d\n",
        "    \n",
        "    s, w = self.sengat(content)  # GAT for sen-level graph\n",
        "    d = self.doclevelgat(content_doc)  # GAT for doc-level\n",
        "    #d = self.docgcn(content_doc)\n",
        "    \n",
        "    #s = s.unsqueeze(dim=1)\n",
        "    #d = d.unsqueeze(dim=1)\n",
        "    #w = w.unsqueeze(dim=1)\n",
        "    #t_res, pos = torch.max(torch.cat([s, d], dim=1), dim=1)  # Temp result and max value position\n",
        "    \n",
        "    res = self.gamma * d + self.beta * s + self.alpha * w  # (batch_size, hidden_size_node)\n",
        "\n",
        "    drop = self.dropout(res)\n",
        "    act = self.activation(drop)\n",
        "    l = self.Linear(act)  # (batch_size, class_num)\n",
        "\n",
        "    return l\n",
        "\n",
        "\n",
        "def cal_PMI(dataset: str, window_size=20):  # The point wise mutual information.\n",
        "    helper = DataHelper(dataset=dataset, mode=\"train\")\n",
        "    content, _ = helper.get_content()  # function returns (content, label)\n",
        "    pair_count_matrix = np.zeros((len(helper.vocab), len(helper.vocab)), dtype=int)  # p(i,j)\n",
        "    word_count =np.zeros(len(helper.vocab), dtype=int)  # 公式中的#W(i)\n",
        "    print(f'Vocab of dataset: {len(helper.vocab)}')\n",
        "    \n",
        "    for sentence in content:  # one  sentence per document\n",
        "        sentence = sentence.split()  # get the words in a sentence\n",
        "        for i, word in enumerate(sentence):\n",
        "            try:\n",
        "                word_count[helper.d[word]] += 1\n",
        "            except KeyError:\n",
        "                continue\n",
        "            start_index = max(0, i - window_size)\n",
        "            end_index = min(len(sentence), i + window_size)\n",
        "            for j in range(start_index, end_index):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                else:\n",
        "                    target_word = sentence[j]\n",
        "                    try:\n",
        "                        pair_count_matrix[helper.d[word], helper.d[target_word]] += 1  # p(i,j)\n",
        "                    except KeyError:\n",
        "                        continue\n",
        "        \n",
        "    total_count = np.sum(word_count)\n",
        "    # print(f'line 408:total_count: {total_count}')\n",
        "    word_count = word_count / total_count\n",
        "    # print(f'word_count: {word_count}')\n",
        "    pair_count_matrix = pair_count_matrix / total_count\n",
        "    \n",
        "    pmi_matrix = np.zeros((len(helper.vocab), len(helper.vocab)), dtype=float)\n",
        "    for i in range(len(helper.vocab)):\n",
        "        for j in range(len(helper.vocab)):\n",
        "            pmi_matrix[i, j] = np.log(\n",
        "                pair_count_matrix[i, j] / (word_count[i] * word_count[j])  # The dividend is very small, divide by 0, get the NaN value\n",
        "            )\n",
        "    \n",
        "    pmi_matrix = np.nan_to_num(pmi_matrix)  # replace NaN with zero and infinity with large finite number\n",
        "    \n",
        "    pmi_matrix = np.maximum(pmi_matrix, 0.0)  # Less than 0, set 0.\n",
        "\n",
        "  \n",
        "    count = 1  # The number of edges.\n",
        "    edges_mappings = np.zeros((len(helper.vocab), len(helper.vocab)), dtype=int)\n",
        "    for i in range(len(helper.vocab)):\n",
        "        for j in range(len(helper.vocab)):\n",
        "            if pmi_matrix[i, j] > 0:  # The value of PMI, more than 0 is positive correlation.\n",
        "                edges_mappings[i, j] = count\n",
        "                count += 1\n",
        "   \n",
        "    return edges_mappings, count\n",
        "\n",
        "NUM_ITER_EVAL = 100\n",
        "EARLY_STOP_EPOCH = 10\n",
        "\n",
        "def train(ngram, model_name, drop_out, dataset, total_epoch=1):  # Training function.\n",
        "    data_helper = DataHelper(dataset, mode='train')\n",
        "\n",
        "    tmp_data = dataset.split('/')\n",
        "    dataset_name = tmp_data[len(tmp_data)-1]\n",
        "    if os.path.exists(os.path.join('/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/model', model_name+'.pkl')) and model_name != 'temp_model':\n",
        "        print('load model from file.')\n",
        "        model = torch.load(os.path.join('/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/model', model_name+'.pkl'))\n",
        "    else:\n",
        "        print('Build new model.')\n",
        "        if model_name == 'temp_model':\n",
        "            model_name = f'temp_model_{dataset_name}'\n",
        "        edges_mappings, count = cal_PMI(dataset=dataset)\n",
        "        \n",
        "        model = HieGNN(class_num=len(data_helper.labels_str), hidden_size_node=100,\n",
        "                      vocab=data_helper.vocab, n_gram=ngram, drop_out=drop_out, edges_matrix=edges_mappings, edges_num=count)\n",
        "\n",
        "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "    loss_func = torch.nn.CrossEntropyLoss()\n",
        "    optim = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
        "\n",
        "    iter = 0\n",
        "\n",
        "    best_acc = 0.0\n",
        "    last_best_epoch = 0\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "    for content, content_doc, label, epoch in data_helper.batch_iter(batch_size=64, num_epoch=total_epoch):\n",
        "        improved = ''\n",
        "        model.train()\n",
        "\n",
        "        logits = model(content, content_doc)\n",
        "        loss = loss_func(logits, label)\n",
        "\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "        correct = torch.sum(pred == label)\n",
        "\n",
        "        total_correct += correct\n",
        "        total += len(label)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        iter += 1\n",
        "        if iter % NUM_ITER_EVAL == 0:\n",
        "\n",
        "            val_acc = dev(model, dataset=dataset)\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                last_best_epoch = epoch\n",
        "                improved = '*'\n",
        "\n",
        "                torch.save(model, f'/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/model/{model_name}.pkl')\n",
        "\n",
        "            if epoch - last_best_epoch >= EARLY_STOP_EPOCH:\n",
        "                print('Early stopping...')\n",
        "                return model_name\n",
        "            print(f'Epoch:{epoch}, iter:{iter}, train loss:{total_loss/ NUM_ITER_EVAL :.4f}, train acc: {float(total_correct) / float(total) :.4f}, val acc: {val_acc:.4f},{improved}')\n",
        "\n",
        "            total_loss = 0.0\n",
        "            total_correct = 0\n",
        "            total = 0\n",
        "\n",
        "    return model_name\n",
        "\n",
        "def dev(model, dataset):\n",
        "  data_helper = DataHelper(dataset, mode='dev')\n",
        "\n",
        "  total_pred =0\n",
        "  correct = 0\n",
        "\n",
        "  for content, content_doc, label, _ in data_helper.batch_iter(batch_size=64, num_epoch=1):\n",
        "    model.eval()\n",
        "\n",
        "    logits = model(content, content_doc)\n",
        "    pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "    correct_pred = torch.sum(pred==label)\n",
        "    correct += correct_pred\n",
        "    total_pred += len(content)\n",
        "\n",
        "  total_pred = float(total_pred)\n",
        "  correct = correct.float()\n",
        "\n",
        "  return torch.div(correct, total_pred)\n",
        "\n",
        "def test(model, dataset):\n",
        "  model = torch.load(os.path.join('/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/model', model+'.pkl'))\n",
        "  data_helper = DataHelper(dataset, mode='test')\n",
        "\n",
        "  total_pred = 0\n",
        "  correct = 0\n",
        "  iter = 0\n",
        "  for content, content_doc, label, _ in data_helper.batch_iter(batch_size=64, num_epoch=1):\n",
        "    iter += 1\n",
        "    model.eval()\n",
        "\n",
        "    logits = model(content, content_doc)\n",
        "    pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "    correct_pred = torch.sum(pred==label)\n",
        "    correct += correct_pred\n",
        "    total_pred += len(content)\n",
        "  \n",
        "  total_pred = float(total_pred)\n",
        "  correct = correct.float()\n",
        "\n",
        "  return torch.div(correct, total_pred)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--ngram', required=False, type=int, default=3, help='word level and doc level n-gram')\n",
        "parser.add_argument('--name', required=False, type=str, default='temp_model', help='model name')\n",
        "parser.add_argument('--dropout', required=False, type=float, default=0.5, help='drop out rate')\n",
        "parser.add_argument('--dataset', required=True, type=str, help='dataset')\n",
        "parser.add_argument('--rand', required=False, type=int, default=42, help='rand seed')\n",
        "parser.add_argument('--epoch', required=False, type=int, default=50, help='training epoch')\n",
        "\n",
        "args = parser.parse_args()\n",
        " \n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  # Print running machine\n",
        "if torch.cuda.is_available():\n",
        "  print(f'device: {device}')\n",
        "  print(f'name: {torch.cuda.get_device_name(0)}')\n",
        "  print(f'memory: {torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
        "  print(f'*'*50)\n",
        "\n",
        "SEED = args.rand\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "model = train(args.ngram, args.name, args.dropout, dataset=args.dataset, total_epoch=args.epoch)\n",
        "print(f'test acc: {test(model, args.dataset).cpu().numpy():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92mMXI4DXQ3d"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZle4y75XM8U",
        "outputId": "edb0b77b-91f0-44ff-8bfe-397d07147664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build new model.\n",
            "Vocab of dataset: 7748\n",
            "parsing.py:827: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  pair_count_matrix[i, j] / (word_count[i] * word_count[j])  # The dividend is very small, divide by 0, get the NaN value\n",
            "parsing.py:827: RuntimeWarning: divide by zero encountered in log\n",
            "  pair_count_matrix[i, j] / (word_count[i] * word_count[j])  # The dividend is very small, divide by 0, get the NaN value\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
            "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "Epoch:2, iter:100, train loss:2.7733, train acc: 0.2848, val acc: 0.4688,*\n",
            "Epoch:4, iter:200, train loss:1.4126, train acc: 0.5933, val acc: 0.5813,*\n",
            "Epoch:6, iter:300, train loss:0.6553, train acc: 0.8100, val acc: 0.6125,*\n",
            "Epoch:8, iter:400, train loss:0.3004, train acc: 0.9166, val acc: 0.6500,*\n",
            "Epoch:10, iter:500, train loss:0.1397, train acc: 0.9644, val acc: 0.6500,\n",
            "Epoch:12, iter:600, train loss:0.0905, train acc: 0.9791, val acc: 0.6562,*\n",
            "Epoch:14, iter:700, train loss:0.0625, train acc: 0.9831, val acc: 0.6562,\n",
            "Epoch:17, iter:800, train loss:0.0427, train acc: 0.9897, val acc: 0.6656,*\n",
            "Epoch:19, iter:900, train loss:0.0333, train acc: 0.9928, val acc: 0.6469,\n",
            "Epoch:21, iter:1000, train loss:0.0297, train acc: 0.9930, val acc: 0.6625,\n",
            "Epoch:23, iter:1100, train loss:0.0226, train acc: 0.9952, val acc: 0.6438,\n",
            "Epoch:25, iter:1200, train loss:0.0225, train acc: 0.9955, val acc: 0.6500,\n",
            "Early stopping...\n",
            "test acc: 0.6540\n"
          ]
        }
      ],
      "source": [
        "!python parsing.py --dataset='/content/drive/MyDrive/Colab_Notebooks/CODE/TextLevelGNN/data/oh' --ngram=2 --dropout=0.5 --epoch=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXAThBoprQzs"
      },
      "outputs": [],
      "source": [
        "ga = torch.nn.Parameter(torch.sigmoid(torch.randn(10)))\n",
        "\n",
        "a = torch.rand(6, 10)\n",
        "print(f'ga:\\n{ga}')\n",
        "\n",
        "b = ga * a\n",
        "\n",
        "print(f'b:\\n{b}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HieGNN(v5_4)",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1xIPw0O6rJsprAtX0xS1Fg6h0tGfg0gh1",
      "authorship_tag": "ABX9TyNYn3s/kHf9JMP/l9HasTob",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}