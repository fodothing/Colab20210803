{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SHua_Text_Level_GNN版本2",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "13-62KfdvQja_RjLrRut48pP-fplx52Uf",
      "authorship_tag": "ABX9TyOdC089grNc4LnzeckO3vfA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hshuai97/Colab20210803/blob/main/SHua_Text_Level_GNN%E7%89%88%E6%9C%AC2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "改进：增加词的2个语义和相应的权重，在训练82个周期的基础上，最好结果为0.9689"
      ],
      "metadata": {
        "id": "o4Utxkw1VGoI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfpDj43pU468",
        "outputId": "af958c7c-7bdc-4f97-ce59-1e924ad4750e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing parsing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile parsing.py\n",
        "# 标准库\n",
        "import pandas as pd\n",
        "from numpy import asarray\n",
        "import numpy as np\n",
        "from time import time\n",
        "import argparse\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# 操作文件数据\n",
        "import pickle\n",
        "\n",
        "\n",
        "#兼容原始GloVe和添加OPTED中'非常用词'后的Full_GloVe\n",
        "class Full_GloveTokenizer:\n",
        "    def __init__(self, filename, unk='<unk>', pad='<pad>'): \n",
        "        self.filename = filename  # Full_GloVe文件的路径\n",
        "        self.unk = unk  # 对于未知词汇的处理\n",
        "        self.pad = pad  # 按最大长度对齐每个文本\n",
        "        self.stoi = dict()\n",
        "        self.itos = dict()\n",
        "        self.embedding_matrix = list()  # [[embedded_word1], [embedded_word2],...]\n",
        "\n",
        "        Full_GloVe = pickle.load(open(self.filename, 'rb'))  # 读取Full_GloVe字典\n",
        "\n",
        "        for i, k in enumerate(Full_GloVe.keys()):\n",
        "          self.stoi[k] = i\n",
        "          self.itos[i] = k\n",
        "          self.embedding_matrix.append(Full_GloVe[k])\n",
        "\n",
        "\n",
        "        if self.unk is not None: # 未知词一律用一个随机embedded向量表示\n",
        "            i += 1\n",
        "            self.stoi[self.unk] = i\n",
        "            self.itos[i] = self.unk\n",
        "            self.embedding_matrix.append(asarray(np.random.rand(len(self.embedding_matrix[0])), dtype='float32'))\n",
        "        if self.pad is not None: # Add pad token into the tokenizer # padded词一律用全0的embedded向量表示\n",
        "            i += 1\n",
        "            self.stoi[self.pad] = i\n",
        "            self.itos[i] = self.pad\n",
        "            self.embedding_matrix.append(asarray(np.zeros(len(self.embedding_matrix[0])), dtype='float32'))\n",
        "        self.embedding_matrix = np.array(self.embedding_matrix).astype(np.float32) # Convert if from double to float for efficiency\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        if type(sentence) == str:\n",
        "            sentence = sentence.split(' ')\n",
        "        elif len(sentence): # Convertible to list\n",
        "            sentence = list(sentence)\n",
        "        else:\n",
        "            raise TypeError('sentence should be either a str or a list of str!')\n",
        "        encoded_sentence = []\n",
        "        for word in sentence:\n",
        "            encoded_sentence.append(self.stoi.get(word, self.stoi[self.unk]))\n",
        "        return encoded_sentence\n",
        "\n",
        "    def decode(self, encoded_sentence):\n",
        "        try:\n",
        "            encoded_sentence = list(encoded_sentence)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            raise TypeError('encoded_sentence should be either a str or a data type that is convertible to list type!')\n",
        "        sentence = []\n",
        "        for encoded_word in encoded_sentence:\n",
        "            sentence.append(self.itos[encoded_word])\n",
        "        return sentence\n",
        "\n",
        "    def embedding(self, encoded_sentence):\n",
        "        # Full_GloVe中有464885个嵌入词向量\n",
        "        return self.embedding_matrix[np.array(encoded_sentence)]  # 获得一个句子的每个词的embedded向量(max_sentence_length, d)\n",
        "\n",
        "\n",
        "class TextLevelGNNDataset(Dataset): # For instantiating train, validation and test dataset\n",
        "    def __init__(self, node_sets, neighbor_sets, public_edge_mask, labels):\n",
        "        super(TextLevelGNNDataset).__init__()\n",
        "        self.node_sets = node_sets  # 同batch_size张图的节点集合(batch_size, l)\n",
        "        self.neighbor_sets = neighbor_sets  # batch_size张图的节点邻居集合\n",
        "        self.public_edge_mask = public_edge_mask  # batch_size张图的布尔型张量边标记\n",
        "        self.labels = labels  # batch_size张图的标签\n",
        "\n",
        "    def __getitem__(self, i):  # 将batch_size张图中，每张图对应的四个属性封装为一个长张量单元，这样可以灵活调整batch_size的大小\n",
        "        return torch.LongTensor(self.node_sets[i]), \\\n",
        "               torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1), \\\n",
        "               self.public_edge_mask[torch.LongTensor(self.node_sets[i]).unsqueeze(-1).repeat(1, torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1).shape[-1]), torch.nn.utils.rnn.pad_sequence([torch.LongTensor(neighbor) for neighbor in self.neighbor_sets[i]], batch_first=True, padding_value=1)], \\\n",
        "               torch.FloatTensor(self.labels[i])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "class TextLevelGNNDatasetClass: # This class is used to achieve parameters sharing among datasets (train/validation/test)\n",
        "    def __init__(self, train_filename, test_filename, opted_filename, full_glove_pth, tokenizer, MAX_LENGTH=50, p=2, min_freq=2, train_validation_split=0.8):\n",
        "        self.train_filename = train_filename\n",
        "        self.test_filename = test_filename\n",
        "        # OPTED词典路径\n",
        "        self.opted_filename = opted_filename\n",
        "        self.full_glove_pth = full_glove_pth \n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.MAX_LENGTH = MAX_LENGTH\n",
        "        self.p = p\n",
        "        self.min_freq = min_freq\n",
        "        self.train_validation_split = train_validation_split\n",
        "\n",
        "        self.train_data = pd.read_csv(self.train_filename, sep='\\t', header=None)\n",
        "        self.test_data = pd.read_csv(self.test_filename, sep='\\t', header=None)\n",
        "\n",
        "        self.stoi = {'<unk>': 0, '<pad>': 1} # Re-index\n",
        "        self.itos = {0: '<unk>', 1: '<pad>'} # Re-index\n",
        "        self.vocab_count = len(self.stoi)\n",
        "        self.embedding_matrix = None\n",
        "\n",
        "        # 通过OPTED词典，返回训练集上每个词对应的1个语义，没有语义的用0向量表示,对于没有语义的词，后续可以考虑抓取维基百科的词条解释，类似于不断学习增加知识库的过程\n",
        "        self.meaning_matrix = list()  # [meaning1, meaning2, ...] \n",
        "        self.meaning1 = list()  # 节点语义1\n",
        "        self.meaning2 = list()  # 节点语义2\n",
        "\n",
        "        self.meaning_mask = list()  # 训练集上的词的语义标记矩阵，标记每个词是否存在相应的语义\n",
        "        self.mask1 = list()\n",
        "        self.mask2 = list()\n",
        "\n",
        "        self.label_dict = dict(zip(self.train_data[0].unique(), pd.get_dummies(self.train_data[0].unique()).values.tolist()))\n",
        "\n",
        "        # 将读取的数据转成数组\n",
        "        self.train_dataset, self.validation_dataset = random_split(self.train_data.to_numpy(), [int(len(self.train_data) * train_validation_split), len(self.train_data) - int(len(self.train_data) * train_validation_split)])\n",
        "        self.test_dataset = self.test_data.to_numpy()\n",
        "\n",
        "        self.build_vocab() # Based on train_dataset only. Updates self.stoi, self.itos, self.vocab_count and self.embedding_matrix\n",
        "\n",
        "        self.build_meaning_matrix()  # 获得训练集上每个词对应的2个meaning\n",
        "        \n",
        "        self.train_dataset, self.validation_dataset, self.test_dataset, self.edge_stat, self.public_edge_mask = self.prepare_dataset()\n",
        "\n",
        "    def build_vocab(self):\n",
        "        vocab_list = [sentence.split(' ') for _, sentence in self.train_dataset]\n",
        "        unique_vocab = []\n",
        "        for vocab in vocab_list:\n",
        "            unique_vocab.extend(vocab)\n",
        "        unique_vocab = list(set(unique_vocab))\n",
        "\n",
        "        for vocab in unique_vocab:\n",
        "            if vocab in self.tokenizer.stoi.keys():\n",
        "                self.stoi[vocab] = self.vocab_count\n",
        "                self.itos[self.vocab_count] = vocab\n",
        "                self.vocab_count += 1\n",
        "        self.embedding_matrix = self.tokenizer.embedding(self.tokenizer.encode(list(self.stoi.keys())))  # (|V|, d) 仅根据训练集构建的全局词嵌入向量\n",
        "       \n",
        "    def build_meaning_matrix(self):  # 构建训练集上每个节点对应的2个语义关系\n",
        "        OPTED = pickle.load(open(self.opted_filename, 'rb'))\n",
        "        Full_GloVe = pickle.load(open(self.full_glove_pth, 'rb'))\n",
        "\n",
        "        # meaning1\n",
        "        for vocab in self.stoi.keys():  # 遍历训练集字典下的每个词\n",
        "          m = asarray(np.zeros(300), dtype='float32')   # 初始化一个300维的meaning\n",
        "          j = 0\n",
        "          if vocab in OPTED.keys():  # 词在OPTED中, 对于'unk'、'pad'和其他不在OPTED中的词均用一个全0的300维向量语义表示\n",
        "            for s in OPTED[vocab][0].split():  # 取第一个meaning, 后续可考虑取多个meaning(2, 3, 4...)\n",
        "              if s in Full_GloVe.keys():\n",
        "                j += 1\n",
        "                m += Full_GloVe[s]\n",
        "            if j>0:\n",
        "              m = m/j\n",
        "            self.meaning1.append(m)  # 添加到meaning_matrix中为了和embedded_neighbor_node结构对齐（三维）张量\n",
        "            self.mask1.append(True)  # 有第一个语义\n",
        "          else: # 词不在OPTED中\n",
        "            self.meaning1.append(m)  # 没有解释的词，其meaning用0向量代替\n",
        "            self.mask1.append(False)  # 没有第一个语义\n",
        "        self.meaning1 = np.array(self.meaning1).astype(np.float32)\n",
        "        \n",
        "        # meaning2\n",
        "        for vocab in self.stoi.keys():\n",
        "          m = asarray(np.zeros(300), dtype='float32')\n",
        "          j = 0\n",
        "          if vocab in OPTED.keys():\n",
        "            if len(OPTED[vocab]) > 1:  # 如果词存在第二个语义\n",
        "              for s in OPTED[vocab][1].split():\n",
        "                if s in Full_GloVe.keys():\n",
        "                  j += 1\n",
        "                  m += Full_GloVe[s]\n",
        "              if j > 0:\n",
        "                m = m/j\n",
        "              self.meaning2.append(m)\n",
        "              self.mask2.append(True)  # 有第二个语义\n",
        "            else:  #没有第二个语义\n",
        "              self.meaning2.append(m)\n",
        "              self.mask2.append(False)  # 没有第二个语义\n",
        "          else:  # 词不在OPTED中\n",
        "            self.meaning2.append(m)\n",
        "            self.mask2.append(False)  # 没有第二个语义\n",
        "        \n",
        "        self.meaning2 = np.array(self.meaning2).astype(np.float32)\n",
        "        \n",
        "        # 将meaning1和meaning2合并到meaning_matrix中\n",
        "        for i in range(len(self.meaning1)):\n",
        "          self.meaning_matrix.append([self.meaning1[i], self.meaning2[i]])\n",
        "        self.meaning_matrix = np.array(self.meaning_matrix).astype(np.float32)\n",
        "        \n",
        "        # 将mask1和mask2合并到meaning_mask中\n",
        "        for i in range(len(self.mask1)):\n",
        "          self.meaning_mask.append([self.mask1[i], self.mask2[i]])\n",
        "        \n",
        "        # print(f'self.meaning_matrix:\\n{self.meaning_matrix[0]}')\n",
        "        # print(f'self.meaning_mask:\\n{self.meaning_mask[0]}')\n",
        "        #print(f'len self.meaning_matrix[0]:{len(self.meaning_matrix[0])}')\n",
        "        #print(f'len self.meaning_matrix[1]:{len(self.meaning_matrix[0][1])}')\n",
        "        \n",
        "          \n",
        "    # 本类中的主方法，负责返回结果\n",
        "    def prepare_dataset(self): # will also build self.edge_stat and self.public_edge_mask\n",
        "        # preparing self.train_dataset\n",
        "        node_sets = [[self.stoi.get(vocab, 0) for vocab in sentence.strip().split(' ')][:self.MAX_LENGTH] for _, sentence in self.train_dataset] # Only retrieve the first MAX_LENGTH words in each document\n",
        "        neighbor_sets = [create_neighbor_set(node_set, p=self.p) for node_set in node_sets]\n",
        "        labels = [self.label_dict[label] for label, _ in self.train_dataset]\n",
        "\n",
        "        # 调用类中的其他方法获取边的统计和标记信息\n",
        "        edge_stat, public_edge_mask = self.build_public_edge_mask(node_sets, neighbor_sets, min_freq=self.min_freq)\n",
        "        \n",
        "        train_dataset = TextLevelGNNDataset(node_sets, neighbor_sets, public_edge_mask, labels)  # 传给TextLevelGNNDataset类，返回每张图对应的torch.LongTensor(tensor0, tensor1, tensor2, tensor3)\n",
        "        \n",
        "        # preparing self.validation_dataset\n",
        "        node_sets = [[self.stoi.get(vocab, 0) for vocab in sentence.strip().split(' ')][:self.MAX_LENGTH] for _, sentence in self.validation_dataset] # Only retrieve the first MAX_LENGTH words in each document\n",
        "        neighbor_sets = [create_neighbor_set(node_set, p=self.p) for node_set in node_sets]\n",
        "        labels = [self.label_dict[label] for label, _ in self.validation_dataset]\n",
        "        validation_dataset = TextLevelGNNDataset(node_sets, neighbor_sets, public_edge_mask, labels)  # 传给TextLevelGNNDataset类，返回每张图对应的torch.LongTensor(tensor0, tensor1, tensor2, tensor3)\n",
        "\n",
        "        # preparing self.test_dataset\n",
        "        node_sets = [[self.stoi.get(vocab, 0) for vocab in sentence.strip().split(' ')][:self.MAX_LENGTH] for _, sentence in self.test_dataset] # Only retrieve the first MAX_LENGTH words in each document\n",
        "        neighbor_sets = [create_neighbor_set(node_set, p=self.p) for node_set in node_sets]\n",
        "        labels = [self.label_dict[label] for label, _ in self.test_dataset]\n",
        "        test_dataset = TextLevelGNNDataset(node_sets, neighbor_sets, public_edge_mask, labels)  # 传给TextLevelGNNDataset类，返回每张图对应的torch.LongTensor(tensor0, tensor1, tensor2, tensor3)\n",
        "\n",
        "        return train_dataset, validation_dataset, test_dataset, edge_stat, public_edge_mask  # 将训练集、验证集和测试集对应的每张图的信息封装后的长张量返回，以及边的统计和标记信息\n",
        "\n",
        "    def build_public_edge_mask(self, node_sets, neighbor_sets, min_freq=2):\n",
        "        edge_stat = torch.zeros(self.vocab_count, self.vocab_count)\n",
        "        for node_set, neighbor_set in zip(node_sets, neighbor_sets):\n",
        "            for neighbor in neighbor_set:\n",
        "                for to_node in neighbor:\n",
        "                    edge_stat[node_set, to_node] += 1\n",
        "        public_edge_mask = edge_stat < min_freq # mark True at uncommon edges\n",
        "        return edge_stat, public_edge_mask\n",
        "\n",
        "\n",
        "def create_neighbor_set(node_set, p=3):  # node_sets: 单个数据集(train/ val/ test)上的(batch_size，l), node_set = (max_length,)\n",
        "    if type(node_set[0]) != int:\n",
        "        raise ValueError('node_set should be a 1D list!')\n",
        "    if p < 0:\n",
        "        raise ValueError('p should be an integer >= 0!')\n",
        "    sequence_length = len(node_set)\n",
        "    neighbor_set = []\n",
        "    for i in range(sequence_length):\n",
        "        neighbor = []\n",
        "        for j in range(-p, p+1):\n",
        "            if 0 <= i + j < sequence_length:\n",
        "                neighbor.append(node_set[i+j])\n",
        "        neighbor_set.append(neighbor)\n",
        "    return neighbor_set\n",
        "\n",
        "\n",
        "\n",
        "def pad_custom_sequence(sequences):\n",
        "    '''\n",
        "    To pad different sequences into a padded tensor for training. The main purpose of this function is to separate different sequence, pad them in different ways and return padded sequences.\n",
        "    Input:\n",
        "        sequences <list>: A sequence with a length of 4, representing the node sets sequence in index 0, neighbor sets sequence in index 1, public edge mask sequence in index 2 and label sequence in index 3.\n",
        "                          And the length of each sequences are same as the batch size.\n",
        "                          sequences: [node_sets_sequence, neighbor_sets_sequence, public_edge_mask_sequence, label_sequence]\n",
        "    Return:\n",
        "        node_sets_sequence <torch.LongTensor>: The padded node sets sequence (works with batch_size >= 1).\n",
        "        neighbor_sets_sequence <torch.LongTensor>: The padded neighbor sets sequence (works with batch_size >= 1).\n",
        "        public_edge_mask_sequence <torch.BoolTensor>: The padded public edge mask sequence (works with batch_size >= 1).\n",
        "        label_sequence <torch.FloatTensor>: The padded label sequence (works with batch_size >= 1).\n",
        "    '''\n",
        "    node_sets_sequence = []  # list[tensor0, tensor1, tensor2, tensor3]\n",
        "    neighbor_sets_sequence = []\n",
        "    public_edge_mask_sequence = []\n",
        "    label_sequence = []\n",
        "    for node_sets, neighbor_sets, public_edge_mask, label in sequences:  # 依次从batch_size张图中取每张图的4个数据\n",
        "        node_sets_sequence.append(node_sets)\n",
        "        neighbor_sets_sequence.append(neighbor_sets)\n",
        "        public_edge_mask_sequence.append(public_edge_mask)\n",
        "        label_sequence.append(label) \n",
        "    node_sets_sequence = torch.nn.utils.rnn.pad_sequence(node_sets_sequence, batch_first=True, padding_value=1)  # 统一对齐batch_size下的句子节点为(max_sentence_length, d)（主要对句子长度小于max_sentence_length的句子补丁）\n",
        "    neighbor_sets_sequence, _ = padding_tensor(neighbor_sets_sequence)  # 统一对齐batch_size下的句子的每个节点对应的邻居节点，按max_sentence_length对齐\n",
        "    public_edge_mask_sequence, _ = padding_tensor(public_edge_mask_sequence)  # 统一对齐布尔型的标记张量\n",
        "    label_sequence = torch.nn.utils.rnn.pad_sequence(label_sequence, batch_first=True, padding_value=1)\n",
        "    return node_sets_sequence, neighbor_sets_sequence, public_edge_mask_sequence, label_sequence  # 返回对齐后的4个属性\n",
        "\n",
        "\n",
        "def padding_tensor(sequences, padding_idx=1):\n",
        "    '''\n",
        "    To pad tensor of different shape to be of the same shape, i.e. padding [tensor.rand(2, 3), tensor.rand(3, 5)] to a shape (2, 3, 5), where 0th dimension is batch_size, 1st and 2nd dimensions are padded.\n",
        "    Input:\n",
        "        sequences <list>: A list of tensors  # [tensor0, tensor1, tensor2, tensor3]\n",
        "        padding_idx <int>: The index that corresponds to the padding index\n",
        "    Return:\n",
        "        out_tensor <torch.tensor>: The padded tensor\n",
        "        mask <torch.tensor>: A boolean torch tensor where 1 (represents '<pad>') are marked as true\n",
        "    '''\n",
        "    num = len(sequences)\n",
        "    max_len_0 = max([s.shape[0] for s in sequences])\n",
        "    max_len_1 = max([s.shape[1] for s in sequences])\n",
        "    out_dims = (num, max_len_0, max_len_1)\n",
        "    out_tensor = sequences[0].data.new(*out_dims).fill_(padding_idx)\n",
        "    for i, tensor in enumerate(sequences):\n",
        "        len_0 = tensor.size(0)\n",
        "        len_1 = tensor.size(1)\n",
        "        out_tensor[i, :len_0, :len_1] = tensor\n",
        "    mask = out_tensor == padding_idx # Marking all places with padding_idx as mask\n",
        "    return out_tensor, mask\n",
        "\n",
        "\n",
        "class MessagePassing(nn.Module):  # 一张图上的处理\n",
        "    def __init__(self, vertice_count, input_size, out_size, dropout_rate=0, padding_idx=1):\n",
        "        super(MessagePassing, self).__init__()\n",
        "        self.vertice_count = vertice_count # |V|  r8语料库中有一万多个unique词\n",
        "        self.input_size = input_size # d  300维\n",
        "        self.out_size = out_size # c, r8, c=8\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.padding_idx = padding_idx  # 1\n",
        "        self.information_rate = nn.Parameter(torch.rand(self.vertice_count, 1)) # (|V|, 1), which means it is a column vector\n",
        "        self.linear = nn.Linear(self.input_size, self.out_size) # (d, c)  # 将300维映射到8维\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)  # 0\n",
        "\n",
        "    def forward(self, node_sets, embedded_node, meaning_node1, meaning_node2, edge_weight, meaning_weight, embedded_neighbor_node):\n",
        "        # node_sets: (batch_size, l)  # 对应论文的N_i\n",
        "        # embedded_node: (batch_size, l, d)  # 对应论文的r_n\n",
        "        # edge_weight: (batch_size, max_sentence_length, max_neighbor_count)  # 对应论文e_an\n",
        "        # embedded_neighbor_node: (batch_size, max_sentence_length, max_neighbor_count, d)  对应论文的N_n^p\n",
        "        \n",
        "\n",
        "        # 对词的邻居节点的词向量求权重后，加到M中\n",
        "        tmp_tensor = (edge_weight.view(-1, 1) * embedded_neighbor_node.view(-1, self.input_size)).view(embedded_neighbor_node.shape) # (batch_size, l, max_neighbor_count, d)\n",
        "        tmp_tensor = tmp_tensor.masked_fill(tmp_tensor == 0, -1e18) # (batch_size, max_sentence_length, max_neighbor_count, d), mask for M such that masked places are marked as -1e18 # M不接收来自pad节点的邻居信息，如果是来自pad节点的则将其设置为无穷小\n",
        "        tmp_tensor = self.dropout(tmp_tensor)\n",
        "\n",
        "        # 更新meaning_node1, meaning_node2的词嵌入向量  (batch_size, l, d)\n",
        "        # meaning_weight: (batch_size, l, 2)\n",
        "        meaning = torch.cat([meaning_node1.unsqueeze(dim=2), meaning_node2.unsqueeze(dim=2)], dim =2)  # (batch_size, l, 2, d)\n",
        "        weight = meaning_weight.unsqueeze(dim=3)  # (batch_size, l, 2, 1)\n",
        "        tmp = (weight.view(-1, 1) * meaning.view(-1, self.input_size)).view(meaning.shape)  # (batch_size, l, 2, d)\n",
        "\n",
        "        tmp_tensor = torch.cat([tmp_tensor, tmp], dim=2)  # 将tmp_tensor在dim=2上拉长 (batch_size, l, max_neighbor_count+2, d)\n",
        "\n",
        "        M = tmp_tensor.max(dim=2)[0] # (batch_size, max_sentence_length, d), which is same shape as embedded_node (batch_size, l, d)\n",
        "        \n",
        "        information_rate = self.information_rate[node_sets] # (batch_size, l, 1)\n",
        "        information_rate = information_rate.masked_fill((node_sets == self.padding_idx).unsqueeze(-1), 1) # (batch_size, l, 1), Fill the information rate of the padding index as 1, such that new e_n = (1-i_r) * M + i_r * e_n = (1-1) * 0 + 1 * e_n = e_n (no update)\n",
        "        embedded_node = (1 - information_rate) * M + information_rate * embedded_node # (batch_size, l, d) \n",
        "        sum_embedded_node = embedded_node.sum(dim=1) # (batch_size, d)\n",
        "        x = F.relu(self.linear(sum_embedded_node)) # (batch_size, c)\n",
        "#         x = self.dropout(x) # if putting dropout with p=0.5 here, it is equivalent to wiping 4 choices out of 8 choices on the question sheet, which does not make sense. If a dropout layer is placed at here, it works the best when p=0 (disabled), followed by p=0.05, ..., p=0.5 (worst and does not even converge).\n",
        "        y = F.softmax(x, dim=1) # (batch_size, c) along the c dimension\n",
        "        return y\n",
        "\n",
        "\n",
        "class TextLevelGNN(nn.Module):\n",
        "    def __init__(self, pretrained_embeddings, meaning_embeddings, meaning_mask, out_size=8, dropout_rate=0, padding_idx=1):\n",
        "        super(TextLevelGNN, self).__init__()\n",
        "        self.out_size = out_size # c\n",
        "        self.padding_idx = padding_idx\n",
        "        # nn开头的模型参数\n",
        "        self.weight_matrix = nn.Parameter(torch.randn(pretrained_embeddings.shape[0], pretrained_embeddings.shape[0])) # (|V|, |V|) 邻居节点边的权值\n",
        "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False, padding_idx=self.padding_idx) # (|V|, d), # freeze=False（不冻结）,预训练的词向量值可以调整\n",
        "\n",
        "        m = meaning_embeddings.chunk(chunks=2, dim=1)  # 将(|V|, 2, d)切分为两个张量(|V|, 1, d)并压缩为两个(|V|, d)张量，三维张量不支持embedding，所以需要压缩一下\n",
        "        self.meaning_embedding1 = nn.Embedding.from_pretrained(m[0].squeeze(dim=1), freeze=False, padding_idx=self.padding_idx)  # (|V|, d), # freeze=True（冻结）,预训练的向量值不可以调整\n",
        "        self.meaning_embedding2 = nn.Embedding.from_pretrained(m[1].squeeze(dim=1), freeze=False, padding_idx=self.padding_idx)  # (|V|, d) 词的第二个语义\n",
        "\n",
        "        self.message_passing = MessagePassing(vertice_count=pretrained_embeddings.shape[0], input_size=pretrained_embeddings.shape[1], out_size=self.out_size, dropout_rate=dropout_rate, padding_idx=self.padding_idx) # input_size: (d,); out_size: (c,)\n",
        "        self.public_edge_weight = nn.Parameter(torch.randn(1, 1)) # (1, 1)  二维张量\n",
        "\n",
        "        self.weight_matrix2 = nn.Parameter(torch.randn(pretrained_embeddings.shape[0], 2))  # (|V|, 2)，词的语义邻居边权值, 只要语义词向量不发生变化，仅权重变化，则不需要单独设置mask标记\n",
        "\n",
        "        # meaning_mask默认在cpu上，模型参数在gpu上（若使用GPU），则使用register_buffer(name, tensor, persistent=False)将其临时注册为模型的参数（本质上不属于模型的参数），最后不保存为模型的参数（persistent=False）, 且optim.step之后不更新buffer的变量，只更新模型的参数\n",
        "        self.register_buffer(name='rb_mask', tensor=meaning_mask, persistent=False)  # (|V|, 2)\n",
        "\n",
        "\n",
        "    def forward(self, node_sets, neighbor_sets, public_edge_mask):\n",
        "        # node_sets: (batch_size, l)\n",
        "        # neighbor_sets: (batch_size, max_sentence_length, max_neighbor_count)\n",
        "        # neighbor_sets_mask: (batch_size, max_sentence_length, max_neighbor_count) (no need)\n",
        "        # public_edge_mask: (batch_size, max_sentence_length, max_neighbor_count)\n",
        "\n",
        "        \n",
        "        embedded_node = self.embedding(node_sets) # (batch_size, l, d)  #将句子[12, 78,..., 987]转成各个词对应的Embedding向量\n",
        "\n",
        "        meaning_node1 = self.meaning_embedding1(node_sets)  # (batch_size, l, d)  # 一个词两个语义\n",
        "        meaning_node2 = self.meaning_embedding2(node_sets)  # (batch_size, l, d)\n",
        "\n",
        "        # 对应论文的e_an边权重\n",
        "        edge_weight = model.weight_matrix[node_sets.unsqueeze(2).repeat(1, 1, neighbor_sets.shape[-1]), neighbor_sets]  # (batch_size, max_sentence_length, max_neighbor_count), neighbor_sets.shape[-1]: eg p=2, this expression=5; p=3, this expression=7. This is to first make node_sets to have same shape with neighbor_sets, then just do 1 query instead of 32*100 queries to speed up performance\n",
        "        a = edge_weight * ~public_edge_mask # (batch_size, max_sentence_length, max_neighbor_count)\n",
        "        b = self.public_edge_weight.unsqueeze(2).expand(1, public_edge_mask.shape[-2], public_edge_mask.shape[-1]) * public_edge_mask # (batch_size, max_sentence_length, max_neighbor_count)\n",
        "        edge_weight = a + b # (batch_size, max_sentence_length, max_neighbor_count)\n",
        "        # 根据训练集上的embedding以及节点集合直接得出节点对应的邻居节点的词嵌入向量表示\n",
        "        embedded_neighbor_node = self.embedding(neighbor_sets) # (batch_size, max_sentece_length, max_neighbor_count, d)\n",
        "\n",
        "        # Apply mask to edge_weight, to mask and cut-off any relationships to the padding nodes  # 与'pad'节点相连的边不更新边权重e_an\n",
        "        edge_weight = edge_weight.masked_fill((node_sets.unsqueeze(2).repeat(1, 1, neighbor_sets.shape[-1]) == self.padding_idx) | (neighbor_sets == self.padding_idx), 0) # (batch_size, max_sentence_length, max_neighbor_count)\n",
        "\n",
        "        # 对应添加的词的语义权重\n",
        "        m_mask = self.rb_mask  # 读取mask  # (|V|, 2)\n",
        "        m_weight = model.weight_matrix2[node_sets]  # (batch_size, l, 2)\n",
        "        meaning_mask = m_mask[node_sets]  # (batch_size, l, 2)]\n",
        "        \n",
        "        # 被mask更新的权重\n",
        "        meaning_weight = (m_weight.view(-1, 1) * meaning_mask.view(-1, 1)).view(m_weight.shape)  # (batch_size, l, 2)\n",
        "        meaning_weight = meaning_weight.masked_fill((node_sets.unsqueeze(2).repeat(1, 1, meaning_mask.shape[2]) == self.padding_idx), 0)  # (batch_size, l, 2)\n",
        "\n",
        "        x = self.message_passing(node_sets, embedded_node, meaning_node1, meaning_node2, edge_weight, meaning_weight, embedded_neighbor_node) # x: (batch_size, c)  # 每张图的预测标签\n",
        "        return x\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--cuda', default='0', type=str, required=False,\n",
        "                    help='Choosing which cuda to use')\n",
        "parser.add_argument('--embedding_size', default=300, type=int, required=False,\n",
        "                    help='Number of hidden units in each layer of the graph embedding part')\n",
        "parser.add_argument('--p', default=3, type=int, required=False,\n",
        "                    help='The window size')\n",
        "parser.add_argument('--min_freq', default=2, type=int, required=False,\n",
        "                    help='The minimum no. of occurrence for a word to be considered as a meaningful word. Words with less than this occurrence will be mapped to a globally shared embedding weight (to the <unk> token). It corresponds to the parameter k in the original paper.')\n",
        "parser.add_argument('--max_length', default=70, type=int, required=False,\n",
        "                    help='The max length of each document to be processed')\n",
        "parser.add_argument('--dropout', default=0, type=float, required=False,\n",
        "                    help='Dropout rate')\n",
        "parser.add_argument('--lr', default=1e-3, type=float, required=False,\n",
        "                    help='Initial learning rate')\n",
        "parser.add_argument('--lr_decay_factor', default=0.9, type=float, required=False,\n",
        "                    help='Multiplicative factor of learning rate decays')  # 学习率衰减的乘子\n",
        "parser.add_argument('--lr_decay_every', default=5, type=int, required=False,\n",
        "                    help='Decaying learning rate every ? epochs')\n",
        "parser.add_argument('--weight_decay', default=1e-4, type=float, required=False,\n",
        "                    help='Weight decay (L2 penalty)')\n",
        "parser.add_argument('--warm_up_epoch', default=0, type=int, required=False,\n",
        "                    help='Pretraining for ? epochs before early stopping to be in effect')\n",
        "parser.add_argument('--early_stopping_patience', default=10, type=int, required=False,\n",
        "                    help='Waiting for ? more epochs after the best epoch to see any further improvements')\n",
        "parser.add_argument('--early_stopping_criteria', default='loss', type=str, required=False,\n",
        "                    choices=['accuracy', 'loss'],\n",
        "                    help='Early stopping according to validation accuracy or validation loss')\n",
        "parser.add_argument(\"--epoch\", default=100, type=int, required=False,\n",
        "                    help='Number of epochs to train')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "tokenizer = Full_GloveTokenizer(filename='/content/drive/MyDrive/Colab_Notebooks/DATA/pretrained_model/Full_GloVe_dic.pkl')\n",
        "\n",
        "dataset = TextLevelGNNDatasetClass(train_filename='/content/drive/MyDrive/Colab_Notebooks/DATA/r8-train-all-terms.txt',\n",
        "                                   test_filename='/content/drive/MyDrive/Colab_Notebooks/DATA/r8-test-all-terms.txt',              \n",
        "                                   opted_filename='/content/drive/MyDrive/Colab_Notebooks/DATA/pretrained_model/OPTED_dic.pkl',\n",
        "                                   full_glove_pth='/content/drive/MyDrive/Colab_Notebooks/DATA/pretrained_model/Full_GloVe_dic.pkl',\n",
        "                                   train_validation_split=0.9,  # train = train*0.x\n",
        "                                   tokenizer=tokenizer,\n",
        "                                   p=args.p,\n",
        "                                   min_freq=args.min_freq,\n",
        "                                   MAX_LENGTH=args.max_length)\n",
        "\n",
        "train_loader = DataLoader(dataset.train_dataset, batch_size=128, shuffle=True, collate_fn=pad_custom_sequence)\n",
        "validation_loader = DataLoader(dataset.validation_dataset, batch_size=128, shuffle=True, collate_fn=pad_custom_sequence)\n",
        "test_loader = DataLoader(dataset.test_dataset, batch_size=128, shuffle=True, collate_fn=pad_custom_sequence)\n",
        "\n",
        "device = torch.device(f'cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# device = torch.device('cpu')  # GPU内存太小，直接用CPU计算\n",
        "if torch.cuda.is_available():\n",
        "  print(f'device: {device}')\n",
        "  print(f'name:{torch.cuda.get_device_name(0)}')\n",
        "  print(f'memory:{torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
        "\n",
        "model = TextLevelGNN(pretrained_embeddings=torch.tensor(dataset.embedding_matrix), meaning_embeddings=torch.tensor(dataset.meaning_matrix), meaning_mask=torch.tensor(dataset.meaning_mask), dropout_rate=0).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "lr = args.lr\n",
        "lr_decay_factor = args.lr_decay_factor\n",
        "lr_decay_every = args.lr_decay_every\n",
        "weight_decay = args.weight_decay\n",
        "\n",
        "warm_up_epoch = args.warm_up_epoch\n",
        "early_stopping_patience = args.early_stopping_patience\n",
        "early_stopping_criteria = args.early_stopping_criteria\n",
        "best_epoch = 0 # Initialize\n",
        "\n",
        "training = {}  # {'accuracy':[], 'loss': []}\n",
        "validation = {}\n",
        "testing = {}\n",
        "training['accuracy'] = []\n",
        "training['loss'] = []\n",
        "validation['accuracy'] = []\n",
        "validation['loss'] = []\n",
        "testing['accuracy'] = []\n",
        "testing['loss'] = []\n",
        "\n",
        "for epoch in range(args.epoch):\n",
        "    model.train()  # 训练模式\n",
        "    train_loss = 0\n",
        "    train_correct_items = 0\n",
        "    previous_epoch_timestamp = time()\n",
        "\n",
        "    if epoch % lr_decay_every == 0: # Update optimizer for every lr_decay_every epochs\n",
        "        if epoch != 0: # When it is the first epoch, disable the lr_decay_factor\n",
        "            lr *= lr_decay_factor  # 乘以0.9的速度衰减\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # 更新优化器参数\n",
        "\n",
        "    for i, (node_sets, neighbor_sets, public_edge_masks, labels) in enumerate(train_loader):  # 批量取图数据\n",
        "        #print('Finished batch:', i)\n",
        "        node_sets = node_sets.to(device)\n",
        "        neighbor_sets = neighbor_sets.to(device)\n",
        "        public_edge_masks = public_edge_masks.to(device)\n",
        "        labels = labels.to(device)\n",
        "        prediction = model(node_sets, neighbor_sets, public_edge_masks)  # 调用TextLevelGNN类中的forward方法返回预测预测值，batch_size张图各自对应的预测结果\n",
        "        loss = criterion(prediction, labels).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_correct_items += (prediction.argmax(dim=1) == labels.argmax(dim=1)).sum().item()  # 获得训练集上总的预测正确的数量\n",
        "    train_accuracy = train_correct_items / len(dataset.train_dataset)  # 计算训练集上的准确率\n",
        "\n",
        "    model.eval()  # 评价模式\n",
        "    validation_loss = 0\n",
        "    validation_correct_items = 0\n",
        "    for i, (node_sets, neighbor_sets, public_edge_masks, labels) in enumerate(validation_loader):\n",
        "        node_sets = node_sets.to(device)\n",
        "        neighbor_sets = neighbor_sets.to(device)\n",
        "        public_edge_masks = public_edge_masks.to(device)\n",
        "        labels = labels.to(device)\n",
        "        prediction = model(node_sets, neighbor_sets, public_edge_masks)\n",
        "        loss = criterion(prediction, labels).to(device)\n",
        "        validation_loss += loss.item()\n",
        "        validation_correct_items += (prediction.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
        "    validation_accuracy = validation_correct_items / len(dataset.validation_dataset)\n",
        "\n",
        "#     model.eval()\n",
        "    test_loss = 0\n",
        "    test_correct_items = 0\n",
        "    for i, (node_sets, neighbor_sets, public_edge_masks, labels) in enumerate(test_loader):\n",
        "        node_sets = node_sets.to(device)\n",
        "        neighbor_sets = neighbor_sets.to(device)\n",
        "        public_edge_masks = public_edge_masks.to(device)\n",
        "        labels = labels.to(device)\n",
        "        prediction = model(node_sets, neighbor_sets, public_edge_masks)\n",
        "        loss = criterion(prediction, labels).to(device)\n",
        "        test_loss += loss.item()\n",
        "        test_correct_items += (prediction.argmax(dim=1) == labels.argmax(dim=1)).sum().item()\n",
        "    test_accuracy = test_correct_items / len(dataset.test_dataset)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}, Testing Loss: {test_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {validation_accuracy:.4f}, Testing Accuracy: {test_accuracy:.4f}, Time Used: {time()-previous_epoch_timestamp:.2f}s')\n",
        "    training['accuracy'].append(train_accuracy)\n",
        "    training['loss'].append(train_loss)\n",
        "    validation['accuracy'].append(validation_accuracy)\n",
        "    validation['loss'].append(validation_loss)\n",
        "    testing['accuracy'].append(test_accuracy)\n",
        "    testing['loss'].append(test_loss)\n",
        "\n",
        "    # add warmup mechanism for warm_up_epoch epochs\n",
        "    if epoch >= warm_up_epoch:\n",
        "        best_epoch = warm_up_epoch\n",
        "        # early stopping\n",
        "        if early_stopping_criteria == 'accuracy':\n",
        "            if validation['accuracy'][epoch] > validation['accuracy'][best_epoch]:\n",
        "                best_epoch = epoch\n",
        "            elif epoch >= best_epoch + early_stopping_patience:\n",
        "                print(f'Early stopping... (No further increase in validation accuracy) for consecutive {early_stopping_patience} epochs.')\n",
        "                break\n",
        "        if early_stopping_criteria == 'loss':\n",
        "            if validation['loss'][epoch] < validation['loss'][best_epoch]:\n",
        "                best_epoch = epoch\n",
        "            elif epoch >= best_epoch + early_stopping_patience:\n",
        "                print(f'Early stopping... (No further decrease in validation loss) for consecutive {early_stopping_patience} epochs.')\n",
        "                break\n",
        "    elif epoch + 1 == warm_up_epoch:\n",
        "        print('--- Warm up finished ---')\n",
        "\n",
        "# 保存结果至表格\n",
        "df = pd.concat([pd.DataFrame(training), pd.DataFrame(validation), pd.DataFrame(testing)], axis=1)\n",
        "df.columns = ['Training Accuracy', 'Training Loss', 'Validation Accuracy', 'Validation Loss', 'Testing Accuracy', 'Testing Loss']\n",
        "df.to_csv(f'/content/drive/MyDrive/Colab_Notebooks/DATA/F_r8-eb{args.embedding_size}-p{args.p}-k{args.min_freq}-max_l{args.max_length}-epoch{args.epoch}.csv') # Logging\n",
        "\n",
        "\n",
        "# 保存结果图像\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(8,4))\n",
        "\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax1.plot(training['loss'], label='Training Loss')\n",
        "ax1.plot(validation['loss'], label='Validation Loss')\n",
        "ax1.plot(testing['loss'], label='Testing Loss')\n",
        "ax1.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.plot(training['accuracy'], label='Training Accuracy')\n",
        "ax2.plot(validation['accuracy'], label='Validation Accuracy')\n",
        "ax2.plot(testing['accuracy'], label='Testing Accuracy')\n",
        "ax2.legend()\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.ylabel('Accuracy')\n",
        "plt.savefig(f'/content/drive/MyDrive/Colab_Notebooks/DATA/F_r8-eb{args.embedding_size}-p{args.p}-k{args.min_freq}-max_l{args.max_length}-epoch{args.epoch}.png', dpi=400, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "调参"
      ],
      "metadata": {
        "id": "1LwtpHndVoFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_LAUNCH_BLOCKING=1 python parsing.py --cuda=0 --embedding_size=300 --p=3 --min_freq=2 --max_length=100 --dropout=0.1 --epoch=85"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWcaYifsVmLk",
        "outputId": "8d76f31b-9cb7-4db3-b7c5-fc6631d17b5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcmalloc: large alloc 1115734016 bytes == 0x55d878704000 @  0x7fcd4326e1e7 0x7fcd40cae46e 0x7fcd40cfec7b 0x7fcd40d01e83 0x7fcd40d0207b 0x7fcd40da3761 0x55d8407544b0 0x55d840754240 0x55d8407c80f3 0x55d8407c29ee 0x55d84075648c 0x55d840797159 0x55d8407940a4 0x55d840754d49 0x55d8407c894f 0x55d8407c29ee 0x55d8407c26f3 0x55d84088c4c2 0x55d84088c83d 0x55d84088c6e6 0x55d840864163 0x55d840863e0c 0x7fcd42058bf7 0x55d840863cea\n",
            "device: cuda:0\n",
            "name:Tesla T4\n",
            "memory:15.843721216\n",
            "Epoch: 1, Training Loss: 29.5012, Validation Loss: 4.3510, Testing Loss: 12.3716, Training Accuracy: 0.6220, Validation Accuracy: 0.6776, Testing Accuracy: 0.7474, Time Used: 13.77s\n",
            "Epoch: 2, Training Loss: 21.1510, Validation Loss: 2.7833, Testing Loss: 8.1083, Training Accuracy: 0.7887, Validation Accuracy: 0.6976, Testing Accuracy: 0.7332, Time Used: 13.67s\n",
            "Epoch: 3, Training Loss: 7.0615, Validation Loss: 0.7224, Testing Loss: 2.3312, Training Accuracy: 0.8112, Validation Accuracy: 0.7395, Testing Accuracy: 0.7903, Time Used: 13.62s\n",
            "Epoch: 4, Training Loss: 3.8003, Validation Loss: 0.6132, Testing Loss: 2.0961, Training Accuracy: 0.8395, Validation Accuracy: 0.7650, Testing Accuracy: 0.8122, Time Used: 13.61s\n",
            "Epoch: 5, Training Loss: 3.3468, Validation Loss: 0.6702, Testing Loss: 2.0691, Training Accuracy: 0.8464, Validation Accuracy: 0.7577, Testing Accuracy: 0.8036, Time Used: 13.58s\n",
            "Epoch: 6, Training Loss: 3.6847, Validation Loss: 0.6227, Testing Loss: 1.7812, Training Accuracy: 0.8308, Validation Accuracy: 0.7741, Testing Accuracy: 0.8218, Time Used: 13.63s\n",
            "Epoch: 7, Training Loss: 2.9409, Validation Loss: 0.5340, Testing Loss: 1.8356, Training Accuracy: 0.8547, Validation Accuracy: 0.7887, Testing Accuracy: 0.8296, Time Used: 13.65s\n",
            "Epoch: 8, Training Loss: 2.7943, Validation Loss: 0.5496, Testing Loss: 1.7193, Training Accuracy: 0.8584, Validation Accuracy: 0.7978, Testing Accuracy: 0.8319, Time Used: 13.64s\n",
            "Epoch: 9, Training Loss: 2.7002, Validation Loss: 0.5258, Testing Loss: 1.6032, Training Accuracy: 0.8606, Validation Accuracy: 0.8033, Testing Accuracy: 0.8383, Time Used: 13.66s\n",
            "Epoch: 10, Training Loss: 2.6106, Validation Loss: 0.5267, Testing Loss: 1.4788, Training Accuracy: 0.8643, Validation Accuracy: 0.8106, Testing Accuracy: 0.8451, Time Used: 13.70s\n",
            "Epoch: 11, Training Loss: 2.7212, Validation Loss: 0.4871, Testing Loss: 1.4099, Training Accuracy: 0.8655, Validation Accuracy: 0.8142, Testing Accuracy: 0.8474, Time Used: 13.66s\n",
            "Epoch: 12, Training Loss: 2.5751, Validation Loss: 0.4848, Testing Loss: 1.7193, Training Accuracy: 0.8624, Validation Accuracy: 0.8087, Testing Accuracy: 0.8305, Time Used: 13.56s\n",
            "Epoch: 13, Training Loss: 2.5116, Validation Loss: 0.5061, Testing Loss: 1.6634, Training Accuracy: 0.8635, Validation Accuracy: 0.8033, Testing Accuracy: 0.8278, Time Used: 13.61s\n",
            "Epoch: 14, Training Loss: 2.4802, Validation Loss: 0.5574, Testing Loss: 1.4578, Training Accuracy: 0.8653, Validation Accuracy: 0.8142, Testing Accuracy: 0.8415, Time Used: 13.59s\n",
            "Epoch: 15, Training Loss: 2.4350, Validation Loss: 0.4650, Testing Loss: 1.4531, Training Accuracy: 0.8669, Validation Accuracy: 0.8160, Testing Accuracy: 0.8429, Time Used: 13.60s\n",
            "Epoch: 16, Training Loss: 2.0623, Validation Loss: 0.3830, Testing Loss: 1.2004, Training Accuracy: 0.9234, Validation Accuracy: 0.8780, Testing Accuracy: 0.8995, Time Used: 13.68s\n",
            "Epoch: 17, Training Loss: 1.5259, Validation Loss: 0.3072, Testing Loss: 0.9483, Training Accuracy: 0.9607, Validation Accuracy: 0.9107, Testing Accuracy: 0.9324, Time Used: 13.63s\n",
            "Epoch: 18, Training Loss: 1.2095, Validation Loss: 0.2749, Testing Loss: 0.8937, Training Accuracy: 0.9739, Validation Accuracy: 0.9381, Testing Accuracy: 0.9447, Time Used: 13.61s\n",
            "Epoch: 19, Training Loss: 1.0768, Validation Loss: 0.2418, Testing Loss: 0.8057, Training Accuracy: 0.9793, Validation Accuracy: 0.9399, Testing Accuracy: 0.9493, Time Used: 13.65s\n",
            "Epoch: 20, Training Loss: 0.9795, Validation Loss: 0.2383, Testing Loss: 0.7905, Training Accuracy: 0.9820, Validation Accuracy: 0.9435, Testing Accuracy: 0.9497, Time Used: 13.69s\n",
            "Epoch: 21, Training Loss: 1.0096, Validation Loss: 0.2294, Testing Loss: 0.8893, Training Accuracy: 0.9783, Validation Accuracy: 0.9362, Testing Accuracy: 0.9488, Time Used: 13.69s\n",
            "Epoch: 22, Training Loss: 0.8747, Validation Loss: 0.2197, Testing Loss: 0.8807, Training Accuracy: 0.9801, Validation Accuracy: 0.9454, Testing Accuracy: 0.9497, Time Used: 13.62s\n",
            "Epoch: 23, Training Loss: 0.7925, Validation Loss: 0.2370, Testing Loss: 0.7725, Training Accuracy: 0.9830, Validation Accuracy: 0.9417, Testing Accuracy: 0.9548, Time Used: 13.68s\n",
            "Epoch: 24, Training Loss: 0.7571, Validation Loss: 0.1870, Testing Loss: 0.7522, Training Accuracy: 0.9846, Validation Accuracy: 0.9526, Testing Accuracy: 0.9529, Time Used: 13.65s\n",
            "Epoch: 25, Training Loss: 0.7248, Validation Loss: 0.2020, Testing Loss: 0.7671, Training Accuracy: 0.9852, Validation Accuracy: 0.9508, Testing Accuracy: 0.9497, Time Used: 13.64s\n",
            "Epoch: 26, Training Loss: 0.7406, Validation Loss: 0.2062, Testing Loss: 0.7555, Training Accuracy: 0.9836, Validation Accuracy: 0.9545, Testing Accuracy: 0.9529, Time Used: 13.66s\n",
            "Epoch: 27, Training Loss: 0.6430, Validation Loss: 0.1939, Testing Loss: 0.7598, Training Accuracy: 0.9887, Validation Accuracy: 0.9417, Testing Accuracy: 0.9525, Time Used: 13.58s\n",
            "Epoch: 28, Training Loss: 0.6155, Validation Loss: 0.1919, Testing Loss: 0.6874, Training Accuracy: 0.9889, Validation Accuracy: 0.9581, Testing Accuracy: 0.9593, Time Used: 13.73s\n",
            "Epoch: 29, Training Loss: 0.5637, Validation Loss: 0.1968, Testing Loss: 0.7630, Training Accuracy: 0.9913, Validation Accuracy: 0.9508, Testing Accuracy: 0.9589, Time Used: 13.63s\n",
            "Epoch: 30, Training Loss: 0.5420, Validation Loss: 0.1940, Testing Loss: 0.7672, Training Accuracy: 0.9921, Validation Accuracy: 0.9526, Testing Accuracy: 0.9520, Time Used: 13.64s\n",
            "Epoch: 31, Training Loss: 0.5870, Validation Loss: 0.2169, Testing Loss: 0.7887, Training Accuracy: 0.9905, Validation Accuracy: 0.9508, Testing Accuracy: 0.9516, Time Used: 13.62s\n",
            "Epoch: 32, Training Loss: 0.5118, Validation Loss: 0.1834, Testing Loss: 0.6579, Training Accuracy: 0.9923, Validation Accuracy: 0.9526, Testing Accuracy: 0.9625, Time Used: 13.67s\n",
            "Epoch: 33, Training Loss: 0.4937, Validation Loss: 0.1776, Testing Loss: 0.7626, Training Accuracy: 0.9933, Validation Accuracy: 0.9581, Testing Accuracy: 0.9621, Time Used: 13.69s\n",
            "Epoch: 34, Training Loss: 0.4738, Validation Loss: 0.1919, Testing Loss: 0.6862, Training Accuracy: 0.9935, Validation Accuracy: 0.9545, Testing Accuracy: 0.9603, Time Used: 13.67s\n",
            "Epoch: 35, Training Loss: 0.4347, Validation Loss: 0.2145, Testing Loss: 0.6918, Training Accuracy: 0.9943, Validation Accuracy: 0.9563, Testing Accuracy: 0.9593, Time Used: 13.65s\n",
            "Epoch: 36, Training Loss: 0.4788, Validation Loss: 0.2370, Testing Loss: 0.8101, Training Accuracy: 0.9933, Validation Accuracy: 0.9508, Testing Accuracy: 0.9529, Time Used: 13.64s\n",
            "Epoch: 37, Training Loss: 0.4527, Validation Loss: 0.1834, Testing Loss: 0.6769, Training Accuracy: 0.9935, Validation Accuracy: 0.9545, Testing Accuracy: 0.9621, Time Used: 13.64s\n",
            "Epoch: 38, Training Loss: 0.4100, Validation Loss: 0.1729, Testing Loss: 0.6724, Training Accuracy: 0.9953, Validation Accuracy: 0.9526, Testing Accuracy: 0.9621, Time Used: 13.60s\n",
            "Epoch: 39, Training Loss: 0.3802, Validation Loss: 0.1754, Testing Loss: 0.6360, Training Accuracy: 0.9951, Validation Accuracy: 0.9563, Testing Accuracy: 0.9644, Time Used: 13.65s\n",
            "Epoch: 40, Training Loss: 0.3828, Validation Loss: 0.2109, Testing Loss: 0.6422, Training Accuracy: 0.9947, Validation Accuracy: 0.9490, Testing Accuracy: 0.9625, Time Used: 13.73s\n",
            "Epoch: 41, Training Loss: 0.4148, Validation Loss: 0.1787, Testing Loss: 0.6884, Training Accuracy: 0.9947, Validation Accuracy: 0.9508, Testing Accuracy: 0.9616, Time Used: 13.72s\n",
            "Epoch: 42, Training Loss: 0.3810, Validation Loss: 0.1955, Testing Loss: 0.6633, Training Accuracy: 0.9949, Validation Accuracy: 0.9508, Testing Accuracy: 0.9648, Time Used: 13.69s\n",
            "Epoch: 43, Training Loss: 0.3599, Validation Loss: 0.1656, Testing Loss: 0.6242, Training Accuracy: 0.9951, Validation Accuracy: 0.9545, Testing Accuracy: 0.9671, Time Used: 13.69s\n",
            "Epoch: 44, Training Loss: 0.3423, Validation Loss: 0.1870, Testing Loss: 0.6526, Training Accuracy: 0.9957, Validation Accuracy: 0.9508, Testing Accuracy: 0.9616, Time Used: 13.65s\n",
            "Epoch: 45, Training Loss: 0.3384, Validation Loss: 0.1793, Testing Loss: 0.6304, Training Accuracy: 0.9959, Validation Accuracy: 0.9526, Testing Accuracy: 0.9621, Time Used: 13.73s\n",
            "Epoch: 46, Training Loss: 0.3679, Validation Loss: 0.1868, Testing Loss: 0.6503, Training Accuracy: 0.9947, Validation Accuracy: 0.9508, Testing Accuracy: 0.9639, Time Used: 13.64s\n",
            "Epoch: 47, Training Loss: 0.3366, Validation Loss: 0.1926, Testing Loss: 0.6435, Training Accuracy: 0.9951, Validation Accuracy: 0.9508, Testing Accuracy: 0.9635, Time Used: 13.67s\n",
            "Epoch: 48, Training Loss: 0.3208, Validation Loss: 0.1853, Testing Loss: 0.7594, Training Accuracy: 0.9968, Validation Accuracy: 0.9581, Testing Accuracy: 0.9607, Time Used: 13.67s\n",
            "Epoch: 49, Training Loss: 0.3059, Validation Loss: 0.1569, Testing Loss: 0.6169, Training Accuracy: 0.9966, Validation Accuracy: 0.9563, Testing Accuracy: 0.9639, Time Used: 13.67s\n",
            "Epoch: 50, Training Loss: 0.2978, Validation Loss: 0.1861, Testing Loss: 0.6356, Training Accuracy: 0.9968, Validation Accuracy: 0.9508, Testing Accuracy: 0.9653, Time Used: 13.71s\n",
            "Epoch: 51, Training Loss: 0.3450, Validation Loss: 0.1718, Testing Loss: 0.6739, Training Accuracy: 0.9953, Validation Accuracy: 0.9563, Testing Accuracy: 0.9630, Time Used: 13.66s\n",
            "Epoch: 52, Training Loss: 0.3097, Validation Loss: 0.1685, Testing Loss: 0.6211, Training Accuracy: 0.9962, Validation Accuracy: 0.9545, Testing Accuracy: 0.9676, Time Used: 13.70s\n",
            "Epoch: 53, Training Loss: 0.2916, Validation Loss: 0.1627, Testing Loss: 0.6137, Training Accuracy: 0.9968, Validation Accuracy: 0.9508, Testing Accuracy: 0.9667, Time Used: 13.64s\n",
            "Epoch: 54, Training Loss: 0.2820, Validation Loss: 0.1574, Testing Loss: 0.6042, Training Accuracy: 0.9972, Validation Accuracy: 0.9545, Testing Accuracy: 0.9648, Time Used: 13.69s\n",
            "Epoch: 55, Training Loss: 0.2788, Validation Loss: 0.2221, Testing Loss: 0.6317, Training Accuracy: 0.9966, Validation Accuracy: 0.9508, Testing Accuracy: 0.9644, Time Used: 13.72s\n",
            "Epoch: 56, Training Loss: 0.3088, Validation Loss: 0.1792, Testing Loss: 0.6066, Training Accuracy: 0.9962, Validation Accuracy: 0.9545, Testing Accuracy: 0.9667, Time Used: 13.64s\n",
            "Epoch: 57, Training Loss: 0.2928, Validation Loss: 0.1826, Testing Loss: 0.6214, Training Accuracy: 0.9970, Validation Accuracy: 0.9472, Testing Accuracy: 0.9676, Time Used: 13.70s\n",
            "Epoch: 58, Training Loss: 0.2714, Validation Loss: 0.1721, Testing Loss: 0.6172, Training Accuracy: 0.9966, Validation Accuracy: 0.9508, Testing Accuracy: 0.9635, Time Used: 13.67s\n",
            "Epoch: 59, Training Loss: 0.2665, Validation Loss: 0.1761, Testing Loss: 0.5936, Training Accuracy: 0.9970, Validation Accuracy: 0.9508, Testing Accuracy: 0.9662, Time Used: 13.78s\n",
            "Epoch: 60, Training Loss: 0.2507, Validation Loss: 0.1837, Testing Loss: 0.6493, Training Accuracy: 0.9974, Validation Accuracy: 0.9508, Testing Accuracy: 0.9639, Time Used: 13.70s\n",
            "Epoch: 61, Training Loss: 0.2832, Validation Loss: 0.1851, Testing Loss: 0.6462, Training Accuracy: 0.9964, Validation Accuracy: 0.9508, Testing Accuracy: 0.9635, Time Used: 13.73s\n",
            "Epoch: 62, Training Loss: 0.2684, Validation Loss: 0.2049, Testing Loss: 0.6020, Training Accuracy: 0.9976, Validation Accuracy: 0.9454, Testing Accuracy: 0.9671, Time Used: 13.68s\n",
            "Epoch: 63, Training Loss: 0.2548, Validation Loss: 0.2062, Testing Loss: 0.5807, Training Accuracy: 0.9972, Validation Accuracy: 0.9490, Testing Accuracy: 0.9689, Time Used: 13.70s\n",
            "Epoch: 64, Training Loss: 0.2472, Validation Loss: 0.1659, Testing Loss: 0.6062, Training Accuracy: 0.9974, Validation Accuracy: 0.9508, Testing Accuracy: 0.9644, Time Used: 13.76s\n",
            "Epoch: 65, Training Loss: 0.2402, Validation Loss: 0.1647, Testing Loss: 0.6028, Training Accuracy: 0.9972, Validation Accuracy: 0.9508, Testing Accuracy: 0.9648, Time Used: 13.66s\n",
            "Epoch: 66, Training Loss: 0.2677, Validation Loss: 0.1773, Testing Loss: 0.6654, Training Accuracy: 0.9974, Validation Accuracy: 0.9526, Testing Accuracy: 0.9657, Time Used: 13.69s\n",
            "Epoch: 67, Training Loss: 0.2525, Validation Loss: 0.1699, Testing Loss: 0.6055, Training Accuracy: 0.9972, Validation Accuracy: 0.9508, Testing Accuracy: 0.9680, Time Used: 13.69s\n",
            "Epoch: 68, Training Loss: 0.2381, Validation Loss: 0.1637, Testing Loss: 0.6056, Training Accuracy: 0.9976, Validation Accuracy: 0.9490, Testing Accuracy: 0.9671, Time Used: 13.67s\n",
            "Epoch: 69, Training Loss: 0.2328, Validation Loss: 0.1568, Testing Loss: 0.6438, Training Accuracy: 0.9974, Validation Accuracy: 0.9490, Testing Accuracy: 0.9644, Time Used: 13.71s\n",
            "Epoch: 70, Training Loss: 0.2255, Validation Loss: 0.1854, Testing Loss: 0.6527, Training Accuracy: 0.9974, Validation Accuracy: 0.9490, Testing Accuracy: 0.9671, Time Used: 13.71s\n",
            "Epoch: 71, Training Loss: 0.2524, Validation Loss: 0.1574, Testing Loss: 0.6229, Training Accuracy: 0.9970, Validation Accuracy: 0.9490, Testing Accuracy: 0.9662, Time Used: 13.70s\n",
            "Epoch: 72, Training Loss: 0.2352, Validation Loss: 0.1904, Testing Loss: 1.6728, Training Accuracy: 0.9972, Validation Accuracy: 0.9508, Testing Accuracy: 0.9680, Time Used: 13.68s\n",
            "Epoch: 73, Training Loss: 0.2270, Validation Loss: 0.1574, Testing Loss: 0.6039, Training Accuracy: 0.9976, Validation Accuracy: 0.9490, Testing Accuracy: 0.9662, Time Used: 13.69s\n",
            "Epoch: 74, Training Loss: 0.2223, Validation Loss: 0.1695, Testing Loss: 0.6006, Training Accuracy: 0.9974, Validation Accuracy: 0.9472, Testing Accuracy: 0.9657, Time Used: 13.68s\n",
            "Epoch: 75, Training Loss: 0.2163, Validation Loss: 0.1605, Testing Loss: 0.5857, Training Accuracy: 0.9972, Validation Accuracy: 0.9490, Testing Accuracy: 0.9662, Time Used: 13.69s\n",
            "Epoch: 76, Training Loss: 0.2345, Validation Loss: 0.1713, Testing Loss: 0.5890, Training Accuracy: 0.9974, Validation Accuracy: 0.9490, Testing Accuracy: 0.9671, Time Used: 13.72s\n",
            "Epoch: 77, Training Loss: 0.2233, Validation Loss: 0.1719, Testing Loss: 0.6857, Training Accuracy: 0.9972, Validation Accuracy: 0.9472, Testing Accuracy: 0.9662, Time Used: 13.62s\n",
            "Epoch: 78, Training Loss: 0.2144, Validation Loss: 0.1681, Testing Loss: 0.6408, Training Accuracy: 0.9974, Validation Accuracy: 0.9545, Testing Accuracy: 0.9657, Time Used: 13.82s\n",
            "Epoch: 79, Training Loss: 0.2177, Validation Loss: 0.1603, Testing Loss: 0.5897, Training Accuracy: 0.9974, Validation Accuracy: 0.9490, Testing Accuracy: 0.9662, Time Used: 13.70s\n",
            "Epoch: 80, Training Loss: 0.2084, Validation Loss: 0.1564, Testing Loss: 0.5978, Training Accuracy: 0.9974, Validation Accuracy: 0.9490, Testing Accuracy: 0.9676, Time Used: 13.67s\n",
            "Epoch: 81, Training Loss: 0.2253, Validation Loss: 0.1549, Testing Loss: 0.5846, Training Accuracy: 0.9976, Validation Accuracy: 0.9490, Testing Accuracy: 0.9667, Time Used: 13.70s\n",
            "Epoch: 82, Training Loss: 0.2134, Validation Loss: 0.1879, Testing Loss: 0.7037, Training Accuracy: 0.9976, Validation Accuracy: 0.9508, Testing Accuracy: 0.9680, Time Used: 13.84s\n",
            "Epoch: 83, Training Loss: 0.2093, Validation Loss: 0.1633, Testing Loss: 0.5873, Training Accuracy: 0.9976, Validation Accuracy: 0.9490, Testing Accuracy: 0.9667, Time Used: 13.71s\n",
            "Epoch: 84, Training Loss: 0.2043, Validation Loss: 0.2007, Testing Loss: 0.5963, Training Accuracy: 0.9978, Validation Accuracy: 0.9508, Testing Accuracy: 0.9676, Time Used: 13.64s\n",
            "Epoch: 85, Training Loss: 0.2003, Validation Loss: 0.1634, Testing Loss: 0.5792, Training Accuracy: 0.9978, Validation Accuracy: 0.9508, Testing Accuracy: 0.9671, Time Used: 13.65s\n"
          ]
        }
      ]
    }
  ]
}